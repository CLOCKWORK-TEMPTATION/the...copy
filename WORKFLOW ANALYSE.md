

Ø¥Ù„ÙŠÙƒ Ø§Ù„Ù…Ù„Ù ÙƒØ§Ù…Ù„Ø§Ù‹ Ø¨Ø¹Ø¯ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØµØ­ÙŠØ­Ø§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø© Ø¹Ù„Ù‰ ØªØ±Ù‚ÙŠÙ… Ø®Ø·ÙˆØ§Øª Ø§Ù„Ù€ Workflow Ø¯Ø§Ø®Ù„ Ø¯Ø§Ù„Ø© `main`:

```markdown
# Workflow ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹ - Ù…ÙØ­Ø³Ù‘Ù† Ù„Ù„Ø­Ø°Ù Ø§Ù„Ø¢Ù…Ù†

## Ø§Ù„Ù‡Ø¯Ù Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
**Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆØ¯Ø¹ Ù†Ø¸ÙŠÙ: ÙƒÙ„ Ù…Ù„Ù ÙÙŠÙ‡ Ù…ÙØ¹Ù‘Ù„ ÙˆÙ„Ù‡ Ø¹Ù„Ø§Ù‚Ø© Ù…Ø¨Ø§Ø´Ø±Ø© Ø¨Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ØŒ ÙˆØ§Ù„ØªØ®Ù„Øµ Ù…Ù† Ø£ÙŠ Ù…Ù„ÙØ§Øª ØºÙŠØ± Ø¶Ø±ÙˆØ±ÙŠØ© Ø¨Ø´ÙƒÙ„ Ø¢Ù…Ù†.**

---

## Ø§Ù„Ù…Ø±Ø­Ù„Ø© 0: Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯ ÙˆØ§Ù„Ù€ Backup Ø§Ù„Ø¥Ù„Ø²Ø§Ù…ÙŠ

### 0.1 Ø§Ù„ØªØ«Ø¨ÙŠØª

```bash
# Ø£Ø¯ÙˆØ§Øª Ø£Ø³Ø§Ø³ÙŠØ©
npm install --save-dev dependency-cruiser knip ts-prune
npm install -g madge depcheck

# Ø£Ø¯ÙˆØ§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ù„Ù„ØªØ­Ù‚Ù‚
npm install --save-dev eslint typescript
pip install --break-system-packages gitpython
```

### 0.2 Ø¥Ù†Ø´Ø§Ø¡ Backup ÙƒØ§Ù…Ù„

```python
import shutil
import datetime
import os
from pathlib import Path

def create_backup(repo_path):
    """
    Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© ÙƒØ§Ù…Ù„Ø© Ù‚Ø¨Ù„ Ø£ÙŠ Ø­Ø°Ù
    """
    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
    backup_name = f"backup_{timestamp}"
    backup_path = Path(repo_path).parent / backup_name
    
    print(f"ğŸ”„ Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©...")
    
    # Ù†Ø³Ø® ÙƒØ§Ù…Ù„ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹
    shutil.copytree(
        repo_path, 
        backup_path,
        ignore=shutil.ignore_patterns('node_modules', '.git', 'dist', 'build', '__pycache__')
    )
    
    # Ø­ÙØ¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù€ backup
    backup_info = {
        'timestamp': timestamp,
        'original_path': str(repo_path),
        'backup_path': str(backup_path),
        'commit_hash': get_current_commit_hash(repo_path)
    }
    
    with open(backup_path / 'BACKUP_INFO.json', 'w') as f:
        json.dump(backup_info, f, indent=2)
    
    print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©: {backup_path}")
    return backup_path

def get_current_commit_hash(repo_path):
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¢Ø®Ø± commit hash"""
    try:
        result = subprocess.run(
            ['git', 'rev-parse', 'HEAD'],
            cwd=repo_path,
            capture_output=True,
            text=True
        )
        return result.stdout.strip()
    except:
        return None
```

### 0.3 Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ù„Ù Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª

```python
# cleanup_config.json
{
    "repo_path": "./src",
    "ignore_patterns": [
        "node_modules",
        ".git",
        "dist",
        "build",
        "__pycache__",
        ".vscode",
        ".idea"
    ],
    "entry_points": [
        "src/main.ts",
        "src/index.ts",
        "src/app.tsx",
        "src/server.js"
    ],
    "protected_files": [
        "package.json",
        "tsconfig.json",
        ".env.example",
        "README.md",
        ".gitignore"
    ],
    "safe_mode": true,  # Ø·Ù„Ø¨ Ù…ÙˆØ§ÙÙ‚Ø© Ù‚Ø¨Ù„ ÙƒÙ„ Ø­Ø°Ù
    "create_backup": true,
    "dry_run": false  # true = Ù…Ø­Ø§ÙƒØ§Ø© ÙÙ‚Ø· Ø¯ÙˆÙ† Ø­Ø°Ù ÙØ¹Ù„ÙŠ
}
```

---

## Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø§Ù„Ù…Ø³Ø­ Ø§Ù„Ø´Ø§Ù…Ù„ ÙˆØ¨Ù†Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ§Øª Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø©

### 1.1 Ø¬Ù…Ø¹ ÙƒÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª

```python
def collect_all_files(repo_path, ignore_patterns, config):
    """
    Ø¬Ù…Ø¹ ÙƒÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ù…Ø¹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ©
    """
    all_files = {}
    
    for root, dirs, files in os.walk(repo_path):
        # ØªØµÙÙŠØ© Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ù…Ø³ØªØ«Ù†Ø§Ø©
        dirs[:] = [d for d in dirs if d not in ignore_patterns]
        
        for file in files:
            file_path = os.path.join(root, file)
            relative_path = os.path.relpath(file_path, repo_path)
            
            # ØªØ®Ø·ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ù…ÙŠØ©
            if relative_path in config['protected_files']:
                continue
            
            all_files[relative_path] = {
                'absolute_path': file_path,
                'relative_path': relative_path,
                'extension': Path(file).suffix,
                'size_bytes': os.path.getsize(file_path),
                'is_protected': False,
                'analysis_status': 'pending'
            }
    
    return all_files
```

### 1.2 Ø¨Ù†Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ§Øª Ø´Ø§Ù…Ù„Ø©

```python
def build_complete_dependency_map(repo_path):
    """
    Ø¨Ù†Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ§Øª Ø¯Ù‚ÙŠÙ‚Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙƒÙ„ Ø§Ù„Ø£Ø¯ÙˆØ§Øª
    """
    print("ğŸ” Ø¬Ø§Ø±ÙŠ Ø¨Ù†Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ§Øª...")
    
    dependency_map = {
        'imports': {},      # Ù…Ù† ÙŠØ³ØªÙˆØ±Ø¯ Ù…Ù†
        'imported_by': {},  # Ù…Ù† ÙŠØªÙ… Ø§Ø³ØªÙŠØ±Ø§Ø¯Ù‡ Ø¨ÙˆØ§Ø³Ø·Ø©
        'unused_exports': [],
        'unused_dependencies': [],
        'circular_dependencies': []
    }
    
    # 1. dependency-cruiser - Ø§Ù„Ø£Ø¯Ù‚
    print("  â”œâ”€ ØªØ´ØºÙŠÙ„ dependency-cruiser...")
    dep_cruise_result = run_dependency_cruiser(repo_path)
    dependency_map = merge_depcruise_results(dependency_map, dep_cruise_result)
    
    # 2. madge - Ù„Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹
    print("  â”œâ”€ ØªØ´ØºÙŠÙ„ madge...")
    madge_result = run_madge(repo_path)
    dependency_map = merge_madge_results(dependency_map, madge_result)
    
    # 3. Knip - Ù„ÙƒØ´Ù Ø§Ù„Ù€ exports ØºÙŠØ± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©
    print("  â”œâ”€ ØªØ´ØºÙŠÙ„ knip...")
    knip_result = run_knip(repo_path)
    dependency_map['unused_exports'] = knip_result['unused_exports']
    
    # 4. depcheck - Ù„ÙƒØ´Ù Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ§Øª ØºÙŠØ± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©
    print("  â”œâ”€ ØªØ´ØºÙŠÙ„ depcheck...")
    depcheck_result = run_depcheck(repo_path)
    dependency_map['unused_dependencies'] = depcheck_result['unused']
    
    # 5. ts-prune (Ù„Ù„Ù€ TypeScript)
    if has_typescript_files(repo_path):
        print("  â””â”€ ØªØ´ØºÙŠÙ„ ts-prune...")
        ts_prune_result = run_ts_prune(repo_path)
        dependency_map['unused_exports'].extend(ts_prune_result)
    
    print("âœ… ØªÙ… Ø¨Ù†Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ§Øª")
    return dependency_map

def run_dependency_cruiser(repo_path):
    """ØªØ´ØºÙŠÙ„ dependency-cruiser"""
    cmd = f'depcruise --include-only "^src" --output-type json {repo_path}'
    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
    return json.loads(result.stdout) if result.returncode == 0 else {}

def run_knip(repo_path):
    """ØªØ´ØºÙŠÙ„ knip"""
    cmd = 'npx knip --reporter json'
    result = subprocess.run(cmd, shell=True, capture_output=True, text=True, cwd=repo_path)
    return json.loads(result.stdout) if result.returncode == 0 else {'unused_exports': []}
```

---

## Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1.5: Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ Ø¨Ø§Ù„Ù€ AI

### 1.5.1 ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù†Ù…ÙˆØ°Ø¬

```python
import os
import google.generativeai as genai
from dotenv import load_dotenv

def initialize_gemini():
    """
    ØªÙ‡ÙŠØ¦Ø© Ù†Ù…ÙˆØ°Ø¬ Gemini Ù„Ù„ØªØ­Ù„ÙŠÙ„
    """
    load_dotenv()
    
    api_key = os.getenv('GEMINI_API_KEY')
    if not api_key:
        raise ValueError("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ GEMINI_API_KEY ÙÙŠ Ù…Ù„Ù .env")
    
    genai.configure(api_key=api_key)
    
    # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ù‚ÙˆÙ‰ Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¯Ù‚ÙŠÙ‚
    model = genai.GenerativeModel('gemini-2.0-flash-exp')
    
    print("âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Gemini 2.0 Flash")
    return model
```

### 1.5.2 Ø§Ù„Ø£Ù…Ø± Ø§Ù„ØªÙˆØ¬ÙŠÙ‡ÙŠ Ø§Ù„Ù…Ø®ØµØµ

```python
CLEANUP_FOCUSED_PROMPT = """
**Ø§Ù„Ù‡Ø¯Ù Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ:** Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø®Ø¨ÙŠØ± Ù…ØªØ®ØµØµ ÙÙŠ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹Ø§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ©. Ù…Ù‡Ù…ØªÙƒ Ø§Ù„ÙˆØ­ÙŠØ¯Ø© Ù‡ÙŠ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø¢Ù…Ù†Ø© Ù„Ù„Ø­Ø°Ù Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆØ¯Ø¹ Ù†Ø¸ÙŠÙ Ø­ÙŠØ« ÙƒÙ„ Ù…Ù„Ù Ù…ÙØ¹Ù‘Ù„ ÙˆÙ„Ù‡ Ø¹Ù„Ø§Ù‚Ø© Ù…Ø¨Ø§Ø´Ø±Ø© Ø¨Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.

**Ø§Ù„ØªØ±ÙƒÙŠØ² Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ:**

Ø£Ù†Øª Ù…Ø·Ø§Ù„Ø¨ Ø¨ØªØ­Ù„ÙŠÙ„ ÙƒÙ„ Ù…Ù„Ù ÙˆØªØ­Ø¯ÙŠØ¯ **Ø¨Ø¯Ù‚Ø© Ø´Ø¯ÙŠØ¯Ø©** Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù†:

1. **KEEP** (Ø§Ø­ØªÙØ¸ Ø¨Ù‡) - Ù…Ù„Ù Ù†Ø´Ø· ÙˆÙ…ÙØ¹Ù‘Ù„:
   - ÙŠØªÙ… Ø§Ø³ØªÙŠØ±Ø§Ø¯Ù‡ ÙˆØ§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ ÙØ¹Ù„ÙŠØ§Ù‹ ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
   - Ù…ØªØµÙ„ Ø¨Ù€ entry points
   - Ù„Ù‡ ØªØ£Ø«ÙŠØ± Ø¹Ù„Ù‰ Ø¹Ù…Ù„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚

2. **DELETE_SAFE** (Ø§Ø­Ø°Ù Ø¨Ø£Ù…Ø§Ù†) - Ù…Ù„Ù Ø¹Ø¯ÙŠÙ… Ø§Ù„ÙØ§Ø¦Ø¯Ø©:
   - Ù„Ø§ ÙŠØªÙ… Ø§Ø³ØªÙŠØ±Ø§Ø¯Ù‡ Ù…Ù† Ø£ÙŠ Ù…Ù„Ù
   - ØºÙŠØ± Ù…ØªØµÙ„ Ø¨Ø£ÙŠ entry point
   - Ù…Ù„Ù ÙØ§Ø±Øº Ø£Ùˆ ØªØ¬Ø±ÙŠØ¨ÙŠ Ø£Ùˆ Ù…Ø¹Ø·Ù„
   - Ø£Ø³Ù…Ø§Ø¡ Ù…Ø´Ø¨ÙˆÙ‡Ø© (test, temp, backup, old, deprecated, unused)
   - exports ØºÙŠØ± Ù…Ø³ØªØ®Ø¯Ù…Ø© Ù†Ù‡Ø§Ø¦ÙŠØ§Ù‹

3. **DELETE_PROBABLY** (ØºØ§Ù„Ø¨Ø§Ù‹ Ø¢Ù…Ù† Ù„Ù„Ø­Ø°Ù) - ÙŠØ­ØªØ§Ø¬ Ù…Ø±Ø§Ø¬Ø¹Ø©:
   - Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ù„Ù‡ Ø§Ø³ØªØ®Ø¯Ø§Ù… ØºÙŠØ± ÙˆØ§Ø¶Ø­
   - Ø¨Ø¹ÙŠØ¯ Ø¬Ø¯Ø§Ù‹ Ù…Ù† entry points (> 5 Ù…Ø³ØªÙˆÙŠØ§Øª)
   - Ù„Ù… ÙŠØªÙ… ØªØ¹Ø¯ÙŠÙ„Ù‡ Ù…Ù†Ø° ÙØªØ±Ø© Ø·ÙˆÙŠÙ„Ø© (> 6 Ø´Ù‡ÙˆØ±)

4. **UNCERTAIN** (ØºÙŠØ± Ù…ØªØ£ÙƒØ¯) - Ù…Ø±Ø§Ø¬Ø¹Ø© ÙŠØ¯ÙˆÙŠØ© Ø¥Ù„Ø²Ø§Ù…ÙŠØ©:
   - Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ© Ù„Ù„Ø­ÙƒÙ…
   - Ù…Ù„ÙØ§Øª config Ø£Ùˆ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
   - Ù…Ù„ÙØ§Øª ÙƒØ¨ÙŠØ±Ø© Ù…Ø¹Ù‚Ø¯Ø©
   - Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© ØªØ£Ø«ÙŠØ± ØºÙŠØ± ÙˆØ§Ø¶Ø­Ø©

**Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…ØªØ§Ø­Ø©:**

Ø³ÙŠØªÙ… ØªØ²ÙˆÙŠØ¯Ùƒ Ø¨Ù€:
- **Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹ Ø§Ù„ÙƒØ§Ù…Ù„Ø©:** Ø¨Ù†ÙŠØ© Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª ÙˆØ§Ù„Ù…Ù„ÙØ§Øª
- **Ø´Ø¨ÙƒØ© Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ§Øª Ø§Ù„ÙƒØ§Ù…Ù„Ø©:** Ù…Ù† ÙŠØ³ØªÙˆØ±Ø¯ Ù…Ù†ØŒ ÙˆÙ…Ù† ÙŠØªÙ… Ø§Ø³ØªÙŠØ±Ø§Ø¯Ù‡ Ø¨ÙˆØ§Ø³Ø·Ø©
- **Ù†Ù‚Ø§Ø· Ø§Ù„Ø¯Ø®ÙˆÙ„ (Entry Points):** Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„ØªØ·Ø¨ÙŠÙ‚
- **Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªÙ‚Ù†ÙŠØ©:** Ø­Ø¬Ù… Ø§Ù„Ù…Ù„ÙØŒ Ø§Ù„Ø§Ù…ØªØ¯Ø§Ø¯ØŒ ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ¹Ø¯ÙŠÙ„
- **ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯ÙˆØ§Øª:** Ù†ØªØ§Ø¦Ø¬ knip, depcheck, dependency-cruiser
- **Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù„Ù Ø£Ùˆ Ù…Ù„Ø®ØµÙ‡:** Ø­Ø³Ø¨ Ø§Ù„Ø­Ø¬Ù…

**ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ (JSON ÙÙ‚Ø·):**

```json
{
  "decision": "KEEP|DELETE_SAFE|DELETE_PROBABLY|UNCERTAIN",
  "confidence": 0-100,
  "reasons": [
    "Ø³Ø¨Ø¨ Ø±Ø¦ÙŠØ³ÙŠ 1",
    "Ø³Ø¨Ø¨ Ø±Ø¦ÙŠØ³ÙŠ 2",
    "Ø³Ø¨Ø¨ Ø±Ø¦ÙŠØ³ÙŠ 3"
  ],
  "usage_analysis": {
    "is_imported": true|false,
    "import_count": 0,
    "distance_from_entry": -1|0|1|2|...,
    "has_unused_exports": true|false
  },
  "risk_assessment": {
    "deletion_safety_score": 0-100,
    "potential_impact": "none|minimal|moderate|high|critical",
    "affected_files": []
  },
  "recommendation": "Ù†Øµ Ù‚ØµÙŠØ± ÙŠØ´Ø±Ø­ Ø§Ù„Ù‚Ø±Ø§Ø± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ ÙˆØ§Ù„Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„Ù…ÙˆØµÙ‰ Ø¨Ù‡"
}
```

**Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ø­ÙƒÙ… Ø§Ù„ØµØ§Ø±Ù…Ø©:**

1. **KEEP Ø¥Ø°Ø§ ÙˆÙÙ‚Ø· Ø¥Ø°Ø§:**
   - ÙŠØªÙ… Ø§Ø³ØªÙŠØ±Ø§Ø¯Ù‡ Ø¨ÙˆØ§Ø³Ø·Ø© Ù…Ù„Ù ÙˆØ§Ø­Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ØŒ Ø£Ùˆ
   - Ù‡Ùˆ entry point Ù†ÙØ³Ù‡ØŒ Ø£Ùˆ
   - Ù…Ø³Ø§ÙØªÙ‡ Ù…Ù† entry points â‰¤ 3 Ù…Ø³ØªÙˆÙŠØ§Øª

2. **DELETE_SAFE Ø¥Ø°Ø§:**
   - Ù„Ø§ ÙŠØªÙ… Ø§Ø³ØªÙŠØ±Ø§Ø¯Ù‡ Ù†Ù‡Ø§Ø¦ÙŠØ§Ù‹ (import_count = 0)ØŒ Ùˆ
   - ØºÙŠØ± Ù…ØªØµÙ„ Ø¨Ø£ÙŠ entry point (distance = -1)ØŒ Ùˆ
   - Ø¬Ù…ÙŠØ¹ exports ØºÙŠØ± Ù…Ø³ØªØ®Ø¯Ù…Ø©ØŒ Ùˆ
   - Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø£ÙŠ ØªØ£Ø«ÙŠØ± Ø¹Ù„Ù‰ Ù…Ù„ÙØ§Øª Ø£Ø®Ø±Ù‰

3. **DELETE_PROBABLY Ø¥Ø°Ø§:**
   - import_count = 0ØŒ Ùˆ
   - distance > 5 Ø£Ùˆ -1ØŒ Ùˆ
   - confidence >= 70

4. **UNCERTAIN Ø¥Ø°Ø§:**
   - confidence < 70ØŒ Ø£Ùˆ
   - Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù†Ø§Ù‚ØµØ©ØŒ Ø£Ùˆ
   - Ù…Ù„Ù config/settingsØŒ Ø£Ùˆ
   - Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© ØªØ£Ø«ÙŠØ± ØºÙŠØ± ÙˆØ§Ø¶Ø­Ø©

**Ù‚ÙˆØ§Ø¹Ø¯ Ø­Ø§Ø³Ù…Ø©:**

- âŒ **Ù„Ø§ ØªÙƒÙ† Ù…ØªØ³Ø§Ù‡Ù„Ø§Ù‹** - Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ: Ø§Ø­Ø°Ù Ø¥Ù„Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ø¯Ù„ÙŠÙ„ ÙˆØ§Ø¶Ø­ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
- âœ… **ÙƒÙ† ØµØ§Ø±Ù…Ø§Ù‹ ÙÙŠ KEEP** - ÙÙ‚Ø· Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙØ¹Ù‘Ù„Ø© ÙØ¹Ù„ÙŠØ§Ù‹
- âš ï¸ **ÙƒÙ† Ø­Ø°Ø±Ø§Ù‹ Ù…Ø¹ UNCERTAIN** - Ø¹Ù†Ø¯ Ø£Ø¯Ù†Ù‰ Ø´ÙƒØŒ Ø¶Ø¹Ù‡ ÙÙŠ uncertain
- ğŸ“Š **Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙ‚Ø·** - Ù„Ø§ ØªØ®Ù…Ù†ØŒ Ø§Ø¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ§Øª ÙˆØ§Ù„Ø£Ø¯ÙˆØ§Øª
- ğŸ¯ **Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø§Ù„Ù‡Ø¯Ù** - Ù…Ø³ØªÙˆØ¯Ø¹ Ù†Ø¸ÙŠÙ = ØµÙØ± Ù…Ù„ÙØ§Øª ØºÙŠØ± Ù…ÙØ¹Ù‘Ù„Ø©

**Ø£Ù…Ø«Ù„Ø©:**

âŒ **Ø®Ø·Ø£:**
```json
{
  "decision": "KEEP",
  "reasons": ["Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ù…ÙÙŠØ¯Ø§Ù‹ ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„"]
}
```

âœ… **ØµØ­ÙŠØ­:**
```json
{
  "decision": "DELETE_SAFE",
  "confidence": 95,
  "reasons": [
    "import_count = 0 - Ù„Ø§ ÙŠØ³ØªÙˆØ±Ø¯Ù‡ Ø£ÙŠ Ù…Ù„Ù",
    "distance_from_entry = -1 - ØºÙŠØ± Ù…ØªØµÙ„ Ù†Ù‡Ø§Ø¦ÙŠØ§Ù‹",
    "Ø§Ù„Ù…Ù„Ù ÙØ§Ø±Øº - 0 Ø¨Ø§ÙŠØª"
  ]
}
```

**Ù…Ù„Ø§Ø­Ø¸Ø© Ù†Ù‡Ø§Ø¦ÙŠØ©:**

Ù‡Ø¯ÙÙƒ Ø§Ù„ÙˆØ­ÙŠØ¯ Ù‡Ùˆ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹. ÙƒÙ† Ø­Ø§Ø²Ù…Ø§Ù‹ ÙÙŠ Ù‚Ø±Ø§Ø±Ø§Øª Ø§Ù„Ø­Ø°Ù Ø¹Ù†Ø¯Ù…Ø§ ØªÙƒÙˆÙ† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ø¶Ø­Ø©.
"""
```

### 1.5.3 Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù€ Prompt Ù„ÙƒÙ„ Ù…Ù„Ù

```python
def build_ai_analysis_prompt(file_path, file_info, dependency_map, repo_map, entry_points):
    """
    Ø¨Ù†Ø§Ø¡ prompt Ù…Ø®ØµØµ Ù„ÙƒÙ„ Ù…Ù„Ù Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ø§Ù„Ù€ AI
    """
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ§Øª
    imports_from = dependency_map['imports'].get(file_path, [])
    imported_by = dependency_map['imported_by'].get(file_path, [])
    is_unused_export = file_path in dependency_map['unused_exports']
    
    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø³Ø§ÙØ© Ù…Ù† entry points
    distance = calculate_distance_from_entry_points(file_path, entry_points, dependency_map)
    
    # Ù‚Ø±Ø§Ø¡Ø© Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù„Ù Ø£Ùˆ Ù…Ù„Ø®ØµÙ‡
    if file_info['size_bytes'] < 50000:  # Ø£Ù‚Ù„ Ù…Ù† ~50KB
        with open(file_info['absolute_path'], 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
        content_section = f"### Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù„Ù Ø§Ù„ÙƒØ§Ù…Ù„:\n```\n{content}\n```"
    else:
        content_summary = generate_structure_summary(file_info['absolute_path'])
        content_section = f"### Ù…Ù„Ø®Øµ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ù„Ù (Ù…Ù„Ù ÙƒØ¨ÙŠØ±):\n{json.dumps(content_summary, ensure_ascii=False, indent=2)}"
    
    # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù€ prompt Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
    prompt = f"""{CLEANUP_FOCUSED_PROMPT}

---

# Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù„Ù Ù„Ù„ØªØ­Ù„ÙŠÙ„

## Ø§Ù„Ù…Ù„Ù: `{file_path}`

### Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªÙ‚Ù†ÙŠØ©:
- **Ø§Ù„Ø­Ø¬Ù…:** {file_info['size_bytes']} Ø¨Ø§ÙŠØª
- **Ø§Ù„Ø§Ù…ØªØ¯Ø§Ø¯:** {file_info['extension']}
- **Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„ÙƒØ§Ù…Ù„:** {file_info['absolute_path']}

### ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ§Øª:
- **ÙŠØ³ØªÙˆØ±Ø¯ Ù…Ù† ({len(imports_from)} Ù…Ù„Ù):** {', '.join(imports_from) if imports_from else 'Ù„Ø§ ÙŠØ³ØªÙˆØ±Ø¯ Ø´ÙŠØ¦Ø§Ù‹'}
- **ÙŠØªÙ… Ø§Ø³ØªÙŠØ±Ø§Ø¯Ù‡ Ø¨ÙˆØ§Ø³Ø·Ø© ({len(imported_by)} Ù…Ù„Ù):** {', '.join(imported_by) if imported_by else 'Ù„Ø§ ÙŠØ³ØªÙˆØ±Ø¯Ù‡ Ø£ÙŠ Ù…Ù„Ù'}
- **Ø§Ù„Ù…Ø³Ø§ÙØ© Ù…Ù† entry points:** {distance if distance != -1 else 'ØºÙŠØ± Ù…ØªØµÙ„'}
- **exports ØºÙŠØ± Ù…Ø³ØªØ®Ø¯Ù…Ø©:** {'Ù†Ø¹Ù…' if is_unused_export else 'Ù„Ø§'}

### Ù†Ù‚Ø§Ø· Ø§Ù„Ø¯Ø®ÙˆÙ„ Ù„Ù„Ù…Ø´Ø±ÙˆØ¹:
{chr(10).join(['- ' + ep for ep in entry_points])}

### Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø£Ø¯ÙˆØ§Øª:
- **Knip:** {'Ù…Ù„Ù ØºÙŠØ± Ù…Ø³ØªØ®Ø¯Ù…' if is_unused_export else 'Ù‚ÙŠØ¯ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…'}
- **Import Count:** {len(imported_by)}
- **Export Count:** {len(imports_from)}

{content_section}

---

**Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:** Ù‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ Ù‡Ø°Ø§ Ø§Ù„Ù…Ù„Ù ÙˆÙÙ‚Ø§Ù‹ Ù„Ù„Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ù…Ø°ÙƒÙˆØ±Ø© ÙˆØ£Ø±Ø¬Ø¹ JSON ÙÙ‚Ø·.
"""
    
    return prompt
```

### 1.5.4 ØªÙ†ÙÙŠØ° Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ø§Ù„Ù€ AI

```python
def analyze_files_with_ai(all_files, dependency_map, repo_map, entry_points, model):
    """
    ØªØ­Ù„ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Gemini AI
    """
    print(f"\nğŸ¤– Ø¬Ø§Ø±ÙŠ ØªØ­Ù„ÙŠÙ„ {len(all_files)} Ù…Ù„Ù Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Gemini 2.0 Flash...")
    
    ai_analysis_results = {}
    
    for i, (file_path, file_info) in enumerate(all_files.items(), 1):
        try:
            print(f"\r  [{i}/{len(all_files)}] ØªØ­Ù„ÙŠÙ„: {file_path[:50]}...", end='', flush=True)
            
            # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù€ prompt
            prompt = build_ai_analysis_prompt(
                file_path, 
                file_info, 
                dependency_map, 
                repo_map, 
                entry_points
            )
            
            # Ø¥Ø±Ø³Ø§Ù„ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬
            response = model.generate_content(
                prompt,
                generation_config={
                    'temperature': 0.1,  # Ø£Ù‚Ù„ Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ù„Ù„Ø¯Ù‚Ø©
                    'response_mime_type': 'application/json'
                }
            )
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØªÙŠØ¬Ø©
            analysis = json.loads(response.text)
            
            ai_analysis_results[file_path] = {
                'analysis': analysis,
                'file_info': file_info,
                'timestamp': datetime.datetime.now().isoformat()
            }
            
        except json.JSONDecodeError as e:
            print(f"\n  âš ï¸  Ø®Ø·Ø£ ÙÙŠ parsing JSON Ù„Ù€ {file_path}: {e}")
            ai_analysis_results[file_path] = {
                'analysis': {
                    'decision': 'UNCERTAIN',
                    'confidence': 0,
                    'reasons': [f'ÙØ´Ù„ parsing: {str(e)}'],
                    'error': True
                },
                'file_info': file_info
            }
        
        except Exception as e:
            print(f"\n  âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ {file_path}: {e}")
            ai_analysis_results[file_path] = {
                'analysis': {
                    'decision': 'UNCERTAIN',
                    'confidence': 0,
                    'reasons': [f'Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {str(e)}'],
                    'error': True
                },
                'file_info': file_info
            }
    
    print(f"\nâœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ø§Ù„Ù€ AI Ù„Ù€ {len(ai_analysis_results)} Ù…Ù„Ù")
    
    # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    save_ai_analysis_results(ai_analysis_results)
    
    return ai_analysis_results

def save_ai_analysis_results(results, output_file='ai_analysis_results.json'):
    """
    Ø­ÙØ¸ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„
    """
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙÙŠ: {output_file}")
```

### 1.5.5 ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØªØ§Ø¦Ø¬

```python
def categorize_ai_results(ai_analysis_results):
    """
    ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ù„ÙØ§Øª Ø­Ø³Ø¨ Ù‚Ø±Ø§Ø±Ø§Øª Ø§Ù„Ù€ AI
    """
    categorized = {
        'KEEP': [],
        'DELETE_SAFE': [],
        'DELETE_PROBABLY': [],
        'UNCERTAIN': [],
        'ERROR': []
    }
    
    for file_path, result in ai_analysis_results.items():
        analysis = result['analysis']
        decision = analysis.get('decision', 'UNCERTAIN')
        
        if analysis.get('error'):
            categorized['ERROR'].append({
                'path': file_path,
                'result': result
            })
        else:
            categorized[decision].append({
                'path': file_path,
                'result': result,
                'confidence': analysis.get('confidence', 0)
            })
    
    # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ø«Ù‚Ø©
    for category in ['DELETE_SAFE', 'DELETE_PROBABLY']:
        categorized[category].sort(key=lambda x: x['confidence'], reverse=True)
    
    print("\nğŸ“Š ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØªØ§Ø¦Ø¬:")
    print(f"  â”œâ”€ KEEP: {len(categorized['KEEP'])}")
    print(f"  â”œâ”€ DELETE_SAFE: {len(categorized['DELETE_SAFE'])}")
    print(f"  â”œâ”€ DELETE_PROBABLY: {len(categorized['DELETE_PROBABLY'])}")
    print(f"  â”œâ”€ UNCERTAIN: {len(categorized['UNCERTAIN'])}")
    print(f"  â””â”€ ERROR: {len(categorized['ERROR'])}")
    
    return categorized

def convert_ai_results_to_candidates(categorized_results):
    """
    ØªØ­ÙˆÙŠÙ„ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù€ AI Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ø±Ø´Ø­ÙŠÙ† Ù„Ù„Ø­Ø°Ù
    """
    candidates = {
        'safe_to_delete': [],
        'probably_unused': [],
        'uncertain': [],
        'keep': []
    }
    
    # ØªØ­ÙˆÙŠÙ„ DELETE_SAFE
    for item in categorized_results['DELETE_SAFE']:
        candidates['safe_to_delete'].append({
            'path': item['path'],
            'info': item['result']['file_info'],
            'classification': {
                'category': 'safe_to_delete',
                'reasons': item['result']['analysis'].get('reasons', []),
                'safety_score': item['result']['analysis'].get('confidence', 0),
                'risk_factors': []
            },
            'deletion_safety': item['confidence']
        })
    
    # ØªØ­ÙˆÙŠÙ„ DELETE_PROBABLY
    for item in categorized_results['DELETE_PROBABLY']:
        candidates['probably_unused'].append({
            'path': item['path'],
            'info': item['result']['file_info'],
            'classification': {
                'category': 'probably_unused',
                'reasons': item['result']['analysis'].get('reasons', []),
                'safety_score': item['result']['analysis'].get('confidence', 0),
                'risk_factors': []
            },
            'deletion_safety': item['confidence']
        })
    
    # ØªØ­ÙˆÙŠÙ„ UNCERTAIN
    for item in categorized_results['UNCERTAIN']:
        candidates['uncertain'].append({
            'path': item['path'],
            'info': item['result']['file_info'],
            'classification': {
                'category': 'uncertain',
                'reasons': item['result']['analysis'].get('reasons', []),
                'safety_score': item['result']['analysis'].get('confidence', 0),
                'risk_factors': []
            },
            'deletion_safety': item['confidence']
        })
    
    # ØªØ­ÙˆÙŠÙ„ KEEP
    for item in categorized_results['KEEP']:
        candidates['keep'].append({
            'path': item['path'],
            'info': item['result']['file_info'],
            'classification': {
                'category': 'keep',
                'reasons': item['result']['analysis'].get('reasons', []),
                'safety_score': 0,
                'risk_factors': []
            },
            'deletion_safety': 0
        })
    
    return candidates
```

---

## Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø±Ø´Ø­Ø© Ù„Ù„Ø­Ø°Ù

### 2.1 ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ù„ÙØ§Øª Ø­Ø³Ø¨ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…

```python
def identify_deletion_candidates(all_files, dependency_map, entry_points):
    """
    ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø±Ø´Ø­Ø© Ù„Ù„Ø­Ø°Ù Ø¨Ø¯Ù‚Ø©
    """
    candidates = {
        'safe_to_delete': [],      # Ø¢Ù…Ù† Ù„Ù„Ø­Ø°Ù - Ø£ÙˆÙ„ÙˆÙŠØ© Ø¹Ø§Ù„ÙŠØ©
        'probably_unused': [],     # ØºØ§Ù„Ø¨Ø§Ù‹ ØºÙŠØ± Ù…Ø³ØªØ®Ø¯Ù… - ÙŠØ­ØªØ§Ø¬ Ù…Ø±Ø§Ø¬Ø¹Ø©
        'uncertain': [],           # ØºÙŠØ± Ù…ØªØ£ÙƒØ¯ - ÙŠØ­ØªØ§Ø¬ ÙØ­Øµ ÙŠØ¯ÙˆÙŠ
        'keep': []                 # ÙŠØ¬Ø¨ Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ù‡
    }
    
    for file_path, file_info in all_files.items():
        classification = classify_file(file_path, file_info, dependency_map, entry_points)
        category = classification['category']
        
        candidates[category].append({
            'path': file_path,
            'info': file_info,
            'classification': classification,
            'deletion_safety': classification['safety_score']
        })
    
    # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø£Ù…Ø§Ù†
    for category in candidates:
        candidates[category].sort(key=lambda x: x['deletion_safety'], reverse=True)
    
    return candidates

def classify_file(file_path, file_info, dependency_map, entry_points):
    """
    ØªØµÙ†ÙŠÙ Ø¯Ù‚ÙŠÙ‚ Ù„Ù„Ù…Ù„Ù
    """
    classification = {
        'category': 'uncertain',
        'reasons': [],
        'safety_score': 0,  # 0-100 (ÙƒÙ„Ù…Ø§ Ø£Ø¹Ù„Ù‰ = Ø£ÙƒØ«Ø± Ø£Ù…Ø§Ù†Ù‹Ø§ Ù„Ù„Ø­Ø°Ù)
        'risk_factors': []
    }
    
    # 1. ÙØ­Øµ Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±
    is_imported = file_path in dependency_map['imported_by']
    has_importers = len(dependency_map['imported_by'].get(file_path, [])) > 0
    
    if not has_importers:
        classification['reasons'].append('Ù„Ø§ ÙŠØªÙ… Ø§Ø³ØªÙŠØ±Ø§Ø¯Ù‡ Ù…Ù† Ø£ÙŠ Ù…Ù„Ù')
        classification['safety_score'] += 40
    
    # 2. ÙØ­Øµ Ø§Ù„Ù…Ø³Ø§ÙØ© Ù…Ù† entry points
    distance = calculate_distance_from_entry_points(file_path, entry_points, dependency_map)
    
    if distance == -1:  # ØºÙŠØ± Ù…ØªØµÙ„ Ù†Ù‡Ø§Ø¦ÙŠÙ‹Ø§
        classification['reasons'].append('ØºÙŠØ± Ù…ØªØµÙ„ Ø¨Ø£ÙŠ entry point')
        classification['safety_score'] += 30
    elif distance > 5:  # Ø¨Ø¹ÙŠØ¯ Ø¬Ø¯Ù‹Ø§
        classification['reasons'].append(f'Ø¨Ø¹ÙŠØ¯ Ø¹Ù† entry points ({distance} Ù…Ø³ØªÙˆÙŠØ§Øª)')
        classification['safety_score'] += 10
    else:
        classification['risk_factors'].append(f'Ù‚Ø±ÙŠØ¨ Ù…Ù† entry points ({distance} Ù…Ø³ØªÙˆÙŠØ§Øª)')
        classification['safety_score'] -= 20
    
    # 3. ÙØ­Øµ exports ØºÙŠØ± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©
    if file_path in dependency_map['unused_exports']:
        classification['reasons'].append('Ø¬Ù…ÙŠØ¹ exports ØºÙŠØ± Ù…Ø³ØªØ®Ø¯Ù…Ø©')
        classification['safety_score'] += 20
    
    # 4. ÙØ­Øµ Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù
    if file_info['size_bytes'] == 0:
        classification['reasons'].append('Ù…Ù„Ù ÙØ§Ø±Øº')
        classification['safety_score'] += 10
        classification['category'] = 'safe_to_delete'
        return classification
    
    # 5. ÙØ­Øµ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…Ø´Ø¨ÙˆÙ‡Ø©
    suspicious_patterns = ['test', 'temp', 'backup', 'old', 'deprecated', 'unused', '.bak']
    if any(pattern in file_path.lower() for pattern in suspicious_patterns):
        classification['reasons'].append('Ø§Ø³Ù… Ù…Ø´Ø¨ÙˆÙ‡ ÙŠØ¯Ù„ Ø¹Ù„Ù‰ Ø¹Ø¯Ù… Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…')
        classification['safety_score'] += 15
    
    # 6. ÙØ­Øµ Ø§Ù„ØªØ§Ø±ÙŠØ® (Ø¥Ù† Ø£Ù…ÙƒÙ†)
    git_info = get_git_file_info(file_path)
    if git_info and git_info['days_since_modified'] > 180:
        classification['reasons'].append(f'Ù„Ù… ÙŠØªÙ… ØªØ¹Ø¯ÙŠÙ„Ù‡ Ù…Ù†Ø° {git_info["days_since_modified"]} ÙŠÙˆÙ…')
        classification['safety_score'] += 5
    
    # 7. Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
    if classification['safety_score'] >= 70:
        classification['category'] = 'safe_to_delete'
    elif classification['safety_score'] >= 40:
        classification['category'] = 'probably_unused'
    elif classification['safety_score'] >= 20:
        classification['category'] = 'uncertain'
    else:
        classification['category'] = 'keep'
        classification['reasons'].append('Ø§Ù„Ù…Ù„Ù Ù†Ø´Ø· ÙˆÙ…Ø³ØªØ®Ø¯Ù…')
    
    return classification

def calculate_distance_from_entry_points(file_path, entry_points, dependency_map):
    """
    Ø­Ø³Ø§Ø¨ Ø£Ù‚ØµØ± Ù…Ø³Ø§ÙØ© Ù…Ù† Ø£ÙŠ entry point
    """
    from collections import deque
    
    # BFS Ù…Ù† ÙƒÙ„ entry point
    min_distance = float('inf')
    
    for entry_point in entry_points:
        queue = deque([(entry_point, 0)])
        visited = {entry_point}
        
        while queue:
            current, distance = queue.popleft()
            
            if current == file_path:
                min_distance = min(min_distance, distance)
                break
            
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙŠ ÙŠØ³ØªÙˆØ±Ø¯Ù‡Ø§ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø­Ø§Ù„ÙŠ
            imports = dependency_map['imports'].get(current, [])
            
            for imported_file in imports:
                if imported_file not in visited:
                    visited.add(imported_file)
                    queue.append((imported_file, distance + 1))
    
    return min_distance if min_distance != float('inf') else -1
```

---

## Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ø¢Ù…Ù† Ø§Ù„Ù…ØªÙ‚Ø¯Ù…

### 3.1 ÙØ­Øµ Ø§Ù„ØªØ£Ø«ÙŠØ± Ù‚Ø¨Ù„ Ø§Ù„Ø­Ø°Ù

```python
def perform_safety_checks(candidates, dependency_map, config):
    """
    Ø¥Ø¬Ø±Ø§Ø¡ ÙØ­ÙˆØµØ§Øª Ø£Ù…Ø§Ù† Ø´Ø§Ù…Ù„Ø© Ù‚Ø¨Ù„ Ø§Ù„Ø­Ø°Ù
    """
    print("\nğŸ”’ Ø¬Ø§Ø±ÙŠ Ø¥Ø¬Ø±Ø§Ø¡ ÙØ­ÙˆØµØ§Øª Ø§Ù„Ø£Ù…Ø§Ù†...")
    
    safety_report = {
        'approved_for_deletion': [],
        'needs_review': [],
        'blocked': [],
        'warnings': []
    }
    
    # ÙØ­Øµ ÙƒÙ„ Ù…Ù„Ù Ù…Ø±Ø´Ø­ Ù„Ù„Ø­Ø°Ù
    files_to_check = candidates['safe_to_delete'] + candidates['probably_unused']
    
    for candidate in files_to_check:
        file_path = candidate['path']
        
        # 1. ÙØ­Øµ Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ§Øª Ø§Ù„Ø¹ÙƒØ³ÙŠØ©
        reverse_deps = get_reverse_dependencies(file_path, dependency_map)
        if reverse_deps:
            candidate['blocked_reason'] = f'ÙŠØªÙ… Ø§Ø³ØªÙŠØ±Ø§Ø¯Ù‡ Ù…Ù†: {", ".join(reverse_deps[:3])}'
            safety_report['blocked'].append(candidate)
            continue
        
        # 2. ÙØ­Øµ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† entry point
        if is_entry_point(file_path, config['entry_points']):
            candidate['blocked_reason'] = 'Ù…Ù„Ù entry point - Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø­Ø°ÙÙ‡'
            safety_report['blocked'].append(candidate)
            continue
        
        # 3. ÙØ­Øµ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø®Ø§ØµØ©
        if requires_manual_review(file_path, candidate):
            safety_report['needs_review'].append(candidate)
            continue
        
        # 4. Ø§Ø¬ØªØ§Ø² Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙØ­ÙˆØµØ§Øª
        safety_report['approved_for_deletion'].append(candidate)
    
    # 5. ÙØ­Øµ Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ§Øª Ø§Ù„Ø¯Ø§Ø¦Ø±ÙŠØ©
    circular = check_circular_dependencies(dependency_map)
    if circular:
        safety_report['warnings'].append(f'ØªØ­Ø°ÙŠØ±: ÙˆØ¬ÙˆØ¯ {len(circular)} Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ§Øª Ø¯Ø§Ø¦Ø±ÙŠØ©')
    
    print(f"âœ… Ø§Ù„ÙØ­ÙˆØµØ§Øª Ø§Ù„Ø£Ù…Ù†ÙŠØ© Ø§ÙƒØªÙ…Ù„Øª:")
    print(f"  â”œâ”€ Ù…ÙˆØ§ÙÙ‚ Ù„Ù„Ø­Ø°Ù: {len(safety_report['approved_for_deletion'])}")
    print(f"  â”œâ”€ ÙŠØ­ØªØ§Ø¬ Ù…Ø±Ø§Ø¬Ø¹Ø©: {len(safety_report['needs_review'])}")
    print(f"  â””â”€ Ù…Ø­Ø¸ÙˆØ±: {len(safety_report['blocked'])}")
    
    return safety_report

def requires_manual_review(file_path, candidate):
    """
    ØªØ­Ø¯ÙŠØ¯ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ù„Ù ÙŠØ­ØªØ§Ø¬ Ù…Ø±Ø§Ø¬Ø¹Ø© ÙŠØ¯ÙˆÙŠØ©
    """
    # Ù…Ù„ÙØ§Øª config Ø¯Ø§Ø¦Ù…Ø§Ù‹ ØªØ­ØªØ§Ø¬ Ù…Ø±Ø§Ø¬Ø¹Ø©
    config_extensions = ['.json', '.yaml', '.yml', '.toml', '.ini', '.env']
    if any(file_path.endswith(ext) for ext in config_extensions):
        return True
    
    # Ù…Ù„ÙØ§Øª Ø°Ø§Øª safety score Ù…ØªÙˆØ³Ø·
    if 40 <= candidate['deletion_safety'] < 70:
        return True
    
    # Ù…Ù„ÙØ§Øª ÙƒØ¨ÙŠØ±Ø© (> 500 Ø³Ø·Ø±)
    if candidate['info']['size_bytes'] > 500 * 80:  # ØªÙ‚Ø±ÙŠØ¨Ø§Ù‹ 500 Ø³Ø·Ø±
        return True
    
    return False
```

### 3.2 Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„Ø­Ø°Ù (Dry Run)

```python
def simulate_deletion(safety_report, dependency_map):
    """
    Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„Ø­Ø°Ù Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¹Ø¯Ù… ÙƒØ³Ø± Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
    """
    print("\nğŸ­ Ø¬Ø§Ø±ÙŠ Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„Ø­Ø°Ù...")
    
    simulation_results = {
        'would_break': [],
        'safe': [],
        'uncertain': []
    }
    
    approved_files = [f['path'] for f in safety_report['approved_for_deletion']]
    
    # Ø¥Ù†Ø´Ø§Ø¡ dependency map Ù…Ø¤Ù‚Øª Ø¨Ø¯ÙˆÙ† Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ø°ÙˆÙØ©
    simulated_map = simulate_dependency_removal(dependency_map, approved_files)
    
    # ÙØ­Øµ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø¹Ø¯ Ø§Ù„Ø­Ø°Ù
    for file in approved_files:
        # ÙØ­Øµ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ù…Ù„ÙØ§Øª Ø³ØªØµØ¨Ø­ Ù…Ù†ÙØµÙ„Ø© Ø¨Ø¹Ø¯ Ø­Ø°Ù Ù‡Ø°Ø§ Ø§Ù„Ù…Ù„Ù
        would_orphan = find_would_be_orphaned_files(file, simulated_map)
        
        if would_orphan:
            simulation_results['would_break'].append({
                'file': file,
                'reason': f'Ø­Ø°ÙÙ‡ Ø³ÙŠØ¬Ø¹Ù„ {len(would_orphan)} Ù…Ù„Ù Ù…Ù†ÙØµÙ„',
                'orphaned_files': would_orphan
            })
        else:
            simulation_results['safe'].append(file)
    
    print(f"âœ… Ø§Ù„Ù…Ø­Ø§ÙƒØ§Ø© Ø§ÙƒØªÙ…Ù„Øª:")
    print(f"  â”œâ”€ Ø¢Ù…Ù†: {len(simulation_results['safe'])}")
    print(f"  â””â”€ Ø®Ø·Ø±: {len(simulation_results['would_break'])}")
    
    return simulation_results
```

---

## Ø§Ù„Ù…Ø±Ø­Ù„Ø© 4: Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø© ÙˆØ§Ù„Ù…ÙˆØ§ÙÙ‚Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ©

### 4.1 ÙˆØ§Ø¬Ù‡Ø© Ù…Ø±Ø§Ø¬Ø¹Ø© ØªÙØ§Ø¹Ù„ÙŠØ©

```python
def interactive_review(safety_report, simulation_results, config):
    """
    ÙˆØ§Ø¬Ù‡Ø© ØªÙØ§Ø¹Ù„ÙŠØ© Ù„Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ù‚Ø¨Ù„ Ø§Ù„Ø­Ø°Ù
    """
    if not config['safe_mode']:
        print("âš ï¸  Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø¢Ù…Ù† Ù…Ø¹Ø·Ù„ - Ø³ÙŠØªÙ… Ø§Ù„Ø­Ø°Ù ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹")
        return safety_report['approved_for_deletion']
    
    print("\n" + "="*70)
    print("ğŸ“‹ Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø±Ø´Ø­Ø© Ù„Ù„Ø­Ø°Ù")
    print("="*70)
    
    final_approved = []
    
    # Ø¹Ø±Ø¶ Ù…Ù„Ø®Øµ
    safe_files = simulation_results['safe']
    print(f"\nğŸŸ¢ Ù…Ù„ÙØ§Øª Ø¢Ù…Ù†Ø© Ù„Ù„Ø­Ø°Ù: {len(safe_files)}")
    
    if len(safe_files) > 0:
        print("\nØ£ÙˆÙ„ 10 Ù…Ù„ÙØ§Øª:")
        for i, file_path in enumerate(safe_files[:10], 1):
            candidate = next(f for f in safety_report['approved_for_deletion'] if f['path'] == file_path)
            print(f"  {i}. {file_path}")
            print(f"     Ø§Ù„Ø³Ø¨Ø¨: {', '.join(candidate['classification']['reasons'][:2])}")
            print(f"     Ø¯Ø±Ø¬Ø© Ø§Ù„Ø£Ù…Ø§Ù†: {candidate['deletion_safety']}/100")
        
        if len(safe_files) > 10:
            print(f"  ... Ùˆ {len(safe_files) - 10} Ù…Ù„Ù Ø¢Ø®Ø±")
        
        # Ø·Ù„Ø¨ Ø§Ù„Ù…ÙˆØ§ÙÙ‚Ø©
        print("\n" + "-"*70)
        choice = input(f"\nâ“ Ù‡Ù„ ØªÙˆØ§ÙÙ‚ Ø¹Ù„Ù‰ Ø­Ø°Ù Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù€ {len(safe_files)} Ù…Ù„ÙØŸ (y/n/review): ").lower()
        
        if choice == 'y':
            final_approved = safe_files
            print(f"âœ… ØªÙ…Øª Ø§Ù„Ù…ÙˆØ§ÙÙ‚Ø© Ø¹Ù„Ù‰ Ø­Ø°Ù {len(final_approved)} Ù…Ù„Ù")
        
        elif choice == 'review':
            # Ù…Ø±Ø§Ø¬Ø¹Ø© Ù…Ù„Ù Ø¨Ù…Ù„Ù
            final_approved = detailed_file_review(safe_files, safety_report)
        
        else:
            print("âŒ ØªÙ… Ø¥Ù„ØºØ§Ø¡ Ø§Ù„Ø­Ø°Ù")
            return []
    
    # Ø¹Ø±Ø¶ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø®Ø·Ø±Ø©
    if simulation_results['would_break']:
        print(f"\nğŸ”´ Ù…Ù„ÙØ§Øª Ø®Ø·Ø±Ø© (Ù„Ù† ÙŠØªÙ… Ø­Ø°ÙÙ‡Ø§): {len(simulation_results['would_break'])}")
        for item in simulation_results['would_break'][:5]:
            print(f"  - {item['file']}")
            print(f"    Ø§Ù„Ø³Ø¨Ø¨: {item['reason']}")
    
    # Ø¹Ø±Ø¶ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙŠ ØªØ­ØªØ§Ø¬ Ù…Ø±Ø§Ø¬Ø¹Ø©
    if safety_report['needs_review']:
        print(f"\nğŸŸ¡ Ù…Ù„ÙØ§Øª ØªØ­ØªØ§Ø¬ Ù…Ø±Ø§Ø¬Ø¹Ø© ÙŠØ¯ÙˆÙŠØ©: {len(safety_report['needs_review'])}")
        review_choice = input("\nÙ‡Ù„ ØªØ±ÙŠØ¯ Ù…Ø±Ø§Ø¬Ø¹ØªÙ‡Ø§ Ø§Ù„Ø¢Ù†ØŸ (y/n): ").lower()
        
        if review_choice == 'y':
            reviewed = detailed_file_review(
                [f['path'] for f in safety_report['needs_review']], 
                safety_report
            )
            final_approved.extend(reviewed)
    
    return final_approved

def detailed_file_review(files, safety_report):
    """
    Ù…Ø±Ø§Ø¬Ø¹Ø© ØªÙØµÙŠÙ„ÙŠØ© Ù„ÙƒÙ„ Ù…Ù„Ù
    """
    approved = []
    
    print("\n" + "="*70)
    print("ğŸ” Ù…Ø±Ø§Ø¬Ø¹Ø© ØªÙØµÙŠÙ„ÙŠØ©")
    print("="*70)
    
    for i, file_path in enumerate(files, 1):
        candidate = next(f for f in safety_report['approved_for_deletion'] if f['path'] == file_path)
        
        print(f"\n[{i}/{len(files)}] {file_path}")
        print(f"  Ø§Ù„Ø­Ø¬Ù…: {candidate['info']['size_bytes']} Ø¨Ø§ÙŠØª")
        print(f"  Ø¯Ø±Ø¬Ø© Ø§Ù„Ø£Ù…Ø§Ù†: {candidate['deletion_safety']}/100")
        print(f"  Ø§Ù„Ø£Ø³Ø¨Ø§Ø¨:")
        for reason in candidate['classification']['reasons']:
            print(f"    - {reason}")
        
        if candidate['classification']['risk_factors']:
            print(f"  âš ï¸  Ø¹ÙˆØ§Ù…Ù„ Ø®Ø·Ø±:")
            for risk in candidate['classification']['risk_factors']:
                print(f"    - {risk}")
        
        choice = input(f"  Ø§Ø­Ø°Ù Ù‡Ø°Ø§ Ø§Ù„Ù…Ù„ÙØŸ (y/n/view/skip-rest): ").lower()
        
        if choice == 'y':
            approved.append(file_path)
        elif choice == 'view':
            view_file_content(candidate['info']['absolute_path'])
            # Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø³Ø¤Ø§Ù„
            if input("  Ø§Ø­Ø°ÙØŸ (y/n): ").lower() == 'y':
                approved.append(file_path)
        elif choice == 'skip-rest':
            break
    
    return approved

def view_file_content(file_path, lines=20):
    """
    Ø¹Ø±Ø¶ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù„Ù
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.readlines()
        
        print(f"\n  --- Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù„Ù (Ø£ÙˆÙ„ {lines} Ø³Ø·Ø±) ---")
        for i, line in enumerate(content[:lines], 1):
            print(f"  {i:3} | {line.rstrip()}")
        
        if len(content) > lines:
            print(f"  ... ({len(content) - lines} Ø³Ø·Ø± Ø¥Ø¶Ø§ÙÙŠ)")
        print("  " + "-"*50)
    except Exception as e:
        print(f"  âŒ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù: {e}")
```

---

## Ø§Ù„Ù…Ø±Ø­Ù„Ø© 5: Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¢Ù…Ù†

### 5.1 Ø§Ù„Ø­Ø°Ù Ø§Ù„Ù…Ø±Ø­Ù„ÙŠ

```python
def safe_deletion_execution(approved_files, config, backup_path):
    """
    ØªÙ†ÙÙŠØ° Ø§Ù„Ø­Ø°Ù Ø¨Ø´ÙƒÙ„ Ø¢Ù…Ù† ÙˆÙ…Ø±Ø­Ù„ÙŠ
    """
    print("\n" + "="*70)
    print("ğŸ—‘ï¸  Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø­Ø°Ù Ø§Ù„Ø¢Ù…Ù†")
    print("="*70)
    
    deletion_log = {
        'timestamp': datetime.datetime.now().isoformat(),
        'backup_path': str(backup_path),
        'total_files': len(approved_files),
        'deleted': [],
        'failed': [],
        'rollback_available': True
    }
    
    if config['dry_run']:
        print("\nâš ï¸  ÙˆØ¶Ø¹ Ø§Ù„Ù…Ø­Ø§ÙƒØ§Ø© (DRY RUN) - Ù„Ù† ÙŠØªÙ… Ø­Ø°Ù Ø£ÙŠ Ù…Ù„Ù ÙØ¹Ù„ÙŠØ§Ù‹")
        for file_path in approved_files:
            print(f"  [Ù…Ø­Ø§ÙƒØ§Ø©] Ø³ÙŠØªÙ… Ø­Ø°Ù: {file_path}")
            deletion_log['deleted'].append({
                'path': file_path,
                'dry_run': True
            })
        return deletion_log
    
    # Ø§Ù„Ø­Ø°Ù Ø§Ù„ÙØ¹Ù„ÙŠ
    for i, file_path in enumerate(approved_files, 1):
        try:
            print(f"\n[{i}/{len(approved_files)}] Ø­Ø°Ù: {file_path}")
            
            # 1. Ù†Ø³Ø® Ø§Ù„Ù…Ù„Ù Ø¥Ù„Ù‰ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø­Ø°Ù (Ù„Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ø³Ø±ÙŠØ¹)
            deleted_backup = backup_path / 'deleted_files' / file_path
            deleted_backup.parent.mkdir(parents=True, exist_ok=True)
            shutil.copy2(file_path, deleted_backup)
            
            # 2. Ø­Ø°Ù Ø§Ù„Ù…Ù„Ù Ø§Ù„ÙØ¹Ù„ÙŠ
            os.remove(file_path)
            
            deletion_log['deleted'].append({
                'path': file_path,
                'backup_location': str(deleted_backup),
                'timestamp': datetime.datetime.now().isoformat(),
                'status': 'success'
            })
            
            print(f"  âœ… ØªÙ… Ø§Ù„Ø­Ø°Ù")
            
        except Exception as e:
            print(f"  âŒ ÙØ´Ù„ Ø§Ù„Ø­Ø°Ù: {e}")
            deletion_log['failed'].append({
                'path': file_path,
                'error': str(e)
            })
    
    # Ø­ÙØ¸ Ø³Ø¬Ù„ Ø§Ù„Ø­Ø°Ù
    log_file = backup_path / 'deletion_log.json'
    with open(log_file, 'w', encoding='utf-8') as f:
        json.dump(deletion_log, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… Ø§ÙƒØªÙ…Ù„Øª Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø­Ø°Ù:")
    print(f"  â”œâ”€ Ù†Ø¬Ø­: {len(deletion_log['deleted'])}")
    print(f"  â”œâ”€ ÙØ´Ù„: {len(deletion_log['failed'])}")
    print(f"  â””â”€ Ø³Ø¬Ù„ Ø§Ù„Ø­Ø°Ù: {log_file}")
    
    return deletion_log
```

### 5.2 Ù†Ø¸Ø§Ù… Rollback

```python
def rollback_deletion(deletion_log):
    """
    Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ø°ÙˆÙØ© ÙÙŠ Ø­Ø§Ù„Ø© ÙˆØ¬ÙˆØ¯ Ù…Ø´ÙƒÙ„Ø©
    """
    print("\nğŸ”„ Ø¬Ø§Ø±ÙŠ Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ø°ÙˆÙØ©...")
    
    rollback_report = {
        'restored': [],
        'failed': []
    }
    
    for deleted_file in deletion_log['deleted']:
        try:
            backup_location = deleted_file['backup_location']
            original_path = deleted_file['path']
            
            # Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ù„Ù
            Path(original_path).parent.mkdir(parents=True, exist_ok=True)
            shutil.copy2(backup_location, original_path)
            
            rollback_report['restored'].append(original_path)
            print(f"  âœ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹: {original_path}")
            
        except Exception as e:
            rollback_report['failed'].append({
                'path': original_path,
                'error': str(e)
            })
            print(f"  âŒ ÙØ´Ù„ Ø§Ø³ØªØ±Ø¬Ø§Ø¹: {original_path}")
    
    print(f"\nâœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹:")
    print(f"  â”œâ”€ ØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹: {len(rollback_report['restored'])}")
    print(f"  â””â”€ ÙØ´Ù„: {len(rollback_report['failed'])}")
    
    return rollback_report
```

---

## Ø§Ù„Ù…Ø±Ø­Ù„Ø© 6: Ø§Ù„ØªØ­Ù‚Ù‚ Ø¨Ø¹Ø¯ Ø§Ù„Ø­Ø°Ù

### 6.1 Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¨Ø¹Ø¯ Ø§Ù„Ø­Ø°Ù

```python
def post_deletion_validation(repo_path, config):
    """
    Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø³Ù„Ø§Ù…Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¨Ø¹Ø¯ Ø§Ù„Ø­Ø°Ù
    """
    print("\n" + "="*70)
    print("ğŸ§ª Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø³Ù„Ø§Ù…Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚")
    print("="*70)
    
    validation_report = {
        'build_status': None,
        'tests_status': None,
        'linting_status': None,
        'issues_found': [],
        'overall_status': 'unknown'
    }
    
    # 1. ÙØ­Øµ Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹
    print("\n1ï¸âƒ£ ÙØ­Øµ Ø§Ù„Ø¨Ù†Ø§Ø¡ (Build)...")
    try:
        build_result = subprocess.run(
            ['npm', 'run', 'build'],
            cwd=repo_path,
            capture_output=True,
            text=True,
            timeout=300
        )
        
        if build_result.returncode == 0:
            validation_report['build_status'] = 'success'
            print("  âœ… Ø§Ù„Ø¨Ù†Ø§Ø¡ Ù†Ø¬Ø­")
        else:
            validation_report['build_status'] = 'failed'
            validation_report['issues_found'].append({
                'type': 'build_error',
                'message': build_result.stderr[:500]
            })
            print("  âŒ Ø§Ù„Ø¨Ù†Ø§Ø¡ ÙØ´Ù„")
            print(f"  Ø§Ù„Ø®Ø·Ø£: {build_result.stderr[:200]}")
    
    except subprocess.TimeoutExpired:
        validation_report['build_status'] = 'timeout'
        print("  â±ï¸  Ø§Ù„Ø¨Ù†Ø§Ø¡ Ø§Ø³ØªØºØ±Ù‚ ÙˆÙ‚ØªØ§Ù‹ Ø·ÙˆÙŠÙ„Ø§Ù‹")
    
    except Exception as e:
        validation_report['build_status'] = 'error'
        print(f"  âŒ Ø®Ø·Ø£: {e}")
    
    # 2. ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª
    print("\n2ï¸âƒ£ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª...")
    try:
        test_result = subprocess.run(
            ['npm', 'test'],
            cwd=repo_path,
            capture_output=True,
            text=True,
            timeout=300
        )
        
        if test_result.returncode == 0:
            validation_report['tests_status'] = 'passed'
            print("  âœ… Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ù†Ø¬Ø­Øª")
        else:
            validation_report['tests_status'] = 'failed'
            validation_report['issues_found'].append({
                'type': 'test_failure',
                'message': test_result.stderr[:500]
            })
            print("  âŒ Ø¨Ø¹Ø¶ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª ÙØ´Ù„Øª")
    
    except Exception as e:
        validation_report['tests_status'] = 'skipped'
        print(f"  â­ï¸  ØªØ®Ø·ÙŠ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª: {e}")
    
    # 3. ÙØ­Øµ Linting
    print("\n3ï¸âƒ£ ÙØ­Øµ Linting...")
    try:
        lint_result = subprocess.run(
            ['npm', 'run', 'lint'],
            cwd=repo_path,
            capture_output=True,
            text=True,
            timeout=60
        )
        
        validation_report['linting_status'] = 'passed' if lint_result.returncode == 0 else 'warnings'
        print(f"  {'âœ…' if lint_result.returncode == 0 else 'âš ï¸'} Linting")
    
    except Exception as e:
        validation_report['linting_status'] = 'skipped'
        print(f"  â­ï¸  ØªØ®Ø·ÙŠ Linting")
    
    # 4. Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
    if (validation_report['build_status'] == 'success' and 
        validation_report['tests_status'] in ['passed', 'skipped']):
        validation_report['overall_status'] = 'healthy'
        print("\nâœ… Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ÙÙŠ Ø­Ø§Ù„Ø© Ø¬ÙŠØ¯Ø©")
    else:
        validation_report['overall_status'] = 'unhealthy'
        print("\nâŒ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ÙŠØ­ØªØ§Ø¬ Ù…Ø±Ø§Ø¬Ø¹Ø©")
    
    return validation_report
```

### 6.2 Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„ÙØ´Ù„

```python
def handle_validation_failure(validation_report, deletion_log, backup_path):
    """
    Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ ÙØ´Ù„ Ø§Ù„ØªØ­Ù‚Ù‚ Ø¨Ø¹Ø¯ Ø§Ù„Ø­Ø°Ù
    """
    print("\nâš ï¸  ØªÙ… Ø§ÙƒØªØ´Ø§Ù Ù…Ø´Ø§ÙƒÙ„ Ø¨Ø¹Ø¯ Ø§Ù„Ø­Ø°Ù!")
    print("\nØ§Ù„Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©:")
    print("  1. Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª (Rollback ÙƒØ§Ù…Ù„)")
    print("  2. Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø¨Ø¹Ø¶ Ø§Ù„Ù…Ù„ÙØ§Øª (Rollback Ø¬Ø²Ø¦ÙŠ)")
    print("  3. Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ ÙŠØ¯ÙˆÙŠØ§Ù‹")
    
    choice = input("\nØ§Ø®ØªÙŠØ§Ø±Ùƒ (1/2/3): ")
    
    if choice == '1':
        print("\nğŸ”„ Ø¬Ø§Ø±ÙŠ Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª...")
        rollback_deletion(deletion_log)
        print("âœ… ØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª")
        
    elif choice == '2':
        print("\nğŸ“‹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ø°ÙˆÙØ©:")
        for i, file_info in enumerate(deletion_log['deleted'], 1):
            print(f"  {i}. {file_info['path']}")
        
        to_restore = input("\nØ£Ø¯Ø®Ù„ Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù…Ù„ÙØ§Øª Ù„Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ (Ù…ÙØµÙˆÙ„Ø© Ø¨ÙÙˆØ§ØµÙ„): ")
        indices = [int(x.strip()) - 1 for x in to_restore.split(',')]
        
        partial_rollback(deletion_log, indices)
        
    else:
        print("\nâš ï¸  ØªÙ… Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª - ÙŠÙØ±Ø¬Ù‰ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ ÙŠØ¯ÙˆÙŠØ§Ù‹")
        print(f"Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ù…ØªÙˆÙØ±Ø© ÙÙŠ: {backup_path}")
```

---

## Ø§Ù„Ù…Ø±Ø­Ù„Ø© 7: Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ø§Ù„Ø´Ø§Ù…Ù„

### 7.1 ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ‚Ø±ÙŠØ±

```python
def generate_final_report(deletion_log, validation_report, stats_before, stats_after):
    """
    ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø±ÙŠØ± Ù†Ù‡Ø§Ø¦ÙŠ Ø´Ø§Ù…Ù„
    """
    report = f"""
{'='*70}
ğŸ“Š ØªÙ‚Ø±ÙŠØ± ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹ - Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
{'='*70}

â° Ø§Ù„ØªØ§Ø±ÙŠØ®: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

{'â”€'*70}
ğŸ“ˆ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù‚Ø¨Ù„ ÙˆØ¨Ø¹Ø¯
{'â”€'*70}

Ù‚Ø¨Ù„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ:
  â€¢ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª: {stats_before['total_files']}
  â€¢ Ø§Ù„Ø­Ø¬Ù… Ø§Ù„ÙƒÙ„ÙŠ: {stats_before['total_size_mb']:.2f} MB
  â€¢ Ù…Ù„ÙØ§Øª ØºÙŠØ± Ù…Ø³ØªØ®Ø¯Ù…Ø©: {stats_before['unused_files']}

Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ:
  â€¢ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª: {stats_after['total_files']}
  â€¢ Ø§Ù„Ø­Ø¬Ù… Ø§Ù„ÙƒÙ„ÙŠ: {stats_after['total_size_mb']:.2f} MB
  â€¢ Ù…Ù„ÙØ§Øª Ù†Ø´Ø·Ø©: {stats_after['active_files']}

Ø§Ù„ØªØ­Ø³ÙŠÙ†:
  â€¢ ØªÙ… Ø­Ø°Ù: {deletion_log['total_files']} Ù…Ù„Ù
  â€¢ ØªÙ… ØªÙˆÙÙŠØ±: {stats_before['total_size_mb'] - stats_after['total_size_mb']:.2f} MB
  â€¢ Ù†Ø³Ø¨Ø© Ø§Ù„ØªÙ‚Ù„ÙŠÙ„: {(1 - stats_after['total_files']/stats_before['total_files'])*100:.1f}%

{'â”€'*70}
ğŸ—‘ï¸  ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø­Ø°Ù
{'â”€'*70}

âœ… ØªÙ… Ø­Ø°ÙÙ‡Ø§ Ø¨Ù†Ø¬Ø§Ø­: {len(deletion_log['deleted'])}
âŒ ÙØ´Ù„ Ø§Ù„Ø­Ø°Ù: {len(deletion_log['failed'])}

{'â”€'*70}
ğŸ§ª Ø­Ø§Ù„Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ
{'â”€'*70}

Ø§Ù„Ø¨Ù†Ø§Ø¡ (Build): {get_status_emoji(validation_report['build_status'])} {validation_report['build_status']}
Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª (Tests): {get_status_emoji(validation_report['tests_status'])} {validation_report['tests_status']}
Linting: {get_status_emoji(validation_report['linting_status'])} {validation_report['linting_status']}

Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø¹Ø§Ù…Ø©: {get_status_emoji(validation_report['overall_status'])} {validation_report['overall_status'].upper()}

{'â”€'*70}
ğŸ’¾ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ
{'â”€'*70}

Ø§Ù„Ù…Ø³Ø§Ø±: {deletion_log['backup_path']}
Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ø°ÙˆÙØ©: {deletion_log['backup_path']}/deleted_files/
Ø³Ø¬Ù„ Ø§Ù„Ø­Ø°Ù: {deletion_log['backup_path']}/deletion_log.json

âš ï¸  Ù…Ù„Ø§Ø­Ø¸Ø©: ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø£ÙŠ Ù…Ù„Ù Ù…Ù† Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©

{'='*70}
"""
    
    # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
    report_file = Path(deletion_log['backup_path']) / 'cleanup_report.txt'
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(report)
    print(f"\nğŸ“„ ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙÙŠ: {report_file}")
    
    return report

def get_status_emoji(status):
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ emoji Ø­Ø³Ø¨ Ø§Ù„Ø­Ø§Ù„Ø©"""
    emoji_map = {
        'success': 'âœ…',
        'passed': 'âœ…',
        'healthy': 'âœ…',
        'failed': 'âŒ',
        'unhealthy': 'âŒ',
        'warnings': 'âš ï¸',
        'skipped': 'â­ï¸',
        'timeout': 'â±ï¸',
        'unknown': 'â“'
    }
    return emoji_map.get(status, 'â“')
```

---

## Ø§Ù„Ø³ÙƒØ±ÙŠØ¨Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ø§Ù„Ù…ÙØ¬Ù…Ù‘Ø¹
```python
#!/usr/bin/env python3
"""
Ø³ÙƒØ±ÙŠØ¨Øª ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹ - Ø§Ù„Ø¥ØµØ¯Ø§Ø± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù…Ø¹ Ù…Ø³Ø§Ø± Ø¥Ø®Ø±Ø§Ø¬ Ù…Ø®ØµØµ
Ø§Ù„Ù‡Ø¯Ù: Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆØ¯Ø¹ Ù†Ø¸ÙŠÙ Ø­ÙŠØ« ÙƒÙ„ Ù…Ù„Ù Ù…ÙØ¹Ù‘Ù„ ÙˆÙ„Ù‡ Ø¹Ù„Ø§Ù‚Ø© Ø¨Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù†Ø§ØªØ¬Ø© ØªÙØ­ÙØ¸ ÙÙŠ: D:\New folder (56)
"""

import os
import sys
import json
import shutil
import subprocess
from pathlib import Path
import datetime
import json
import subprocess
from collections import deque

# ==========================================
# Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø© - Ù…Ø³Ø§Ø± Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ Ø§Ù„Ø«Ø§Ø¨Øª
# ==========================================
OUTPUT_BASE_PATH = Path("D:/New folder (56)")

def load_config(config_path='cleanup_config.json'):
    """ØªØ­Ù…ÙŠÙ„ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªÙ†Ø¸ÙŠÙ"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        # ØªØ£ÙƒÙŠØ¯ Ù…Ø³Ø§Ø± Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
        config['output_path'] = str(OUTPUT_BASE_PATH)
        return config
    except FileNotFoundError:
        print("âŒ Ù…Ù„Ù Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯. Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©.")
        return {
            'repo_path': './src',
            'ignore_patterns': ['node_modules', '.git', 'dist', 'build', '__pycache__', '.vscode', '.idea'],
            'entry_points': ['src/main.ts', 'src/index.ts', 'src/app.tsx', 'src/server.js'],
            'protected_files': ['package.json', 'tsconfig.json', '.env.example', 'README.md', '.gitignore'],
            'safe_mode': True,
            'create_backup': True,
            'dry_run': False,
            'output_path': str(OUTPUT_BASE_PATH)
        }

def create_backup(repo_path, config):
    """
    Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© ÙƒØ§Ù…Ù„Ø© Ù‚Ø¨Ù„ Ø£ÙŠ Ø­Ø°Ù ÙÙŠ D:\New folder (56)
    """
    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
    backup_name = f"backup_{timestamp}"
    backup_path = Path(config['output_path']) / backup_name
    
    print(f"ğŸ”„ Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© ÙÙŠ: {backup_path}...")
    
    try:
        OUTPUT_BASE_PATH.mkdir(parents=True, exist_ok=True)
        
        # Ù†Ø³Ø® ÙƒØ§Ù…Ù„ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹
        shutil.copytree(
            repo_path, 
            backup_path,
            ignore=shutil.ignore_patterns('node_modules', '.git', 'dist', 'build', '__pycache__')
        )
        
        # Ø­ÙØ¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù€ backup
        backup_info = {
            'timestamp': timestamp,
            'original_path': str(repo_path),
            'backup_path': str(backup_path),
            'commit_hash': get_current_commit_hash(repo_path)
        }
        
        with open(backup_path / 'BACKUP_INFO.json', 'w', encoding='utf-8') as f:
            json.dump(backup_info, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© ÙÙŠ: {backup_path}")
        return backup_path
        
    except Exception as e:
        print(f"âŒ ÙØ´Ù„ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©: {e}")
        sys.exit(1)

def get_current_commit_hash(repo_path):
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¢Ø®Ø± commit hash"""
    try:
        result = subprocess.run(
            ['git', 'rev-parse', 'HEAD'],
            cwd=repo_path,
            capture_output=True,
            text=True
        )
        return result.stdout.strip()
    except:
        return None

def collect_all_files(repo_path, ignore_patterns, config):
    """
    Ø¬Ù…Ø¹ ÙƒÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ù…Ø¹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ©
    """
    all_files = {}
    
    for root, dirs, files in os.walk(repo_path):
        # ØªØµÙÙŠØ© Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ù…Ø³ØªØ«Ù†Ø§Ø©
        dirs[:] = [d for d in dirs if d not in ignore_patterns]
        
        for file in files:
            file_path = os.path.join(root, file)
            relative_path = os.path.relpath(file_path, repo_path)
            
            # ØªØ®Ø·ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ù…ÙŠØ©
            if relative_path in config['protected_files']:
                continue
            
            all_files[relative_path] = {
                'absolute_path': file_path,
                'relative_path': relative_path,
                'extension': Path(file).suffix,
                'size_bytes': os.path.getsize(file_path),
                'is_protected': False,
                'analysis_status': 'pending'
            }
    
    return all_files

def build_complete_dependency_map(repo_path):
    """
    Ø¨Ù†Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ§Øª Ø¯Ù‚ÙŠÙ‚Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙƒÙ„ Ø§Ù„Ø£Ø¯ÙˆØ§Øª
    """
    print("ğŸ” Ø¬Ø§Ø±ÙŠ Ø¨Ù†Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ§Øª...")
    
    dependency_map = {
        'imports': {},      # Ù…Ù† ÙŠØ³ØªÙˆØ±Ø¯ Ù…Ù†
        'imported_by': {},  # Ù…Ù† ÙŠØªÙ… Ø§Ø³ØªÙŠØ±Ø§Ø¯Ù‡ Ø¨ÙˆØ§Ø³Ø·Ø©
        'unused_exports': [],
        'unused_dependencies': [],
        'circular_dependencies': []
    }
    
    # 1. dependency-cruiser - Ø§Ù„Ø£Ø¯Ù‚
    print("  â”œâ”€ ØªØ´ØºÙŠÙ„ dependency-cruiser...")
    dep_cruise_result = run_dependency_cruiser(repo_path)
    dependency_map = merge_depcruise_results(dependency_map, dep_cruise_result)
    
    # 2. madge - Ù„Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹
    print("  â”œâ”€ ØªØ´ØºÙŠÙ„ madge...")
    madge_result = run_madge(repo_path)
    dependency_map = merge_madge_results(dependency_map, madge_result)
    
    # 3. Knip - Ù„ÙƒØ´Ù Ø§Ù„Ù€ exports ØºÙŠØ± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©
    print("  â”œâ”€ ØªØ´ØºÙŠÙ„ knip...")
    knip_result = run_knip(repo_path)
    dependency_map['unused_exports'] = knip_result['unused_exports']
    
    # 4. depcheck - Ù„ÙƒØ´Ù Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ§Øª ØºÙŠØ± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©
    print("  â”œâ”€ ØªØ´ØºÙŠÙ„ depcheck...")
    depcheck_result = run_depcheck(repo_path)
    dependency_map['unused_dependencies'] = depcheck_result['unused']
    
    # 5. ts-prune (Ù„Ù„Ù€ TypeScript)
    if has_typescript_files(repo_path):
        print("  â””â”€ ØªØ´ØºÙŠÙ„ ts-prune...")
        ts_prune_result = run_ts_prune(repo_path)
        dependency_map['unused_exports'].extend(ts_prune_result)
    
    print("âœ… ØªÙ… Ø¨Ù†Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ§Øª")
    return dependency_map

def run_dependency_cruiser(repo_path):
    """ØªØ´ØºÙŠÙ„ dependency-cruiser"""
    try:
        cmd = f'depcruise --include-only "^src" --output-type json {repo_path}'
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        return json.loads(result.stdout) if result.returncode == 0 else {}
    except:
        return {}

def run_madge(repo_path):
    """ØªØ´ØºÙŠÙ„ madge"""
    try:
        cmd = f'madge --json {repo_path}'
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        return json.loads(result.stdout) if result.returncode == 0 else {}
    except:
        return {}

def run_knip(repo_path):
    """ØªØ´ØºÙŠÙ„ knip"""
    try:
        cmd = 'npx knip --reporter json'
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, cwd=repo_path)
        return json.loads(result.stdout) if result.returncode == 0 else {'unused_exports': []}
    except:
        return {'unused_exports': []}

def run_depcheck(repo_path):
    """ØªØ´ØºÙŠÙ„ depcheck"""
    try:
        cmd = 'depcheck --json'
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, cwd=repo_path)
        return json.loads(result.stdout) if result.returncode == 0 else {'unused': []}
    except:
        return {'unused': []}

def has_typescript_files(repo_path):
    """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ù„ÙØ§Øª TypeScript"""
    for root, dirs, files in os.walk(repo_path):
        if any(f.endswith('.ts') or f.endswith('.tsx') for f in files):
            return True
    return False

def run_ts_prune(repo_path):
    """ØªØ´ØºÙŠÙ„ ts-prune"""
    try:
        cmd = 'ts-prune --json'
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, cwd=repo_path)
        return json.loads(result.stdout) if result.returncode == 0 else []
    except:
        return []

def merge_depcruise_results(dependency_map, result):
    """Ø¯Ù…Ø¬ Ù†ØªØ§Ø¦Ø¬ dependency-cruiser"""
    # implementation placeholder
    return dependency_map

def merge_madge_results(dependency_map, result):
    """Ø¯Ù…Ø¬ Ù†ØªØ§Ø¦Ø¬ madge"""
    # implementation placeholder
    return dependency_map

def generate_repo_map(repo_path, ignore_patterns):
    """ØªÙˆÙ„ÙŠØ¯ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹"""
    # implementation placeholder
    return {}

def initialize_gemini():
    """
    ØªÙ‡ÙŠØ¦Ø© Ù†Ù…ÙˆØ°Ø¬ Gemini Ù„Ù„ØªØ­Ù„ÙŠÙ„
    """
    try:
        import google.generativeai as genai
        from dotenv import load_dotenv
        
        load_dotenv()
        api_key = os.getenv('GEMINI_API_KEY')
        if not api_key:
            raise ValueError("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ GEMINI_API_KEY ÙÙŠ Ù…Ù„Ù .env")
        
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.0-flash-exp')
        print("âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Gemini 2.0 Flash")
        return model
    except ImportError:
        print("âš ï¸  Ù„Ù… ÙŠØªÙ… ØªØ«Ø¨ÙŠØª Ù…ÙƒØªØ¨Ø§Øª Gemini - Ø³ÙŠØªÙ… ØªØ®Ø·ÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ")
        return None

CLEANUP_FOCUSED_PROMPT = """
**Ø§Ù„Ù‡Ø¯Ù Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ:** Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø®Ø¨ÙŠØ± Ù…ØªØ®ØµØµ ÙÙŠ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹Ø§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ©. Ù…Ù‡Ù…ØªÙƒ Ø§Ù„ÙˆØ­ÙŠØ¯Ø© Ù‡ÙŠ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø¢Ù…Ù†Ø© Ù„Ù„Ø­Ø°Ù Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆØ¯Ø¹ Ù†Ø¸ÙŠÙ Ø­ÙŠØ« ÙƒÙ„ Ù…Ù„Ù Ù…ÙØ¹Ù‘Ù„ ÙˆÙ„Ù‡ Ø¹Ù„Ø§Ù‚Ø© Ù…Ø¨Ø§Ø´Ø±Ø© Ø¨Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.

**Ø§Ù„ØªØ±ÙƒÙŠØ² Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ:**

Ø£Ù†Øª Ù…Ø·Ø§Ù„Ø¨ Ø¨ØªØ­Ù„ÙŠÙ„ ÙƒÙ„ Ù…Ù„Ù ÙˆØªØ­Ø¯ÙŠØ¯ **Ø¨Ø¯Ù‚Ø© Ø´Ø¯ÙŠØ¯Ø©** Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù†:

1. **KEEP** (Ø§Ø­ØªÙØ¸ Ø¨Ù‡) - Ù…Ù„Ù Ù†Ø´Ø· ÙˆÙ…ÙØ¹Ù‘Ù„:
   - ÙŠØªÙ… Ø§Ø³ØªÙŠØ±Ø§Ø¯Ù‡ ÙˆØ§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ ÙØ¹Ù„ÙŠØ§Ù‹ ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
   - Ù…ØªØµÙ„ Ø¨Ù€ entry points
   - Ù„Ù‡ ØªØ£Ø«ÙŠØ± Ø¹Ù„Ù‰ Ø¹Ù…Ù„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚

2. **DELETE_SAFE** (Ø§Ø­Ø°Ù Ø¨Ø£Ù…Ø§Ù†) - Ù…Ù„Ù Ø¹Ø¯ÙŠÙ… Ø§Ù„ÙØ§Ø¦Ø¯Ø©:
   - Ù„Ø§ ÙŠØªÙ… Ø§Ø³ØªÙŠØ±Ø§Ø¯Ù‡ Ù…Ù† Ø£ÙŠ Ù…Ù„Ù
   - ØºÙŠØ± Ù…ØªØµÙ„ Ø¨Ø£ÙŠ entry point
   - Ù…Ù„Ù ÙØ§Ø±Øº Ø£Ùˆ ØªØ¬Ø±ÙŠØ¨ÙŠ Ø£Ùˆ Ù…Ø¹Ø·Ù„
   - Ø£Ø³Ù…Ø§Ø¡ Ù…Ø´Ø¨ÙˆÙ‡Ø© (test, temp, backup, old, deprecated, unused)
   - exports ØºÙŠØ± Ù…Ø³ØªØ®Ø¯Ù…Ø© Ù†Ù‡Ø§Ø¦ÙŠØ§Ù‹

3. **DELETE_PROBABLY** (ØºØ§Ù„Ø¨Ø§Ù‹ Ø¢Ù…Ù† Ù„Ù„Ø­Ø°Ù) - ÙŠØ­ØªØ§Ø¬ Ù…Ø±Ø§Ø¬Ø¹Ø©:
   - Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ù„Ù‡ Ø§Ø³ØªØ®Ø¯Ø§Ù… ØºÙŠØ± ÙˆØ§Ø¶Ø­
   - Ø¨Ø¹ÙŠØ¯ Ø¬Ø¯Ø§Ù‹ Ù…Ù† entry points (> 5 Ù…Ø³ØªÙˆÙŠØ§Øª)
   - Ù„Ù… ÙŠØªÙ… ØªØ¹Ø¯ÙŠÙ„Ù‡ Ù…Ù†Ø° ÙØªØ±Ø© Ø·ÙˆÙŠÙ„Ø© (> 6 Ø´Ù‡ÙˆØ±)

4. **UNCERTAIN** (ØºÙŠØ± Ù…ØªØ£ÙƒØ¯) - Ù…Ø±Ø§Ø¬Ø¹Ø© ÙŠØ¯ÙˆÙŠØ© Ø¥Ù„Ø²Ø§Ù…ÙŠØ©:
   - Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ© Ù„Ù„Ø­ÙƒÙ…
   - Ù…Ù„ÙØ§Øª config Ø£Ùˆ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
   - Ù…Ù„ÙØ§Øª ÙƒØ¨ÙŠØ±Ø© Ù…Ø¹Ù‚Ø¯Ø©
   - Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© ØªØ£Ø«ÙŠØ± ØºÙŠØ± ÙˆØ§Ø¶Ø­Ø©

**ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ (JSON ÙÙ‚Ø·):**
```json
{
  "decision": "KEEP|DELETE_SAFE|DELETE_PROBABLY|UNCERTAIN",
  "confidence": 0-100,
  "reasons": ["Ø³Ø¨Ø¨ Ø±Ø¦ÙŠØ³ÙŠ 1", "Ø³Ø¨Ø¨ Ø±Ø¦ÙŠØ³ÙŠ 2", "Ø³Ø¨Ø¨ Ø±Ø¦ÙŠØ³ÙŠ 3"],
  "usage_analysis": {
    "is_imported": true|false,
    "import_count": 0,
    "distance_from_entry": -1|0|1|2|...,
    "has_unused_exports": true|false
  },
  "risk_assessment": {
    "deletion_safety_score": 0-100,
    "potential_impact": "none|minimal|moderate|high|critical",
    "affected_files": []
  },
  "recommendation": "Ù†Øµ Ù‚ØµÙŠØ± ÙŠØ´Ø±Ø­ Ø§Ù„Ù‚Ø±Ø§Ø± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ ÙˆØ§Ù„Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„Ù…ÙˆØµÙ‰ Ø¨Ù‡"
}
```
"""

def build_ai_analysis_prompt(file_path, file_info, dependency_map, repo_map, entry_points):
    """
    Ø¨Ù†Ø§Ø¡ prompt Ù…Ø®ØµØµ Ù„ÙƒÙ„ Ù…Ù„Ù Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ø§Ù„Ù€ AI
    """
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ§Øª
    imports_from = dependency_map['imports'].get(file_path, [])
    imported_by = dependency_map['imported_by'].get(file_path, [])
    is_unused_export = file_path in dependency_map['unused_exports']
    
    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø³Ø§ÙØ© Ù…Ù† entry points
    distance = calculate_distance_from_entry_points(file_path, entry_points, dependency_map)
    
    # Ù‚Ø±Ø§Ø¡Ø© Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù„Ù Ø£Ùˆ Ù…Ù„Ø®ØµÙ‡
    if file_info['size_bytes'] < 50000:  # Ø£Ù‚Ù„ Ù…Ù† ~50KB
        with open(file_info['absolute_path'], 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
        content_section = f"### Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù„Ù Ø§Ù„ÙƒØ§Ù…Ù„:\n```\n{content}\n```"
    else:
        content_summary = generate_structure_summary(file_info['absolute_path'])
        content_section = f"### Ù…Ù„Ø®Øµ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ù„Ù (Ù…Ù„Ù ÙƒØ¨ÙŠØ±):\n{json.dumps(content_summary, ensure_ascii=False, indent=2)}"
    
    # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù€ prompt Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
    prompt = f"""{CLEANUP_FOCUSED_PROMPT}

---

# Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù„Ù Ù„Ù„ØªØ­Ù„ÙŠÙ„

## Ø§Ù„Ù…Ù„Ù: `{file_path}`

### Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªÙ‚Ù†ÙŠØ©:
- **Ø§Ù„Ø­Ø¬Ù…:** {file_info['size_bytes']} Ø¨Ø§ÙŠØª
- **Ø§Ù„Ø§Ù…ØªØ¯Ø§Ø¯:** {file_info['extension']}
- **Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„ÙƒØ§Ù…Ù„:** {file_info['absolute_path']}

### ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ§Øª:
- **ÙŠØ³ØªÙˆØ±Ø¯ Ù…Ù† ({len(imports_from)} Ù…Ù„Ù):** {', '.join(imports_from) if imports_from else 'Ù„Ø§ ÙŠØ³ØªÙˆØ±Ø¯ Ø´ÙŠØ¦Ø§Ù‹'}
- **ÙŠØªÙ… Ø§Ø³ØªÙŠØ±Ø§Ø¯Ù‡ Ø¨ÙˆØ§Ø³Ø·Ø© ({len(imported_by)} Ù…Ù„Ù):** {', '.join(imported_by) if imported_by else 'Ù„Ø§ ÙŠØ³ØªÙˆØ±Ø¯Ù‡ Ø£ÙŠ Ù…Ù„Ù'}
- **Ø§Ù„Ù…Ø³Ø§ÙØ© Ù…Ù† entry points:** {distance if distance != -1 else 'ØºÙŠØ± Ù…ØªØµÙ„'}
- **exports ØºÙŠØ± Ù…Ø³ØªØ®Ø¯Ù…Ø©:** {'Ù†Ø¹Ù…' if is_unused_export else 'Ù„Ø§'}

### Ù†Ù‚Ø§Ø· Ø§Ù„Ø¯Ø®ÙˆÙ„ Ù„Ù„Ù…Ø´Ø±ÙˆØ¹:
{chr(10).join(['- ' + ep for ep in entry_points])}

### Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø£Ø¯ÙˆØ§Øª:
- **Knip:** {'Ù…Ù„Ù ØºÙŠØ± Ù…Ø³ØªØ®Ø¯Ù…' if is_unused_export else 'Ù‚ÙŠØ¯ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…'}
- **Import Count:** {len(imported_by)}
- **Export Count:** {len(imports_from)}

{content_section}

---

**Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:** Ù‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ Ù‡Ø°Ø§ Ø§Ù„Ù…Ù„Ù ÙˆÙÙ‚Ø§Ù‹ Ù„Ù„Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ù…Ø°ÙƒÙˆØ±Ø© ÙˆØ£Ø±Ø¬Ø¹ JSON ÙÙ‚Ø·.
"""
    
    return prompt

def analyze_files_with_ai(all_files, dependency_map, repo_map, entry_points, model, config):
    """
    ØªØ­Ù„ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Gemini AI - Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ØªÙØ­ÙØ¸ ÙÙŠ D:\New folder (56)
    """
    if not model:
        print("âš ï¸  ØªØ®Ø·ÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ - Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØºÙŠØ± Ù…ØªØ§Ø­")
        return {}
    
    print(f"\nğŸ¤– Ø¬Ø§Ø±ÙŠ ØªØ­Ù„ÙŠÙ„ {len(all_files)} Ù…Ù„Ù Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Gemini 2.0 Flash...")
    
    ai_analysis_results = {}
    
    for i, (file_path, file_info) in enumerate(all_files.items(), 1):
        try:
            print(f"\r  [{i}/{len(all_files)}] ØªØ­Ù„ÙŠÙ„: {file_path[:50]}...", end='', flush=True)
            
            # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù€ prompt
            prompt = build_ai_analysis_prompt(
                file_path, 
                file_info, 
                dependency_map, 
                repo_map, 
                entry_points
            )
            
            # Ø¥Ø±Ø³Ø§Ù„ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬
            response = model.generate_content(
                prompt,
                generation_config={
                    'temperature': 0.1,  # Ø£Ù‚Ù„ Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ù„Ù„Ø¯Ù‚Ø©
                    'response_mime_type': 'application/json'
                }
            )
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØªÙŠØ¬Ø©
            analysis = json.loads(response.text)
            
            ai_analysis_results[file_path] = {
                'analysis': analysis,
                'file_info': file_info,
                'timestamp': datetime.datetime.now().isoformat()
            }
            
        except json.JSONDecodeError as e:
            print(f"\n  âš ï¸  Ø®Ø·Ø£ ÙÙŠ parsing JSON Ù„Ù€ {file_path}: {e}")
            ai_analysis_results[file_path] = {
                'analysis': {
                    'decision': 'UNCERTAIN',
                    'confidence': 0,
                    'reasons': [f'ÙØ´Ù„ parsing: {str(e)}'],
                    'error': True
                },
                'file_info': file_info
            }
        
        except Exception as e:
            print(f"\n  âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ {file_path}: {e}")
            ai_analysis_results[file_path] = {
                'analysis': {
                    'decision': 'UNCERTAIN',
                    'confidence': 0,
                    'reasons': [f'Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {str(e)}'],
                    'error': True
                },
                'file_info': file_info
            }
    
    print(f"\nâœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ø§Ù„Ù€ AI Ù„Ù€ {len(ai_analysis_results)} Ù…Ù„Ù")
    
    # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ D:\New folder (56)
    save_ai_analysis_results(ai_analysis_results, config)
    
    return ai_analysis_results

def save_ai_analysis_results(results, config, output_file='ai_analysis_results.json'):
    """
    Ø­ÙØ¸ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙÙŠ D:\New folder (56)
    """
    output_path = Path(config['output_path']) / output_file
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙÙŠ: {output_path}")

def categorize_ai_results(ai_analysis_results):
    """
    ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ù„ÙØ§Øª Ø­Ø³Ø¨ Ù‚Ø±Ø§Ø±Ø§Øª Ø§Ù„Ù€ AI
    """
    categorized = {
        'KEEP': [],
        'DELETE_SAFE': [],
        'DELETE_PROBABLY': [],
        'UNCERTAIN': [],
        'ERROR': []
    }
    
    for file_path, result in ai_analysis_results.items():
        analysis = result['analysis']
        decision = analysis.get('decision', 'UNCERTAIN')
        
        if analysis.get('error'):
            categorized['ERROR'].append({
                'path': file_path,
                'result': result
            })
        else:
            categorized[decision].append({
                'path': file_path,
                'result': result,
                'confidence': analysis.get('confidence', 0)
            })
    
    # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ø«Ù‚Ø©
    for category in ['DELETE_SAFE', 'DELETE_PROBABLY']:
        categorized[category].sort(key=lambda x: x['confidence'], reverse=True)
    
    print("\nğŸ“Š ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØªØ§Ø¦Ø¬:")
    print(f"  â”œâ”€ KEEP: {len(categorized['KEEP'])}")
    print(f"  â”œâ”€ DELETE_SAFE: {len(categorized['DELETE_SAFE'])}")
    print(f"  â”œâ”€ DELETE_PROBABLY: {len(categorized['DELETE_PROBABLY'])}")
    print(f"  â”œâ”€ UNCERTAIN: {len(categorized['UNCERTAIN'])}")
    print(f"  â””â”€ ERROR: {len(categorized['ERROR'])}")
    
    return categorized

def convert_ai_results_to_candidates(categorized_results):
    """
    ØªØ­ÙˆÙŠÙ„ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù€ AI Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ø±Ø´Ø­ÙŠÙ† Ù„Ù„Ø­Ø°Ù
    """
    candidates = {
        'safe_to_delete': [],
        'probably_unused': [],
        'uncertain': [],
        'keep': []
    }
    
    # ØªØ­ÙˆÙŠÙ„ DELETE_SAFE
    for item in categorized_results['DELETE_SAFE']:
        candidates['safe_to_delete'].append({
            'path': item['path'],
            'info': item['result']['file_info'],
            'classification': {
                'category': 'safe_to_delete',
                'reasons': item['result']['analysis'].get('reasons', []),
                'safety_score': item['result']['analysis'].get('confidence', 0),
                'risk_factors': []
            },
            'deletion_safety': item['confidence']
        })
    
    # ØªØ­ÙˆÙŠÙ„ DELETE_PROBABLY
    for item in categorized_results['DELETE_PROBABLY']:
        candidates['probably_unused'].append({
            'path': item['path'],
            'info': item['result']['file_info'],
            'classification': {
                'category': 'probably_unused',
                'reasons': item['result']['analysis'].get('reasons', []),
                'safety_score': item['result']['analysis'].get('confidence', 0),
                'risk_factors': []
            },
            'deletion_safety': item['confidence']
        })
    
    # ØªØ­ÙˆÙŠÙ„ UNCERTAIN
    for item in categorized_results['UNCERTAIN']:
        candidates['uncertain'].append({
            'path': item['path'],
            'info': item['result']['file_info'],
            'classification': {
                'category': 'uncertain',
                'reasons': item['result']['analysis'].get('reasons', []),
                'safety_score': item['result']['analysis'].get('confidence', 0),
                'risk_factors': []
            },
            'deletion_safety': item['confidence']
        })
    
    # ØªØ­ÙˆÙŠÙ„ KEEP
    for item in categorized_results['KEEP']:
        candidates['keep'].append({
            'path': item['path'],
            'info': item['result']['file_info'],
            'classification': {
                'category': 'keep',
                'reasons': item['result']['analysis'].get('reasons', []),
                'safety_score': 0,
                'risk_factors': []
            },
            'deletion_safety': 0
        })
    
    return candidates

def identify_deletion_candidates(all_files, dependency_map, entry_points):
    """
    ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø±Ø´Ø­Ø© Ù„Ù„Ø­Ø°Ù Ø¨Ø¯Ù‚Ø©
    """
    candidates = {
        'safe_to_delete': [],      # Ø¢Ù…Ù† Ù„Ù„Ø­Ø°Ù - Ø£ÙˆÙ„ÙˆÙŠØ© Ø¹Ø§Ù„ÙŠØ©
        'probably_unused': [],     # ØºØ§Ù„Ø¨Ø§Ù‹ ØºÙŠØ± Ù…Ø³ØªØ®Ø¯Ù… - ÙŠØ­ØªØ§Ø¬ Ù…Ø±Ø§Ø¬Ø¹Ø©
        'uncertain': [],           # ØºÙŠØ± Ù…ØªØ£ÙƒØ¯ - ÙŠØ­ØªØ§Ø¬ ÙØ­Øµ ÙŠØ¯ÙˆÙŠ
        'keep': []                 # ÙŠØ¬Ø¨ Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ù‡
    }
    
    for file_path, file_info in all_files.items():
        classification = classify_file(file_path, file_info, dependency_map, entry_points)
        category = classification['category']
        
        candidates[category].append({
            'path': file_path,
            'info': file_info,
            'classification': classification,
            'deletion_safety': classification['safety_score']
        })
    
    # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø£Ù…Ø§Ù†
    for category in candidates:
        candidates[category].sort(key=lambda x: x['deletion_safety'], reverse=True)
    
    return candidates

def classify_file(file_path, file_info, dependency_map, entry_points):
    """
    ØªØµÙ†ÙŠÙ Ø¯Ù‚ÙŠÙ‚ Ù„Ù„Ù…Ù„Ù
    """
    classification = {
        'category': 'uncertain',
        'reasons': [],
        'safety_score': 0,  # 0-100 (ÙƒÙ„Ù…Ø§ Ø£Ø¹Ù„Ù‰ = Ø£ÙƒØ«Ø± Ø£Ù…Ø§Ù†Ù‹Ø§ Ù„Ù„Ø­Ø°Ù)
        'risk_factors': []
    }
    
    # 1. ÙØ­Øµ Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±
    is_imported = file_path in dependency_map['imported_by']
    has_importers = len(dependency_map['imported_by'].get(file_path, [])) > 0
    
    if not has_importers:
        classification['reasons'].append('Ù„Ø§ ÙŠØªÙ… Ø§Ø³ØªÙŠØ±Ø§Ø¯Ù‡ Ù…Ù† Ø£ÙŠ Ù…Ù„Ù')
        classification['safety_score'] += 40
    
    # 2. ÙØ­Øµ Ø§Ù„Ù…Ø³Ø§ÙØ© Ù…Ù† entry points
    distance = calculate_distance_from_entry_points(file_path, entry_points, dependency_map)
    
    if distance == -1:  # ØºÙŠØ± Ù…ØªØµÙ„ Ù†Ù‡Ø§Ø¦ÙŠÙ‹Ø§
        classification['reasons'].append('ØºÙŠØ± Ù…ØªØµÙ„ Ø¨Ø£ÙŠ entry point')
        classification['safety_score'] += 30
    elif distance > 5:  # Ø¨Ø¹ÙŠØ¯ Ø¬Ø¯Ù‹Ø§
        classification['reasons'].append(f'Ø¨Ø¹ÙŠØ¯ Ø¹Ù† entry points ({distance} Ù…Ø³ØªÙˆÙŠØ§Øª)')
        classification['safety_score'] += 10
    else:
        classification['risk_factors'].append(f'Ù‚Ø±ÙŠØ¨ Ù…Ù† entry points ({distance} Ù…Ø³ØªÙˆÙŠØ§Øª)')
        classification['safety_score'] -= 20
    
    # 3. ÙØ­Øµ exports ØºÙŠØ± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©
    if file_path in dependency_map['unused_exports']:
        classification['reasons'].append('Ø¬Ù…ÙŠØ¹ exports ØºÙŠØ± Ù…Ø³ØªØ®Ø¯Ù…Ø©')
        classification['safety_score'] += 20
    
    # 4. ÙØ­Øµ Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù
    if file_info['size_bytes'] == 0:
        classification['reasons'].append('Ù…Ù„Ù ÙØ§Ø±Øº')
        classification['safety_score'] += 10
        classification['category'] = 'safe_to_delete'
        return classification
    
    # 5. ÙØ­Øµ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…Ø´Ø¨ÙˆÙ‡Ø©
    suspicious_patterns = ['test', 'temp', 'backup', 'old', 'deprecated', 'unused', '.bak']
    if any(pattern in file_path.lower() for pattern in suspicious_patterns):
        classification['reasons'].append('Ø§Ø³Ù… Ù…Ø´Ø¨ÙˆÙ‡ ÙŠØ¯Ù„ Ø¹Ù„Ù‰ Ø¹Ø¯Ù… Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…')
        classification['safety_score'] += 15
    
    # 6. ÙØ­Øµ Ø§Ù„ØªØ§Ø±ÙŠØ® (Ø¥Ù† Ø£Ù…ÙƒÙ†)
    git_info = get_git_file_info(file_path)
    if git_info and git_info.get('days_since_modified', 0) > 180:
        classification['reasons'].append(f'Ù„Ù… ÙŠØªÙ… ØªØ¹Ø¯ÙŠÙ„Ù‡ Ù…Ù†Ø° {git_info["days_since_modified"]} ÙŠÙˆÙ…')
        classification['safety_score'] += 5
    
    # 7. Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
    if classification['safety_score'] >= 70:
        classification['category'] = 'safe_to_delete'
    elif classification['safety_score'] >= 40:
        classification['category'] = 'probably_unused'
    elif classification['safety_score'] >= 20:
        classification['category'] = 'uncertain'
    else:
        classification['category'] = 'keep'
        classification['reasons'].append('Ø§Ù„Ù…Ù„Ù Ù†Ø´Ø· ÙˆÙ…Ø³ØªØ®Ø¯Ù…')
    
    return classification

def calculate_distance_from_entry_points(file_path, entry_points, dependency_map):
    """
    Ø­Ø³Ø§Ø¨ Ø£Ù‚ØµØ± Ù…Ø³Ø§ÙØ© Ù…Ù† Ø£ÙŠ entry point
    """
    # BFS Ù…Ù† ÙƒÙ„ entry point
    min_distance = float('inf')
    
    for entry_point in entry_points:
        queue = deque([(entry_point, 0)])
        visited = {entry_point}
        
        while queue:
            current, distance = queue.popleft()
            
            if current == file_path:
                min_distance = min(min_distance, distance)
                break
            
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙŠ ÙŠØ³ØªÙˆØ±Ø¯Ù‡Ø§ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø­Ø§Ù„ÙŠ
            imports = dependency_map['imports'].get(current, [])
            
            for imported_file in imports:
                if imported_file not in visited:
                    visited.add(imported_file)
                    queue.append((imported_file, distance + 1))
    
    return min_distance if min_distance != float('inf') else -1

def get_git_file_info(file_path):
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª git Ù„Ù„Ù…Ù„Ù"""
    try:
        result = subprocess.run(
            ['git', 'log', '-1', '--format=%at', file_path],
            capture_output=True,
            text=True
        )
        if result.returncode == 0 and result.stdout.strip():
            import time
            timestamp = int(result.stdout.strip())
            days_since = (time.time() - timestamp) / (86400)
            return {'days_since_modified': int(days_since)}
    except:
        pass
    return None

def get_reverse_dependencies(file_path, dependency_map):
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ§Øª Ø§Ù„Ø¹ÙƒØ³ÙŠØ©"""
    return dependency_map['imported_by'].get(file_path, [])

def is_entry_point(file_path, entry_points):
    """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ù„Ù entry point"""
    return file_path in entry_points

def requires_manual_review(file_path, candidate):
    """ØªØ­Ø¯ÙŠØ¯ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ù„Ù ÙŠØ­ØªØ§Ø¬ Ù…Ø±Ø§Ø¬Ø¹Ø© ÙŠØ¯ÙˆÙŠØ©"""
    # Ù…Ù„ÙØ§Øª config Ø¯Ø§Ø¦Ù…Ø§Ù‹ ØªØ­ØªØ§Ø¬ Ù…Ø±Ø§Ø¬Ø¹Ø©
    config_extensions = ['.json', '.yaml', '.yml', '.toml', '.ini', '.env']
    if any(file_path.endswith(ext) for ext in config_extensions):
        return True
    
    # Ù…Ù„ÙØ§Øª Ø°Ø§Øª safety score Ù…ØªÙˆØ³Ø·
    if 40 <= candidate['deletion_safety'] < 70:
        return True
    
    # Ù…Ù„ÙØ§Øª ÙƒØ¨ÙŠØ±Ø© (> 500 Ø³Ø·Ø±)
    if candidate['info']['size_bytes'] > 500 * 80:  # ØªÙ‚Ø±ÙŠØ¨Ø§Ù‹ 500 Ø³Ø·Ø±
        return True
    
    return False

def check_circular_dependencies(dependency_map):
    """ÙØ­Øµ Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ§Øª Ø§Ù„Ø¯Ø§Ø¦Ø±ÙŠØ©"""
    # implementation placeholder
    return []

def simulate_dependency_removal(dependency_map, files_to_remove):
    """Ù…Ø­Ø§ÙƒØ§Ø© Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ§Øª"""
    # implementation placeholder
    return dependency_map

def find_would_be_orphaned_files(file_path, simulated_map):
    """Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙŠ Ø³ØªØµØ¨Ø­ Ù…Ù†ÙØµÙ„Ø©"""
    # implementation placeholder
    return []

def perform_safety_checks(candidates, dependency_map, config):
    """
    Ø¥Ø¬Ø±Ø§Ø¡ ÙØ­ÙˆØµØ§Øª Ø£Ù…Ø§Ù† Ø´Ø§Ù…Ù„Ø© Ù‚Ø¨Ù„ Ø§Ù„Ø­Ø°Ù
    """
    print("\nğŸ”’ Ø¬Ø§Ø±ÙŠ Ø¥Ø¬Ø±Ø§Ø¡ ÙØ­ÙˆØµØ§Øª Ø§Ù„Ø£Ù…Ø§Ù†...")
    
    safety_report = {
        'approved_for_deletion': [],
        'needs_review': [],
        'blocked': [],
        'warnings': []
    }
    
    # ÙØ­Øµ ÙƒÙ„ Ù…Ù„Ù Ù…Ø±Ø´Ø­ Ù„Ù„Ø­Ø°Ù
    files_to_check = candidates['safe_to_delete'] + candidates['probably_unused']
    
    for candidate in files_to_check:
        file_path = candidate['path']
        
        # 1. ÙØ­Øµ Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ§Øª Ø§Ù„Ø¹ÙƒØ³ÙŠØ©
        reverse_deps = get_reverse_dependencies(file_path, dependency_map)
        if reverse_deps:
            candidate['blocked_reason'] = f'ÙŠØªÙ… Ø§Ø³ØªÙŠØ±Ø§Ø¯Ù‡ Ù…Ù†: {", ".join(reverse_deps[:3])}'
            safety_report['blocked'].append(candidate)
            continue
        
        # 2. ÙØ­Øµ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† entry point
        if is_entry_point(file_path, config['entry_points']):
            candidate['blocked_reason'] = 'Ù…Ù„Ù entry point - Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø­Ø°ÙÙ‡'
            safety_report['blocked'].append(candidate)
            continue
        
        # 3. ÙØ­Øµ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø®Ø§ØµØ©
        if requires_manual_review(file_path, candidate):
            safety_report['needs_review'].append(candidate)
            continue
        
        # 4. Ø§Ø¬ØªØ§Ø² Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙØ­ÙˆØµØ§Øª
        safety_report['approved_for_deletion'].append(candidate)
    
    # 5. ÙØ­Øµ Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ§Øª Ø§Ù„Ø¯Ø§Ø¦Ø±ÙŠØ©
    circular = check_circular_dependencies(dependency_map)
    if circular:
        safety_report['warnings'].append(f'ØªØ­Ø°ÙŠØ±: ÙˆØ¬ÙˆØ¯ {len(circular)} Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ§Øª Ø¯Ø§Ø¦Ø±ÙŠØ©')
    
    print(f"âœ… Ø§Ù„ÙØ­ÙˆØµØ§Øª Ø§Ù„Ø£Ù…Ù†ÙŠØ© Ø§ÙƒØªÙ…Ù„Øª:")
    print(f"  â”œâ”€ Ù…ÙˆØ§ÙÙ‚ Ù„Ù„Ø­Ø°Ù: {len(safety_report['approved_for_deletion'])}")
    print(f"  â”œâ”€ ÙŠØ­ØªØ§Ø¬ Ù…Ø±Ø§Ø¬Ø¹Ø©: {len(safety_report['needs_review'])}")
    print(f"  â””â”€ Ù…Ø­Ø¸ÙˆØ±: {len(safety_report['blocked'])}")
    
    return safety_report

def simulate_deletion(safety_report, dependency_map):
    """
    Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„Ø­Ø°Ù Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¹Ø¯Ù… ÙƒØ³Ø± Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
    """
    print("\nğŸ­ Ø¬Ø§Ø±ÙŠ Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„Ø­Ø°Ù...")
    
    simulation_results = {
        'would_break': [],
        'safe': [],
        'uncertain': []
    }
    
    approved_files = [f['path'] for f in safety_report['approved_for_deletion']]
    
    # Ø¥Ù†Ø´Ø§Ø¡ dependency map Ù…Ø¤Ù‚Øª Ø¨Ø¯ÙˆÙ† Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ø°ÙˆÙØ©
    simulated_map = simulate_dependency_removal(dependency_map, approved_files)
    
    # ÙØ­Øµ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø¹Ø¯ Ø§Ù„Ø­Ø°Ù
    for file in approved_files:
        # ÙØ­Øµ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ù…Ù„ÙØ§Øª Ø³ØªØµØ¨Ø­ Ù…Ù†ÙØµÙ„Ø© Ø¨Ø¹Ø¯ Ø­Ø°Ù Ù‡Ø°Ø§ Ø§Ù„Ù…Ù„Ù
        would_orphan = find_would_be_orphaned_files(file, simulated_map)
        
        if would_orphan:
            simulation_results['would_break'].append({
                'file': file,
                'reason': f'Ø­Ø°ÙÙ‡ Ø³ÙŠØ¬Ø¹Ù„ {len(would_orphan)} Ù…Ù„Ù Ù…Ù†ÙØµÙ„',
                'orphaned_files': would_orphan
            })
        else:
            simulation_results['safe'].append(file)
    
    print(f"âœ… Ø§Ù„Ù…Ø­Ø§ÙƒØ§Ø© Ø§ÙƒØªÙ…Ù„Øª:")
    print(f"  â”œâ”€ Ø¢Ù…Ù†: {len(simulation_results['safe'])}")
    print(f"  â””â”€ Ø®Ø·Ø±: {len(simulation_results['would_break'])}")
    
    return simulation_results

def interactive_review(safety_report, simulation_results, config):
    """
    ÙˆØ§Ø¬Ù‡Ø© ØªÙØ§Ø¹Ù„ÙŠØ© Ù„Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ù‚Ø¨Ù„ Ø§Ù„Ø­Ø°Ù
    """
    if not config['safe_mode']:
        print("âš ï¸  Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø¢Ù…Ù† Ù…Ø¹Ø·Ù„ - Ø³ÙŠØªÙ… Ø§Ù„Ø­Ø°Ù ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹")
        return safety_report['approved_for_deletion']
    
    print("\n" + "="*70)
    print("ğŸ“‹ Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø±Ø´Ø­Ø© Ù„Ù„Ø­Ø°Ù")
    print("="*70)
    
    final_approved = []
    
    # Ø¹Ø±Ø¶ Ù…Ù„Ø®Øµ
    safe_files = simulation_results['safe']
    print(f"\nğŸŸ¢ Ù…Ù„ÙØ§Øª Ø¢Ù…Ù†Ø© Ù„Ù„Ø­Ø°Ù: {len(safe_files)}")
    
    if len(safe_files) > 0:
        print("\nØ£ÙˆÙ„ 10 Ù…Ù„ÙØ§Øª:")
        for i, file_path in enumerate(safe_files[:10], 1):
            candidate = next(f for f in safety_report['approved_for_deletion'] if f['path'] == file_path)
            print(f"  {i}. {file_path}")
            print(f"     Ø§Ù„Ø³Ø¨Ø¨: {', '.join(candidate['classification']['reasons'][:2])}")
            print(f"     Ø¯Ø±Ø¬Ø© Ø§Ù„Ø£Ù…Ø§Ù†: {candidate['deletion_safety']}/100")
        
        if len(safe_files) > 10:
            print(f"  ... Ùˆ {len(safe_files) - 10} Ù…Ù„Ù Ø¢Ø®Ø±")
        
        # Ø·Ù„Ø¨ Ø§Ù„Ù…ÙˆØ§ÙÙ‚Ø©
        print("\n" + "-"*70)
        choice = input(f"\nâ“ Ù‡Ù„ ØªÙˆØ§ÙÙ‚ Ø¹Ù„Ù‰ Ø­Ø°Ù Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù€ {len(safe_files)} Ù…Ù„ÙØŸ (y/n/review): ").lower()
        
        if choice == 'y':
            final_approved = safe_files
            print(f"âœ… ØªÙ…Øª Ø§Ù„Ù…ÙˆØ§ÙÙ‚Ø© Ø¹Ù„Ù‰ Ø­Ø°Ù {len(final_approved)} Ù…Ù„Ù")
        
        elif choice == 'review':
            # Ù…Ø±Ø§Ø¬Ø¹Ø© Ù…Ù„Ù Ø¨Ù…Ù„Ù
            final_approved = detailed_file_review(safe_files, safety_report)
        
        else:
            print("âŒ ØªÙ… Ø¥Ù„ØºØ§Ø¡ Ø§Ù„Ø­Ø°Ù")
            return []
    
    # Ø¹Ø±Ø¶ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø®Ø·Ø±Ø©
    if simulation_results['would_break']:
        print(f"\nğŸ”´ Ù…Ù„ÙØ§Øª Ø®Ø·Ø±Ø© (Ù„Ù† ÙŠØªÙ… Ø­Ø°ÙÙ‡Ø§): {len(simulation_results['would_break'])}")
        for item in simulation_results['would_break'][:5]:
            print(f"  - {item['file']}")
            print(f"    Ø§Ù„Ø³Ø¨Ø¨: {item['reason']}")
    
    # Ø¹Ø±Ø¶ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙŠ ØªØ­ØªØ§Ø¬ Ù…Ø±Ø§Ø¬Ø¹Ø©
    if safety_report['needs_review']:
        print(f"\nğŸŸ¡ Ù…Ù„ÙØ§Øª ØªØ­ØªØ§Ø¬ Ù…Ø±Ø§Ø¬Ø¹Ø© ÙŠØ¯ÙˆÙŠØ©: {len(safety_report['needs_review'])}")
        review_choice = input("\nÙ‡Ù„ ØªØ±ÙŠØ¯ Ù…Ø±Ø§Ø¬Ø¹ØªÙ‡Ø§ Ø§Ù„Ø¢Ù†ØŸ (y/n): ").lower()
        
        if review_choice == 'y':
            reviewed = detailed_file_review(
                [f['path'] for f in safety_report['needs_review']], 
                safety_report
            )
            final_approved.extend(reviewed)
    
    return final_approved

def detailed_file_review(files, safety_report):
    """
    Ù…Ø±Ø§Ø¬Ø¹Ø© ØªÙØµÙŠÙ„ÙŠØ© Ù„ÙƒÙ„ Ù…Ù„Ù
    """
    approved = []
    
    print("\n" + "="*70)
    print("ğŸ” Ù…Ø±Ø§Ø¬Ø¹Ø© ØªÙØµÙŠÙ„ÙŠØ©")
    print("="*70)
    
    for i, file_path in enumerate(files, 1):
        candidate = next(f for f in safety_report['approved_for_deletion'] if f['path'] == file_path)
        
        print(f"\n[{i}/{len(files)}] {file_path}")
        print(f"  Ø§Ù„Ø­Ø¬Ù…: {candidate['info']['size_bytes']} Ø¨Ø§ÙŠØª")
        print(f"  Ø¯Ø±Ø¬Ø© Ø§Ù„Ø£Ù…Ø§Ù†: {candidate['deletion_safety']}/100")
        print(f"  Ø§Ù„Ø£Ø³Ø¨Ø§Ø¨:")
        for reason in candidate['classification']['reasons']:
            print(f"    - {reason}")
        
        if candidate['classification']['risk_factors']:
            print(f"  âš ï¸  Ø¹ÙˆØ§Ù…Ù„ Ø®Ø·Ø±:")
            for risk in candidate['classification']['risk_factors']:
                print(f"    - {risk}")
        
        choice = input(f"  Ø§Ø­Ø°Ù Ù‡Ø°Ø§ Ø§Ù„Ù…Ù„ÙØŸ (y/n/view/skip-rest): ").lower()
        
        if choice == 'y':
            approved.append(file_path)
        elif choice == 'view':
            view_file_content(candidate['info']['absolute_path'])
            # Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø³Ø¤Ø§Ù„
            if input("  Ø§Ø­Ø°ÙØŸ (y/n): ").lower() == 'y':
                approved.append(file_path)
        elif choice == 'skip-rest':
            break
    
    return approved

def view_file_content(file_path, lines=20):
    """
    Ø¹Ø±Ø¶ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù„Ù
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.readlines()
        
        print(f"\n  --- Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù„Ù (Ø£ÙˆÙ„ {lines} Ø³Ø·Ø±) ---")
        for i, line in enumerate(content[:lines], 1):
            print(f"  {i:3} | {line.rstrip()}")
        
        if len(content) > lines:
            print(f"  ... ({len(content) - lines} Ø³Ø·Ø± Ø¥Ø¶Ø§ÙÙŠ)")
        print("  " + "-"*50)
    except Exception as e:
        print(f"  âŒ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù: {e}")

def safe_deletion_execution(approved_files, config, backup_path):
    """
    ØªÙ†ÙÙŠØ° Ø§Ù„Ø­Ø°Ù Ø¨Ø´ÙƒÙ„ Ø¢Ù…Ù† ÙˆÙ…Ø±Ø­Ù„ÙŠ - Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ø°ÙˆÙØ© ØªÙÙ†Ø³Ø® Ø§Ø­ØªÙŠØ§Ø·ÙŠØ§Ù‹ ÙÙŠ D:\New folder (56)
    """
    print("\n" + "="*70)
    print("ğŸ—‘ï¸  Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø­Ø°Ù Ø§Ù„Ø¢Ù…Ù†")
    print("="*70)
    
    deletion_log = {
        'timestamp': datetime.datetime.now().isoformat(),
        'backup_path': str(backup_path),
        'total_files': len(approved_files),
        'deleted': [],
        'failed': [],
        'rollback_available': True
    }
    
    if config['dry_run']:
        print("\nâš ï¸  ÙˆØ¶Ø¹ Ø§Ù„Ù…Ø­Ø§ÙƒØ§Ø© (DRY RUN) - Ù„Ù† ÙŠØªÙ… Ø­Ø°Ù Ø£ÙŠ Ù…Ù„Ù ÙØ¹Ù„ÙŠØ§Ù‹")
        for file_path in approved_files:
            print(f"  [Ù…Ø­Ø§ÙƒØ§Ø©] Ø³ÙŠØªÙ… Ø­Ø°Ù: {file_path}")
            deletion_log['deleted'].append({
                'path': file_path,
                'dry_run': True
            })
        return deletion_log
    
    # Ø§Ù„Ø­Ø°Ù Ø§Ù„ÙØ¹Ù„ÙŠ
    for i, file_path in enumerate(approved_files, 1):
        try:
            print(f"\n[{i}/{len(approved_files)}] Ø­Ø°Ù: {file_path}")
            
            # 1. Ù†Ø³Ø® Ø§Ù„Ù…Ù„Ù Ø¥Ù„Ù‰ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø­Ø°Ù (Ù„Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ø³Ø±ÙŠØ¹) Ø¯Ø§Ø®Ù„ D:\New folder (56)
            deleted_backup = backup_path / 'deleted_files' / file_path
            deleted_backup.parent.mkdir(parents=True, exist_ok=True)
            shutil.copy2(file_path, deleted_backup)
            
            # 2. Ø­Ø°Ù Ø§Ù„Ù…Ù„Ù Ø§Ù„ÙØ¹Ù„ÙŠ
            os.remove(file_path)
            
            deletion_log['deleted'].append({
                'path': file_path,
                'backup_location': str(deleted_backup),
                'timestamp': datetime.datetime.now().isoformat(),
                'status': 'success'
            })
            
            print(f"  âœ… ØªÙ… Ø§Ù„Ø­Ø°Ù")
            
        except Exception as e:
            print(f"  âŒ ÙØ´Ù„ Ø§Ù„Ø­Ø°Ù: {e}")
            deletion_log['failed'].append({
                'path': file_path,
                'error': str(e)
            })
    
    # Ø­ÙØ¸ Ø³Ø¬Ù„ Ø§Ù„Ø­Ø°Ù ÙÙŠ D:\New folder (56)
    log_file = Path(config['output_path']) / 'deletion_log.json'
    with open(log_file, 'w', encoding='utf-8') as f:
        json.dump(deletion_log, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… Ø§ÙƒØªÙ…Ù„Øª Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø­Ø°Ù:")
    print(f"  â”œâ”€ Ù†Ø¬Ø­: {len(deletion_log['deleted'])}")
    print(f"  â”œâ”€ ÙØ´Ù„: {len(deletion_log['failed'])}")
    print(f"  â””â”€ Ø³Ø¬Ù„ Ø§Ù„Ø­Ø°Ù: {log_file}")
    
    return deletion_log

def rollback_deletion(deletion_log):
    """
    Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ø°ÙˆÙØ© ÙÙŠ Ø­Ø§Ù„Ø© ÙˆØ¬ÙˆØ¯ Ù…Ø´ÙƒÙ„Ø© - Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø© Ù…Ù† D:\New folder (56)
    """
    print("\nğŸ”„ Ø¬Ø§Ø±ÙŠ Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ø°ÙˆÙØ©...")
    
    rollback_report = {
        'restored': [],
        'failed': []
    }
    
    for deleted_file in deletion_log['deleted']:
        try:
            backup_location = deleted_file['backup_location']
            original_path = deleted_file['path']
            
            # Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ù„Ù
            Path(original_path).parent.mkdir(parents=True, exist_ok=True)
            shutil.copy2(backup_location, original_path)
            
            rollback_report['restored'].append(original_path)
            print(f"  âœ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹: {original_path}")
            
        except Exception as e:
            rollback_report['failed'].append({
                'path': original_path,
                'error': str(e)
            })
            print(f"  âŒ ÙØ´Ù„ Ø§Ø³ØªØ±Ø¬Ø§Ø¹: {original_path}")
    
    print(f"\nâœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹:")
    print(f"  â”œâ”€ ØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹: {len(rollback_report['restored'])}")
    print(f"  â””â”€ ÙØ´Ù„: {len(rollback_report['failed'])}")
    
    return rollback_report

def post_deletion_validation(repo_path, config):
    """
    Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø³Ù„Ø§Ù…Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¨Ø¹Ø¯ Ø§Ù„Ø­Ø°Ù
    """
    print("\n" + "="*70)
    print("ğŸ§ª Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø³Ù„Ø§Ù…Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚")
    print("="*70)
    
    validation_report = {
        'build_status': None,
        'tests_status': None,
        'linting_status': None,
        'issues_found': [],
        'overall_status': 'unknown'
    }
    
    # 1. ÙØ­Øµ Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹
    print("\n1ï¸âƒ£ ÙØ­Øµ Ø§Ù„Ø¨Ù†Ø§Ø¡ (Build)...")
    try:
        build_result = subprocess.run(
            ['npm', 'run', 'build'],
            cwd=repo_path,
            capture_output=True,
            text=True,
            timeout=300
        )
        
        if build_result.returncode == 0:
            validation_report['build_status'] = 'success'
            print("  âœ… Ø§Ù„Ø¨Ù†Ø§Ø¡ Ù†Ø¬Ø­")
        else:
            validation_report['build_status'] = 'failed'
            validation_report['issues_found'].append({
                'type': 'build_error',
                'message': build_result.stderr[:500]
            })
            print("  âŒ Ø§Ù„Ø¨Ù†Ø§Ø¡ ÙØ´Ù„")
            print(f"  Ø§Ù„Ø®Ø·Ø£: {build_result.stderr[:200]}")
    
    except subprocess.TimeoutExpired:
        validation_report['build_status'] = 'timeout'
        print("  â±ï¸  Ø§Ù„Ø¨Ù†Ø§Ø¡ Ø§Ø³ØªØºØ±Ù‚ ÙˆÙ‚ØªØ§Ù‹ Ø·ÙˆÙŠÙ„Ø§Ù‹")
    
    except Exception as e:
        validation_report['build_status'] = 'error'
        print(f"  âŒ Ø®Ø·Ø£: {e}")
    
    # 2. ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª
    print("\n2ï¸âƒ£ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª...")
    try:
        test_result = subprocess.run(
            ['npm', 'test'],
            cwd=repo_path,
            capture_output=True,
            text=True,
            timeout=300
        )
        
        if test_result.returncode == 0:
            validation_report['tests_status'] = 'passed'
            print("  âœ… Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ù†Ø¬Ø­Øª")
        else:
            validation_report['tests_status'] = 'failed'
            validation_report['issues_found'].append({
                'type': 'test_failure',
                'message': test_result.stderr[:500]
            })
            print("  âŒ Ø¨Ø¹Ø¶ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª ÙØ´Ù„Øª")
    
    except Exception as e:
        validation_report['tests_status'] = 'skipped'
        print(f"  â­ï¸  ØªØ®Ø·ÙŠ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª: {e}")
    
    # 3. ÙØ­Øµ Linting
    print("\n3ï¸âƒ£ ÙØ­Øµ Linting...")
    try:
        lint_result = subprocess.run(
            ['npm', 'run', 'lint'],
            cwd=repo_path,
            capture_output=True,
            text=True,
            timeout=60
        )
        
        validation_report['linting_status'] = 'passed' if lint_result.returncode == 0 else 'warnings'
        print(f"  {'âœ…' if lint_result.returncode == 0 else 'âš ï¸'} Linting")
    
    except Exception as e:
        validation_report['linting_status'] = 'skipped'
        print(f"  â­ï¸  ØªØ®Ø·ÙŠ Linting")
    
    # 4. Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
    if (validation_report['build_status'] == 'success' and 
        validation_report['tests_status'] in ['passed', 'skipped']):
        validation_report['overall_status'] = 'healthy'
        print("\nâœ… Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ÙÙŠ Ø­Ø§Ù„Ø© Ø¬ÙŠØ¯Ø©")
    else:
        validation_report['overall_status'] = 'unhealthy'
        print("\nâŒ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ÙŠØ­ØªØ§Ø¬ Ù…Ø±Ø§Ø¬Ø¹Ø©")
    
    return validation_report

def handle_validation_failure(validation_report, deletion_log, config):
    """
    Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ ÙØ´Ù„ Ø§Ù„ØªØ­Ù‚Ù‚ Ø¨Ø¹Ø¯ Ø§Ù„Ø­Ø°Ù
    """
    print("\nâš ï¸  ØªÙ… Ø§ÙƒØªØ´Ø§Ù Ù…Ø´Ø§ÙƒÙ„ Ø¨Ø¹Ø¯ Ø§Ù„Ø­Ø°Ù!")
    print("\nØ§Ù„Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©:")
    print("  1. Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª (Rollback ÙƒØ§Ù…Ù„)")
    print("  2. Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø¨Ø¹Ø¶ Ø§Ù„Ù…Ù„ÙØ§Øª (Rollback Ø¬Ø²Ø¦ÙŠ)")
    print("  3. Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ ÙŠØ¯ÙˆÙŠØ§Ù‹")
    
    choice = input("\nØ§Ø®ØªÙŠØ§Ø±Ùƒ (1/2/3): ")
    
    if choice == '1':
        print("\nğŸ”„ Ø¬Ø§Ø±ÙŠ Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª...")
        rollback_deletion(deletion_log)
        print("âœ… ØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª")
        
    elif choice == '2':
        print("\nğŸ“‹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ø°ÙˆÙØ©:")
        for i, file_info in enumerate(deletion_log['deleted'], 1):
            print(f"  {i}. {file_info['path']}")
        
        to_restore = input("\nØ£Ø¯Ø®Ù„ Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù…Ù„ÙØ§Øª Ù„Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ (Ù…ÙØµÙˆÙ„Ø© Ø¨ÙÙˆØ§ØµÙ„): ")
        indices = [int(x.strip()) - 1 for x in to_restore.split(',')]
        
        partial_rollback(deletion_log, indices)
        
    else:
        print("\nâš ï¸  ØªÙ… Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª - ÙŠÙØ±Ø¬Ù‰ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ ÙŠØ¯ÙˆÙŠØ§Ù‹")
        print(f"Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ù…ØªÙˆÙØ±Ø© ÙÙŠ: {config['output_path']}")

def partial_rollback(deletion_log, indices):
    """Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø¬Ø²Ø¦ÙŠ Ù„Ù„Ù…Ù„ÙØ§Øª"""
    print("\nğŸ”„ Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ø¬Ø²Ø¦ÙŠ...")
    rollback_report = {
        'restored': [],
        'failed': []
    }
    
    for idx in indices:
        if 0 <= idx < len(deletion_log['deleted']):
            file_info = deletion_log['deleted'][idx]
            try:
                backup_location = file_info['backup_location']
                original_path = file_info['path']
                
                Path(original_path).parent.mkdir(parents=True, exist_ok=True)
                shutil.copy2(backup_location, original_path)
                
                rollback_report['restored'].append(original_path)
                print(f"  âœ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹: {original_path}")
                
            except Exception as e:
                rollback_report['failed'].append({
                    'path': file_info['path'],
                    'error': str(e)
                })
                print(f"  âŒ ÙØ´Ù„ Ø§Ø³ØªØ±Ø¬Ø§Ø¹: {file_info['path']}")
    
    print(f"\nâœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ø¬Ø²Ø¦ÙŠ:")
    print(f"  â”œâ”€ ØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹: {len(rollback_report['restored'])}")
    print(f"  â””â”€ ÙØ´Ù„: {len(rollback_report['failed'])}")

def generate_final_report(deletion_log, validation_report, stats_before, stats_after, config):
    """
    ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø±ÙŠØ± Ù†Ù‡Ø§Ø¦ÙŠ Ø´Ø§Ù…Ù„ - ÙŠÙØ­ÙØ¸ ÙÙŠ D:\New folder (56)
    """
    report_content = f"""
{'='*70}
ğŸ“Š ØªÙ‚Ø±ÙŠØ± ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹ - Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
{'='*70}

â° Ø§Ù„ØªØ§Ø±ÙŠØ®: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

{'â”€'*70}
ğŸ“ˆ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù‚Ø¨Ù„ ÙˆØ¨Ø¹Ø¯
{'â”€'*70}

Ù‚Ø¨Ù„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ:
  â€¢ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª: {stats_before['total_files']}
  â€¢ Ø§Ù„Ø­Ø¬Ù… Ø§Ù„ÙƒÙ„ÙŠ: {stats_before['total_size_mb']:.2f} MB
  â€¢ Ù…Ù„ÙØ§Øª ØºÙŠØ± Ù…Ø³ØªØ®Ø¯Ù…Ø©: {stats_before['unused_files']}

Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ:
  â€¢ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª: {stats_after['total_files']}
  â€¢ Ø§Ù„Ø­Ø¬Ù… Ø§Ù„ÙƒÙ„ÙŠ: {stats_after['total_size_mb']:.2f} MB
  â€¢ Ù…Ù„ÙØ§Øª Ù†Ø´Ø·Ø©: {stats_after['active_files']}

Ø§Ù„ØªØ­Ø³ÙŠÙ†:
  â€¢ ØªÙ… Ø­Ø°Ù: {deletion_log['total_files']} Ù…Ù„Ù
  â€¢ ØªÙ… ØªÙˆÙÙŠØ±: {stats_before['total_size_mb'] - stats_after['total_size_mb']:.2f} MB
  â€¢ Ù†Ø³Ø¨Ø© Ø§Ù„ØªÙ‚Ù„ÙŠÙ„: {(1 - stats_after['total_files']/stats_before['total_files'])*100:.1f}%

{'â”€'*70}
ğŸ—‘ï¸  ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø­Ø°Ù
{'â”€'*70}

âœ… ØªÙ… Ø­Ø°ÙÙ‡Ø§ Ø¨Ù†Ø¬Ø§Ø­: {len(deletion_log['deleted'])}
âŒ ÙØ´Ù„ Ø§Ù„Ø­Ø°Ù: {len(deletion_log['failed'])}

{'â”€'*70}
ğŸ§ª Ø­Ø§Ù„Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ
{'â”€'*70}

Ø§Ù„Ø¨Ù†Ø§Ø¡ (Build): {get_status_emoji(validation_report['build_status'])} {validation_report['build_status']}
Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª (Tests): {get_status_emoji(validation_report['tests_status'])} {validation_report['tests_status']}
Linting: {get_status_emoji(validation_report['linting_status'])} {validation_report['linting_status']}

Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø¹Ø§Ù…Ø©: {get_status_emoji(validation_report['overall_status'])} {validation_report['overall_status'].upper()}

{'â”€'*70}
ğŸ’¾ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ
{'â”€'*70}

Ø§Ù„Ù…Ø³Ø§Ø±: {deletion_log['backup_path']}
Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ø°ÙˆÙØ©: {Path(deletion_log['backup_path']) / 'deleted_files/'}
Ø³Ø¬Ù„ Ø§Ù„Ø­Ø°Ù: {config['output_path']}/deletion_log.json

âš ï¸  Ù…Ù„Ø§Ø­Ø¸Ø©: ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø£ÙŠ Ù…Ù„Ù Ù…Ù† Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©

{'='*70}
"""
    
    # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙÙŠ D:\New folder (56)
    report_file = Path(config['output_path']) / 'cleanup_report.txt'
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    print(report_content)
    print(f"\nğŸ“„ ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙÙŠ: {report_file}")
    
    return report_content

def get_status_emoji(status):
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ emoji Ø­Ø³Ø¨ Ø§Ù„Ø­Ø§Ù„Ø©"""
    emoji_map = {
        'success': 'âœ…',
        'passed': 'âœ…',
        'healthy': 'âœ…',
        'failed': 'âŒ',
        'unhealthy': 'âŒ',
        'warnings': 'âš ï¸',
        'skipped': 'â­ï¸',
        'timeout': 'â±ï¸',
        'unknown': 'â“'
    }
    return emoji_map.get(status, 'â“')

def collect_repo_stats(repo_path, ignore_patterns=None):
    """
    Ø¬Ù…Ø¹ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹
    """
    if ignore_patterns is None:
        ignore_patterns = ['node_modules', '.git', 'dist', 'build', '__pycache__']
    
    total_files = 0
    total_size = 0
    unused_files = 0
    
    for root, dirs, files in os.walk(repo_path):
        dirs[:] = [d for d in dirs if d not in ignore_patterns]
        
        for file in files:
            file_path = os.path.join(root, file)
            total_files += 1
            total_size += os.path.getsize(file_path)
    
    return {
        'total_files': total_files,
        'total_size_mb': total_size / (1024 * 1024),
        'unused_files': unused_files
    }

def generate_structure_summary(file_path):
    """ØªÙˆÙ„ÙŠØ¯ Ù…Ù„Ø®Øµ Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ù„Ù (Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©)"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        return {
            'total_lines': len(lines),
            'import_statements': len([l for l in lines if l.startswith('import')]),
            'export_statements': len([l for l in lines if l.startswith('export')]),
            'class_definitions': len([l for l in lines if l.startswith('class')]),
            'function_definitions': len([l for l in lines if l.startswith('function')])
        }
    except:
        return {'error': 'unable to parse'}

def main():
    """Ù†Ù‚Ø·Ø© Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    
    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘          ğŸ§¹ Ø£Ø¯Ø§Ø© ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹ - Ø¥ØµØ¯Ø§Ø± Ø§Ù„Ø¥Ù†ØªØ§Ø¬                â•‘
â•‘                                                                  â•‘
â•‘  ğŸ“ Ù…Ø³Ø§Ø± Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬: D:\\New folder (56)                            â•‘
â•‘  Ø§Ù„Ù‡Ø¯Ù: Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆØ¯Ø¹ Ù†Ø¸ÙŠÙ - ÙƒÙ„ Ù…Ù„Ù Ù…ÙØ¹Ù‘Ù„ ÙˆÙ…ÙÙŠØ¯           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # ØªØ£ÙƒÙŠØ¯ ÙˆØ¬ÙˆØ¯ Ù…Ø³Ø§Ø± Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
    OUTPUT_BASE_PATH.mkdir(parents=True, exist_ok=True)
    print(f"âœ… ØªÙ… Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù…Ø³Ø§Ø± Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬: {OUTPUT_BASE_PATH}")
    
    # 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
    config = load_config('cleanup_config.json')
    
    # 2. Ø¥Ù†Ø´Ø§Ø¡ backup Ø¥Ù„Ø²Ø§Ù…ÙŠ ÙÙŠ D:\New folder (56)
    print("\nğŸ“¦ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 0: Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©")
    backup_path = create_backup(config['repo_path'], config)
    
    # 3. Ø¬Ù…Ø¹ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù‚Ø¨Ù„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ
    print("\nğŸ“Š Ø¬Ù…Ø¹ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹...")
    stats_before = collect_repo_stats(config['repo_path'], config['ignore_patterns'])
    
    # 4. Ø¨Ù†Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ§Øª
    print("\nğŸ—ºï¸  Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø¨Ù†Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ§Øª")
    dependency_map = build_complete_dependency_map(config['repo_path'])
    
    # 5. Ø¬Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª
    print("\nğŸ“ Ø¬Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª...")
    all_files = collect_all_files(
        config['repo_path'], 
        config['ignore_patterns'],
        config
    )
    print(f"  ÙˆØ¬Ø¯ {len(all_files)} Ù…Ù„Ù")
    
    # 5.5. ØªÙˆÙ„ÙŠØ¯ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹
    print("\nğŸ—ºï¸  ØªÙˆÙ„ÙŠØ¯ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹...")
    repo_map = generate_repo_map(config['repo_path'], config['ignore_patterns'])
    
    # âœ¨ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1.5: Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ Ø¨Ø§Ù„Ù€ AI
    print("\nğŸ¤– Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1.5: Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ Ø¨Ù€ Gemini 2.0 Flash")
    model = initialize_gemini()
    ai_analysis_results = analyze_files_with_ai(
        all_files,
        dependency_map,
        repo_map,
        config['entry_points'],
        model,
        config
    )
    categorized_results = categorize_ai_results(ai_analysis_results)
    
    # 6. ØªØ­ÙˆÙŠÙ„ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù€ AI Ø¥Ù„Ù‰ Ù…Ø±Ø´Ø­ÙŠÙ† Ù„Ù„Ø­Ø°Ù
    print("\nğŸ¯ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø±Ø´Ø­Ø© Ù„Ù„Ø­Ø°Ù (Ù…Ù† Ù†ØªØ§Ø¦Ø¬ AI)")
    candidates = convert_ai_results_to_candidates(categorized_results)
    
    print(f"  â”œâ”€ Ø¢Ù…Ù† Ù„Ù„Ø­Ø°Ù: {len(candidates['safe_to_delete'])}")
    print(f"  â”œâ”€ ØºØ§Ù„Ø¨Ø§Ù‹ ØºÙŠØ± Ù…Ø³ØªØ®Ø¯Ù…: {len(candidates['probably_unused'])}")
    print(f"  â”œâ”€ ØºÙŠØ± Ù…ØªØ£ÙƒØ¯: {len(candidates['uncertain'])}")
    print(f"  â””â”€ Ø§Ø­ØªÙØ¸ Ø¨Ù‡: {len(candidates['keep'])}")
    
    # 7. ÙØ­ÙˆØµØ§Øª Ø§Ù„Ø£Ù…Ø§Ù†
    print("\nğŸ”’ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: ÙØ­ÙˆØµØ§Øª Ø§Ù„Ø£Ù…Ø§Ù†")
    safety_report = perform_safety_checks(candidates, dependency_map, config)
    
    # 8. Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„Ø­Ø°Ù
    print("\nğŸ­ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„Ø­Ø°Ù")
    simulation_results = simulate_deletion(safety_report, dependency_map)
    
    # 9. Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ©
    print("\nğŸ‘€ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 4: Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø© ÙˆØ§Ù„Ù…ÙˆØ§ÙÙ‚Ø©")
    final_approved = interactive_review(safety_report, simulation_results, config)
    
    if not final_approved:
        print("\nâŒ Ù„Ù… ØªØªÙ… Ø§Ù„Ù…ÙˆØ§ÙÙ‚Ø© Ø¹Ù„Ù‰ Ø­Ø°Ù Ø£ÙŠ Ù…Ù„Ù - Ø¥Ù†Ù‡Ø§Ø¡ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬")
        return
    
    # 10. Ø§Ù„ØªÙ†ÙÙŠØ°
    print("\nğŸ—‘ï¸  Ø§Ù„Ù…Ø±Ø­Ù„Ø© 5: Ø§Ù„ØªÙ†ÙÙŠØ°")
    deletion_log = safe_deletion_execution(final_approved, config, backup_path)
    
    # 11. Ø§Ù„ØªØ­Ù‚Ù‚ Ø¨Ø¹Ø¯ Ø§Ù„Ø­Ø°Ù
    print("\nğŸ§ª Ø§Ù„Ù…Ø±Ø­Ù„Ø© 6: Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø³Ù„Ø§Ù…Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚")
    validation_report = post_deletion_validation(config['repo_path'], config)
    
    # 12. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙØ´Ù„
    if validation_report['overall_status'] == 'unhealthy':
        handle_validation_failure(validation_report, deletion_log, config)
    
    # 13. Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ
    stats_after = collect_repo_stats(config['repo_path'], config['ignore_patterns'])
    
    # 14. Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
    print("\nğŸ“„ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 7: ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ")
    final_report = generate_final_report(
        deletion_log,
        validation_report,
        stats_before,
        stats_after,
        config
    )
    
    print("\nâœ… Ø§ÙƒØªÙ…Ù„Øª Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø¨Ù†Ø¬Ø§Ø­!")
    print(f"ğŸ‰ ØªÙ… ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹ - ÙƒÙ„ Ù…Ù„Ù Ø§Ù„Ø¢Ù† Ù…ÙØ¹Ù‘Ù„ ÙˆÙ…ÙÙŠØ¯!")
    print(f"ğŸ“ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù†Ø§ØªØ¬Ø© Ù…Ø­ÙÙˆØ¸Ø© ÙÙŠ: {config['output_path']}")

if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù…Ù† Ù‚Ø¨Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
```

---

### ğŸ“Œ **Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ù‡Ø§Ù…Ø©:**

1. **Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù†Ø§ØªØ¬Ø© ØªÙØ­ÙØ¸ ÙÙŠ:** `D:\New folder (56)`
   - Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ø§Ù„ÙƒØ§Ù…Ù„Ø©
   - Ù…Ù„Ù `ai_analysis_results.json` (Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ)
   - Ù…Ù„Ù `deletion_log.json` (Ø³Ø¬Ù„ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù…Ø­Ø°ÙˆÙØ©)
   - Ù…Ù„Ù `cleanup_report.txt` (Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ)
   - Ù…Ø¬Ù„Ø¯ `deleted_files` (Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ø°ÙˆÙØ© Ù‚Ø¨Ù„ Ø­Ø°ÙÙ‡Ø§)

2. **Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ø°ÙˆÙØ© Ù†ÙØ³Ù‡Ø§** ØªÙØ­Ø°Ù Ù…Ù† Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹ Ø§Ù„Ø£ØµÙ„ÙŠ ÙÙ‚Ø·ØŒ Ø£Ù…Ø§ Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© ÙÙƒÙ„Ù‡Ø§ ÙÙŠ `D:\New folder (56)`

3. **Ù„ØªØºÙŠÙŠØ± Ù…Ø³Ø§Ø± Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬** Ù‚Ù… Ø¨ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø³Ø·Ø± Ø§Ù„Ø£ÙˆÙ„ ÙÙŠ Ø§Ù„ÙƒÙˆØ¯:
   ```python
   OUTPUT_BASE_PATH = Path("D:/New folder (56)")
   ```

4. **Ø§Ù„ØªØ´ØºÙŠÙ„:** Ø£Ù†Ø´Ø¦ Ù…Ù„Ù `cleanup_config.json` ÙÙŠ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹ Ø«Ù… Ø´ØºÙ„ Ø§Ù„Ø³ÙƒØ±ÙŠØ¨Øª:
   ```bash
   python repo_cleanup.py
   ```Ù…")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
```
```