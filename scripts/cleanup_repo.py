#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ø³ÙƒØ±ÙŠØ¨Øª ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹ - Ø§Ù„Ø¥ØµØ¯Ø§Ø± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
Ø§Ù„Ù‡Ø¯Ù: Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆØ¯Ø¹ Ù†Ø¸ÙŠÙ Ø­ÙŠØ« ÙƒÙ„ Ù…Ù„Ù Ù…ÙØ¹Ù‘Ù„ ÙˆÙ„Ù‡ Ø¹Ù„Ø§Ù‚Ø© Ø¨Ø§Ù„ØªØ·Ø¨ÙŠÙ‚

ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰:
- dependency-cruiser, knip, madge, depcheck Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ§Øª
- Gemini 3 Pro Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
"""

import os
import sys
import io

# Ø¥ØµÙ„Ø§Ø­ Ø§Ù„ØªØ±Ù…ÙŠØ² Ø¹Ù„Ù‰ Windows
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

import json
import shutil
import subprocess
from pathlib import Path
import datetime
from collections import deque
from typing import Dict, List, Any, Optional

# ============================================================================
# Ø§Ù„Ø«ÙˆØ§Ø¨Øª ÙˆØ§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
# ============================================================================

CLEANUP_FOCUSED_PROMPT = """
**Ø§Ù„Ù‡Ø¯Ù Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ:** Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø®Ø¨ÙŠØ± Ù…ØªØ®ØµØµ ÙÙŠ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹Ø§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ©. Ù…Ù‡Ù…ØªÙƒ Ø§Ù„ÙˆØ­ÙŠØ¯Ø© Ù‡ÙŠ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø¢Ù…Ù†Ø© Ù„Ù„Ø­Ø°Ù Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆØ¯Ø¹ Ù†Ø¸ÙŠÙ Ø­ÙŠØ« ÙƒÙ„ Ù…Ù„Ù Ù…ÙØ¹Ù‘Ù„ ÙˆÙ„Ù‡ Ø¹Ù„Ø§Ù‚Ø© Ù…Ø¨Ø§Ø´Ø±Ø© Ø¨Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.

**Ø§Ù„ØªØ±ÙƒÙŠØ² Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ:**

Ø£Ù†Øª Ù…Ø·Ø§Ù„Ø¨ Ø¨ØªØ­Ù„ÙŠÙ„ ÙƒÙ„ Ù…Ù„Ù ÙˆØªØ­Ø¯ÙŠØ¯ **Ø¨Ø¯Ù‚Ø© Ø´Ø¯ÙŠØ¯Ø©** Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù†:

1. **KEEP** (Ø§Ø­ØªÙØ¸ Ø¨Ù‡) - Ù…Ù„Ù Ù†Ø´Ø· ÙˆÙ…ÙØ¹Ù‘Ù„:
   - ÙŠØªÙ… Ø§Ø³ØªÙŠØ±Ø§Ø¯Ù‡ ÙˆØ§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ ÙØ¹Ù„ÙŠØ§Ù‹ ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
   - Ù…ØªØµÙ„ Ø¨Ù€ entry points
   - Ù„Ù‡ ØªØ£Ø«ÙŠØ± Ø¹Ù„Ù‰ Ø¹Ù…Ù„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚

2. **DELETE_SAFE** (Ø§Ø­Ø°Ù Ø¨Ø£Ù…Ø§Ù†) - Ù…Ù„Ù Ø¹Ø¯ÙŠÙ… Ø§Ù„ÙØ§Ø¦Ø¯Ø©:
   - Ù„Ø§ ÙŠØªÙ… Ø§Ø³ØªÙŠØ±Ø§Ø¯Ù‡ Ù…Ù† Ø£ÙŠ Ù…Ù„Ù
   - ØºÙŠØ± Ù…ØªØµÙ„ Ø¨Ø£ÙŠ entry point
   - Ù…Ù„Ù ÙØ§Ø±Øº Ø£Ùˆ ØªØ¬Ø±ÙŠØ¨ÙŠ Ø£Ùˆ Ù…Ø¹Ø·Ù„
   - Ø£Ø³Ù…Ø§Ø¡ Ù…Ø´Ø¨ÙˆÙ‡Ø© (test, temp, backup, old, deprecated, unused)
   - exports ØºÙŠØ± Ù…Ø³ØªØ®Ø¯Ù…Ø© Ù†Ù‡Ø§Ø¦ÙŠØ§Ù‹

3. **DELETE_PROBABLY** (ØºØ§Ù„Ø¨Ø§Ù‹ Ø¢Ù…Ù† Ù„Ù„Ø­Ø°Ù) - ÙŠØ­ØªØ§Ø¬ Ù…Ø±Ø§Ø¬Ø¹Ø©:
   - Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ù„Ù‡ Ø§Ø³ØªØ®Ø¯Ø§Ù… ØºÙŠØ± ÙˆØ§Ø¶Ø­
   - Ø¨Ø¹ÙŠØ¯ Ø¬Ø¯Ø§Ù‹ Ù…Ù† entry points (> 5 Ù…Ø³ØªÙˆÙŠØ§Øª)
   - Ù„Ù… ÙŠØªÙ… ØªØ¹Ø¯ÙŠÙ„Ù‡ Ù…Ù†Ø° ÙØªØ±Ø© Ø·ÙˆÙŠÙ„Ø© (> 6 Ø´Ù‡ÙˆØ±)

4. **UNCERTAIN** (ØºÙŠØ± Ù…ØªØ£ÙƒØ¯) - Ù…Ø±Ø§Ø¬Ø¹Ø© ÙŠØ¯ÙˆÙŠØ© Ø¥Ù„Ø²Ø§Ù…ÙŠØ©:
   - Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ© Ù„Ù„Ø­ÙƒÙ…
   - Ù…Ù„ÙØ§Øª config Ø£Ùˆ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
   - Ù…Ù„ÙØ§Øª ÙƒØ¨ÙŠØ±Ø© Ù…Ø¹Ù‚Ø¯Ø©
   - Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© ØªØ£Ø«ÙŠØ± ØºÙŠØ± ÙˆØ§Ø¶Ø­Ø©

**ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ (JSON ÙÙ‚Ø·):**

```json
{
  "decision": "KEEP|DELETE_SAFE|DELETE_PROBABLY|UNCERTAIN",
  "confidence": 0-100,
  "reasons": [
    "Ø³Ø¨Ø¨ Ø±Ø¦ÙŠØ³ÙŠ 1",
    "Ø³Ø¨Ø¨ Ø±Ø¦ÙŠØ³ÙŠ 2"
  ],
  "usage_analysis": {
    "is_imported": true|false,
    "import_count": 0,
    "distance_from_entry": -1|0|1|2|...,
    "has_unused_exports": true|false
  },
  "risk_assessment": {
    "deletion_safety_score": 0-100,
    "potential_impact": "none|minimal|moderate|high|critical",
    "affected_files": []
  },
  "recommendation": "Ù†Øµ Ù‚ØµÙŠØ± ÙŠØ´Ø±Ø­ Ø§Ù„Ù‚Ø±Ø§Ø± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ"
}
```

**Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ø­ÙƒÙ… Ø§Ù„ØµØ§Ø±Ù…Ø©:**

1. **KEEP Ø¥Ø°Ø§ ÙˆÙÙ‚Ø· Ø¥Ø°Ø§:**
   - ÙŠØªÙ… Ø§Ø³ØªÙŠØ±Ø§Ø¯Ù‡ Ø¨ÙˆØ§Ø³Ø·Ø© Ù…Ù„Ù ÙˆØ§Ø­Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ØŒ Ø£Ùˆ
   - Ù‡Ùˆ entry point Ù†ÙØ³Ù‡ØŒ Ø£Ùˆ
   - Ù…Ø³Ø§ÙØªÙ‡ Ù…Ù† entry points â‰¤ 3 Ù…Ø³ØªÙˆÙŠØ§Øª

2. **DELETE_SAFE Ø¥Ø°Ø§:**
   - Ù„Ø§ ÙŠØªÙ… Ø§Ø³ØªÙŠØ±Ø§Ø¯Ù‡ Ù†Ù‡Ø§Ø¦ÙŠØ§Ù‹ (import_count = 0)ØŒ Ùˆ
   - ØºÙŠØ± Ù…ØªØµÙ„ Ø¨Ø£ÙŠ entry point (distance = -1)ØŒ Ùˆ
   - Ø¬Ù…ÙŠØ¹ exports ØºÙŠØ± Ù…Ø³ØªØ®Ø¯Ù…Ø©ØŒ Ùˆ
   - Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø£ÙŠ ØªØ£Ø«ÙŠØ± Ø¹Ù„Ù‰ Ù…Ù„ÙØ§Øª Ø£Ø®Ø±Ù‰

3. **DELETE_PROBABLY Ø¥Ø°Ø§:**
   - import_count = 0ØŒ Ùˆ
   - distance > 5 Ø£Ùˆ -1ØŒ Ùˆ
   - confidence >= 70

4. **UNCERTAIN Ø¥Ø°Ø§:**
   - confidence < 70ØŒ Ø£Ùˆ
   - Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù†Ø§Ù‚ØµØ©ØŒ Ø£Ùˆ
   - Ù…Ù„Ù config/settings

**Ù‚ÙˆØ§Ø¹Ø¯ Ø­Ø§Ø³Ù…Ø©:**

- âŒ **Ù„Ø§ ØªÙƒÙ† Ù…ØªØ³Ø§Ù‡Ù„Ø§Ù‹** - Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ: Ø§Ø­Ø°Ù Ø¥Ù„Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ø¯Ù„ÙŠÙ„ ÙˆØ§Ø¶Ø­ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
- âœ… **ÙƒÙ† ØµØ§Ø±Ù…Ø§Ù‹ ÙÙŠ KEEP** - ÙÙ‚Ø· Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙØ¹Ù‘Ù„Ø© ÙØ¹Ù„ÙŠØ§Ù‹
- âš ï¸ **ÙƒÙ† Ø­Ø°Ø±Ø§Ù‹ Ù…Ø¹ UNCERTAIN** - Ø¹Ù†Ø¯ Ø£Ø¯Ù†Ù‰ Ø´ÙƒØŒ Ø¶Ø¹Ù‡ ÙÙŠ uncertain
- ğŸ“Š **Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙ‚Ø·** - Ù„Ø§ ØªØ®Ù…Ù†ØŒ Ø§Ø¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ§Øª ÙˆØ§Ù„Ø£Ø¯ÙˆØ§Øª
- ğŸ¯ **Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø§Ù„Ù‡Ø¯Ù** - Ù…Ø³ØªÙˆØ¯Ø¹ Ù†Ø¸ÙŠÙ = ØµÙØ± Ù…Ù„ÙØ§Øª ØºÙŠØ± Ù…ÙØ¹Ù‘Ù„Ø©
"""


# ============================================================================
# Ø§Ù„Ù…Ø±Ø­Ù„Ø© 0: Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯ ÙˆØ§Ù„Ù€ Backup
# ============================================================================

def create_backup(repo_path: Path) -> Path:
    """
    Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© ÙƒØ§Ù…Ù„Ø© Ù‚Ø¨Ù„ Ø£ÙŠ Ø­Ø°Ù
    """
    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
    backup_name = f"backup_{timestamp}"
    backup_path = repo_path.parent / backup_name

    print(f"ğŸ”„ Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©...")

    # Ø£Ø³Ù…Ø§Ø¡ Ù…Ø­Ø¬ÙˆØ²Ø© Ø¹Ù„Ù‰ Windows Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ ÙƒÙ…Ù„ÙØ§Øª
    reserved_names = {'nul', 'con', 'prn', 'aux', 'com1', 'com2', 'com3', 'com4',
                      'com5', 'com6', 'com7', 'com8', 'com9', 'lpt1', 'lpt2',
                      'lpt3', 'lpt4', 'lpt5', 'lpt6', 'lpt7', 'lpt8', 'lpt9'}

    def ignore_invalid_files(path, names):
        """ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù…Ù„ÙØ§Øª ØºÙŠØ± Ø§Ù„ØµØ§Ù„Ø­Ø©"""
        ignored = []
        for name in names:
            if name.lower() in reserved_names:
                ignored.append(name)
                print(f"  âš ï¸  ØªÙ… ØªØ¬Ø§Ù‡Ù„ Ù…Ù„Ù ØºÙŠØ± ØµØ§Ù„Ø­: {path}/{name}")
        return ignored

    # Ù†Ø³Ø® ÙƒØ§Ù…Ù„ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹ Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
    base_ignore = ['node_modules', '.git', 'dist', 'build', '__pycache__', '.next', 'temp_backup']

    try:
        shutil.copytree(
            repo_path,
            backup_path,
            ignore=shutil.ignore_patterns(*base_ignore),
            ignore_dangling_symlinks=True
        )
    except shutil.Error as e:
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ø®Ø·Ø§Ø¡ Ù…Ø­Ø¯Ø¯Ø©
        print(f"  âš ï¸  Ø¨Ø¹Ø¶ Ø§Ù„Ù…Ù„ÙØ§Øª Ù„Ù… ÙŠØªÙ… Ù†Ø³Ø®Ù‡Ø§: {e}")
        # Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø© Ø¥Ø°Ø§ ØªÙ… Ù†Ø³Ø® Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø£ÙƒØ¨Ø±
        if not backup_path.exists():
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯ ÙŠØ¯ÙˆÙŠØ§Ù‹ ÙˆØ§Ù„Ù†Ø³Ø® Ù…Ù„ÙØ§Ù‹ Ø¨Ù…Ù„Ù
            backup_path.mkdir(parents=True, exist_ok=True)
            copy_directory_manually(repo_path, backup_path, base_ignore, reserved_names)

    # Ø­ÙØ¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù€ backup
    backup_info = {
        'timestamp': timestamp,
        'original_path': str(repo_path),
        'backup_path': str(backup_path),
        'commit_hash': get_current_commit_hash(repo_path)
    }

    with open(backup_path / 'BACKUP_INFO.json', 'w', encoding='utf-8') as f:
        json.dump(backup_info, f, indent=2, ensure_ascii=False)

    print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©: {backup_path}")
    return backup_path


def copy_directory_manually(src: Path, dst: Path, ignore_patterns: List[str], reserved_names: set):
    """Ù†Ø³Ø® Ø§Ù„Ù…Ø¬Ù„Ø¯ ÙŠØ¯ÙˆÙŠØ§Ù‹ Ù…Ø¹ ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø´ÙƒÙ„Ø©"""
    for item in src.iterdir():
        if item.name in ignore_patterns:
            continue

        if item.name.lower() in reserved_names:
            continue

        dest_item = dst / item.name

        if item.is_dir():
            dest_item.mkdir(exist_ok=True)
            copy_directory_manually(item, dest_item, ignore_patterns, reserved_names)
        else:
            try:
                shutil.copy2(item, dest_item)
            except Exception as e:
                print(f"  âš ï¸  ØªÙ… ØªØ®Ø·ÙŠ {item}: {e}")


def get_current_commit_hash(repo_path: Path) -> Optional[str]:
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¢Ø®Ø± commit hash"""
    try:
        result = subprocess.run(
            ['git', 'rev-parse', 'HEAD'],
            cwd=repo_path,
            capture_output=True,
            text=True
        )
        return result.stdout.strip()
    except:
        return None


def load_config(config_path: str) -> Dict[str, Any]:
    """ØªØ­Ù…ÙŠÙ„ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªÙ†Ø¸ÙŠÙ"""
    default_config = {
        "repo_path": str(Path.cwd()),
        "ignore_patterns": [
            "node_modules",
            ".git",
            "dist",
            "build",
            "__pycache__",
            ".vscode",
            ".idea",
            ".next",
            "coverage",
            "temp_backup"
        ],
        "entry_points": [
            "frontend/src/app/page.tsx",
            "frontend/src/main.tsx",
            "backend/src/server.ts",
            "backend/src/index.ts"
        ],
        "protected_files": [
            "package.json",
            "package-lock.json",
            "pnpm-lock.yaml",
            "tsconfig.json",
            ".env.example",
            "README.md",
            ".gitignore",
            "CLAUDE.md",
            "cleanup_config.json"
        ],
        "safe_mode": True,
        "create_backup": True,
        "dry_run": False
    }

    config_file = Path(config_path)
    if config_file.exists():
        with open(config_file, 'r', encoding='utf-8') as f:
            user_config = json.load(f)
            default_config.update(user_config)

    return default_config


# ============================================================================
# Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø§Ù„Ù…Ø³Ø­ Ø§Ù„Ø´Ø§Ù…Ù„ ÙˆØ¨Ù†Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ§Øª
# ============================================================================

def collect_all_files(repo_path: Path, ignore_patterns: List[str], config: Dict) -> Dict[str, Dict]:
    """
    Ø¬Ù…Ø¹ ÙƒÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ù…Ø¹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ©
    """
    import fnmatch
    all_files = {}

    for root, dirs, files in os.walk(repo_path):
        # ØªØµÙÙŠØ© Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ù…Ø³ØªØ«Ù†Ø§Ø© - Ø§Ø³ØªØ®Ø¯Ø§Ù… pattern matching
        dirs[:] = [d for d in dirs if not any(
            fnmatch.fnmatch(d, pattern) or d == pattern
            for pattern in ignore_patterns
        ) and not d.startswith('.')]

        for file in files:
            file_path = Path(root) / file
            relative_path = str(file_path.relative_to(repo_path))

            # ØªØ®Ø·ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ù…ÙŠØ©
            if relative_path in config['protected_files']:
                continue

            # ØªØ®Ø·ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª ØºÙŠØ± Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ©
            extensions = ['.ts', '.tsx', '.js', '.jsx', '.json', '.md', '.py', '.yaml', '.yml']
            if file_path.suffix not in extensions:
                continue

            try:
                all_files[relative_path] = {
                    'absolute_path': str(file_path),
                    'relative_path': relative_path,
                    'extension': file_path.suffix,
                    'size_bytes': file_path.stat().st_size,
                    'is_protected': False,
                    'analysis_status': 'pending'
                }
            except:
                pass

    return all_files


def build_complete_dependency_map(repo_path: Path) -> Dict[str, Any]:
    """
    Ø¨Ù†Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ§Øª Ø¯Ù‚ÙŠÙ‚Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©
    """
    print("ğŸ” Ø¬Ø§Ø±ÙŠ Ø¨Ù†Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ§Øª...")

    dependency_map = {
        'imports': {},
        'imported_by': {},
        'unused_exports': [],
        'unused_dependencies': [],
        'circular_dependencies': []
    }

    # 1. Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… dependency-cruiser
    print("  â”œâ”€ ØªØ´ØºÙŠÙ„ dependency-cruiser...")
    try:
        result = subprocess.run(
            ['npx', 'depcruise', '--include-only', '^(frontend|backend)/src', '--output-type', 'json', 'frontend', 'backend'],
            cwd=repo_path,
            capture_output=True,
            text=True,
            timeout=120
        )
        if result.returncode == 0:
            dependency_map = merge_depcruise_results(dependency_map, json.loads(result.stdout))
            print("    âœ… dependency-cruiser")
        else:
            print("    âš ï¸  dependency-cruiser ÙØ´Ù„ - Ø³Ù†ØªØ§Ø¨Ø¹ Ø¨Ø¯ÙˆÙ†Ù‡Ø§")
    except Exception as e:
        print(f"    âš ï¸  ÙØ´Ù„ ØªØ´ØºÙŠÙ„ dependency-cruiser: {e}")

    # 2. Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… knip
    print("  â”œâ”€ ØªØ´ØºÙŠÙ„ knip...")
    try:
        result = subprocess.run(
            ['npx', 'knip', '--reporter', 'json'],
            cwd=repo_path,
            capture_output=True,
            text=True,
            timeout=120
        )
        if result.returncode == 0:
            knip_data = json.loads(result.stdout)
            dependency_map['unused_exports'] = extract_knip_unused(knip_data)
            print("    âœ… knip")
        else:
            print("    âš ï¸  knip ÙØ´Ù„ - Ø³Ù†ØªØ§Ø¨Ø¹ Ø¨Ø¯ÙˆÙ†Ù‡Ø§")
    except Exception as e:
        print(f"    âš ï¸  ÙØ´Ù„ ØªØ´ØºÙŠÙ„ knip: {e}")

    # 3. Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… depcheck
    print("  â””â”€ ØªØ´ØºÙŠÙ„ depcheck...")
    try:
        result = subprocess.run(
            ['npx', 'depcheck', '--json'],
            cwd=repo_path,
            capture_output=True,
            text=True,
            timeout=60
        )
        if result.returncode == 0:
            depcheck_data = json.loads(result.stdout)
            dependency_map['unused_dependencies'] = list(depcheck_data.keys())
            print("    âœ… depcheck")
        else:
            print("    âš ï¸  depcheck ÙØ´Ù„ - Ø³Ù†ØªØ§Ø¨Ø¹ Ø¨Ø¯ÙˆÙ†Ù‡Ø§")
    except Exception as e:
        print(f"    âš ï¸  ÙØ´Ù„ ØªØ´ØºÙŠÙ„ depcheck: {e}")

    print("âœ… ØªÙ… Ø¨Ù†Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ§Øª")
    return dependency_map


def merge_depcruise_results(dep_map: Dict, depcruise_data: Dict) -> Dict:
    """Ø¯Ù…Ø¬ Ù†ØªØ§Ø¦Ø¬ dependency-cruiser"""
    for module in depcruise_data.get('modules', []):
        module_path = module.get('source', '')
        dependencies = [d.get('imported', '') for d in module.get('dependencies', [])]

        dep_map['imports'][module_path] = dependencies

        for dep in dependencies:
            if dep not in dep_map['imported_by']:
                dep_map['imported_by'][dep] = []
            if module_path not in dep_map['imported_by'][dep]:
                dep_map['imported_by'][dep].append(module_path)

    return dep_map


def extract_knip_unused(knip_data: Dict) -> List[str]:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ù„ÙØ§Øª ØºÙŠØ± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© Ù…Ù† knip"""
    unused = []

    # Unused files
    for issue in knip_data.get('issues', {}).get('files', []):
        unused.append(issue.get('file', ''))

    # Unused exports
    for issue in knip_data.get('issues', {}).get('dependencies', []):
        if issue.get('file'):
            unused.append(issue['file'])

    return unused


def generate_repo_map(repo_path: Path, ignore_patterns: List[str]) -> Dict[str, Any]:
    """ØªÙˆÙ„ÙŠØ¯ Ø®Ø±ÙŠØ·Ø© Ù‡ÙŠÙƒÙ„ÙŠØ© Ù„Ù„Ù…Ø³ØªÙˆØ¯Ø¹"""
    repo_map = {
        'structure': {},
        'frontend_files': [],
        'backend_files': [],
        'config_files': [],
        'root_files': []
    }

    for root, dirs, files in os.walk(repo_path):
        dirs[:] = [d for d in dirs if d not in ignore_patterns and not d.startswith('.')]

        for file in files:
            file_path = Path(root) / file
            relative_path = str(file_path.relative_to(repo_path))

            if relative_path.startswith('frontend/'):
                repo_map['frontend_files'].append(relative_path)
            elif relative_path.startswith('backend/'):
                repo_map['backend_files'].append(relative_path)
            elif relative_path.startswith('frontend/') or relative_path.startswith('backend/'):
                pass
            else:
                repo_map['root_files'].append(relative_path)

    return repo_map


# ============================================================================
# Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1.5: Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ Ø¨Ø§Ù„Ù€ AI
# ============================================================================

def initialize_gemini():
    """
    ØªÙ‡ÙŠØ¦Ø© Ù†Ù…ÙˆØ°Ø¬ Gemini Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© google-genai
    """
    try:
        from google import genai
        from google.genai import types
        from dotenv import load_dotenv

        load_dotenv()

        api_key = os.getenv('GEMINI_API_KEY') or os.getenv('GOOGLE_GENAI_API_KEY') or os.getenv('GOOGLE_API_KEY')
        if not api_key:
            print("âš ï¸  Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ GEMINI_API_KEY - Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙ‚Ù†ÙŠ ÙÙ‚Ø·")
            print("   Ø£Ø¶Ù ÙÙŠ Ù…Ù„Ù .env: GEMINI_API_KEY=your_key_here")
            return None

        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø¹Ù…ÙŠÙ„
        client = genai.Client(api_key=api_key)

        print("âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Google Gen AI SDK")
        return client

    except ImportError:
        print("âš ï¸  Ù„Ù… ÙŠØªÙ… ØªØ«Ø¨ÙŠØª google-genai - Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙ‚Ù†ÙŠ ÙÙ‚Ø·")
        print("   Ø«Ø¨ØªÙ‡: pip install google-genai")
        return None
    except Exception as e:
        print(f"âš ï¸  ÙØ´Ù„ ØªÙ‡ÙŠØ¦Ø© Gemini: {e} - Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙ‚Ù†ÙŠ ÙÙ‚Ø·")
        return None


def build_ai_analysis_prompt(file_path: str, file_info: Dict, dependency_map: Dict, entry_points: List[str]) -> str:
    """
    Ø¨Ù†Ø§Ø¡ prompt Ù…Ø®ØµØµ Ù„ÙƒÙ„ Ù…Ù„Ù Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ø§Ù„Ù€ AI
    """
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ§Øª
    imported_by = dependency_map['imported_by'].get(file_path, [])
    is_unused_export = file_path in dependency_map['unused_exports']

    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø³Ø§ÙØ© Ù…Ù† entry points
    distance = calculate_distance_from_entry_points(file_path, entry_points, dependency_map)

    # Ù‚Ø±Ø§Ø¡Ø© Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù„Ù Ø£Ùˆ Ù…Ù„Ø®ØµÙ‡
    if file_info['size_bytes'] < 10000:  # Ø£Ù‚Ù„ Ù…Ù† ~10KB
        try:
            with open(file_info['absolute_path'], 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            content_section = f"### Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù„Ù Ø§Ù„ÙƒØ§Ù…Ù„:\n```\n{content}\n```"
        except:
            content_section = "### Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù„Ù: (ØºÙŠØ± Ù‚Ø§Ø¯Ø± Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©)"
    else:
        content_section = f"### Ù…Ù„Ø®Øµ: Ù…Ù„Ù ÙƒØ¨ÙŠØ± ({file_info['size_bytes']} Ø¨Ø§ÙŠØª)"

    # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù€ prompt Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
    prompt = f"""{CLEANUP_FOCUSED_PROMPT}

---

# Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù„Ù Ù„Ù„ØªØ­Ù„ÙŠÙ„

## Ø§Ù„Ù…Ù„Ù: `{file_path}`

### Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªÙ‚Ù†ÙŠØ©:
- **Ø§Ù„Ø­Ø¬Ù…:** {file_info['size_bytes']} Ø¨Ø§ÙŠØª
- **Ø§Ù„Ø§Ù…ØªØ¯Ø§Ø¯:** {file_info['extension']}
- **Ø§Ù„Ù…Ø³Ø§Ø±:** {file_info['absolute_path']}

### ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ§Øª:
- **ÙŠØªÙ… Ø§Ø³ØªÙŠØ±Ø§Ø¯Ù‡ Ø¨ÙˆØ§Ø³Ø·Ø© ({len(imported_by)} Ù…Ù„Ù):** {', '.join(imported_by[:5]) if imported_by else 'Ù„Ø§ ÙŠØ³ØªÙˆØ±Ø¯Ù‡ Ø£ÙŠ Ù…Ù„Ù'}
- **Ø§Ù„Ù…Ø³Ø§ÙØ© Ù…Ù† entry points:** {distance if distance != -1 else 'ØºÙŠØ± Ù…ØªØµÙ„'}
- **exports ØºÙŠØ± Ù…Ø³ØªØ®Ø¯Ù…Ø©:** {'Ù†Ø¹Ù…' if is_unused_export else 'Ù„Ø§'}

### Ù†Ù‚Ø§Ø· Ø§Ù„Ø¯Ø®ÙˆÙ„:
{chr(10).join(['- ' + ep for ep in entry_points])}

{content_section}

---

**Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:** Ù‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ Ù‡Ø°Ø§ Ø§Ù„Ù…Ù„Ù ÙˆØ£Ø±Ø¬Ø¹ JSON ÙÙ‚Ø·.
"""

    return prompt


def analyze_files_with_ai(all_files: Dict, dependency_map: Dict, entry_points: List[str], client, config: Dict) -> Dict[str, Dict]:
    """
    ØªØ­Ù„ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Google Gen AI (Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©)
    """
    if client is None:
        print("âš ï¸  Ù„Ù… ÙŠØªÙ… ØªÙ‡ÙŠØ¦Ø© AI - Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙ‚Ù†ÙŠ ÙÙ‚Ø·")
        return analyze_files_technically(all_files, dependency_map, entry_points)

    print(f"\nğŸ¤– Ø¬Ø§Ø±ÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Google Gen AI...")
    ai_analysis_results = {}

    total = len(all_files)
    for i, (file_path, file_info) in enumerate(all_files.items(), 1):
        try:
            if i % 10 == 0 or i == total:
                print(f"\r  [{i}/{total}] ØªØ­Ù„ÙŠÙ„: {file_path[:50]}...", end='', flush=True)

            # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù€ prompt
            prompt = build_ai_analysis_prompt(file_path, file_info, dependency_map, entry_points)

            # Ø¥Ø±Ø³Ø§Ù„ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
            response = client.models.generate_content(
                model='gemini-2.5-flash-latest',
                contents=prompt,
                config=types.GenerateContentConfig(
                    temperature=0.1,
                    response_mime_type='application/json'
                )
            )

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØªÙŠØ¬Ø©
            analysis = json.loads(response.text)

            ai_analysis_results[file_path] = {
                'analysis': analysis,
                'file_info': file_info,
                'timestamp': datetime.datetime.now().isoformat()
            }

        except json.JSONDecodeError as e:
            # Ø§Ù„ÙØ´Ù„ ÙÙŠ parsing - Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙ‚Ù†ÙŠ
            ai_analysis_results[file_path] = {
                'analysis': analyze_single_file_technical(file_path, file_info, dependency_map, entry_points),
                'file_info': file_info,
                'fallback': 'technical'
            }

        except Exception as e:
            # ÙØ´Ù„ AI - Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙ‚Ù†ÙŠ
            ai_analysis_results[file_path] = {
                'analysis': analyze_single_file_technical(file_path, file_info, dependency_map, entry_points),
                'file_info': file_info,
                'fallback': 'technical',
                'error': str(e)
            }

    print(f"\nâœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ù„Ù€ {len(ai_analysis_results)} Ù…Ù„Ù")

    # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    results_file = Path(config.get('repo_path', '.')) / 'ai_analysis_results.json'
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(ai_analysis_results, f, indent=2, ensure_ascii=False)

    print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ: {results_file}")

    return ai_analysis_results


def analyze_files_technically(all_files: Dict, dependency_map: Dict, entry_points: List[str]) -> Dict[str, Dict]:
    """
    ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø·Ø±Ù‚ Ø§Ù„ØªÙ‚Ù†ÙŠØ© ÙÙ‚Ø· (Ø¨Ø¯ÙˆÙ† AI)
    """
    print(f"\nğŸ”§ Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙ‚Ù†ÙŠ Ù„Ù„Ù…Ù„ÙØ§Øª...")
    ai_analysis_results = {}

    for file_path, file_info in all_files.items():
        ai_analysis_results[file_path] = {
            'analysis': analyze_single_file_technical(file_path, file_info, dependency_map, entry_points),
            'file_info': file_info,
            'method': 'technical'
        }

    return ai_analysis_results


def analyze_single_file_technical(file_path: str, file_info: Dict, dependency_map: Dict, entry_points: List[str]) -> Dict:
    """
    ØªØ­Ù„ÙŠÙ„ Ù…Ù„Ù ÙˆØ§Ø­Ø¯ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø·Ø±Ù‚ Ø§Ù„ØªÙ‚Ù†ÙŠØ©
    """
    analysis = {
        'decision': 'UNCERTAIN',
        'confidence': 50,
        'reasons': [],
        'usage_analysis': {
            'is_imported': False,
            'import_count': 0,
            'distance_from_entry': -1,
            'has_unused_exports': False
        },
        'risk_assessment': {
            'deletion_safety_score': 50,
            'potential_impact': 'unknown',
            'affected_files': []
        },
        'recommendation': ''
    }

    # 1. ÙØ­Øµ Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯
    imported_by = dependency_map['imported_by'].get(file_path, [])
    analysis['usage_analysis']['import_count'] = len(imported_by)
    analysis['usage_analysis']['is_imported'] = len(imported_by) > 0

    # 2. ÙØ­Øµ Ø§Ù„Ù…Ø³Ø§ÙØ©
    distance = calculate_distance_from_entry_points(file_path, entry_points, dependency_map)
    analysis['usage_analysis']['distance_from_entry'] = distance

    # 3. ÙØ­Øµ unused exports
    is_unused = file_path in dependency_map['unused_exports']
    analysis['usage_analysis']['has_unused_exports'] = is_unused

    # 4. Ø§Ù„Ø­ÙƒÙ…
    safety_score = 0

    if len(imported_by) == 0:
        analysis['reasons'].append('Ù„Ø§ ÙŠØªÙ… Ø§Ø³ØªÙŠØ±Ø§Ø¯Ù‡ Ù…Ù† Ø£ÙŠ Ù…Ù„Ù')
        safety_score += 40

    if distance == -1:
        analysis['reasons'].append('ØºÙŠØ± Ù…ØªØµÙ„ Ø¨Ø£ÙŠ entry point')
        safety_score += 30
    elif distance > 5:
        analysis['reasons'].append(f'Ø¨Ø¹ÙŠØ¯ Ø¹Ù† entry points ({distance} Ù…Ø³ØªÙˆÙŠØ§Øª)')
        safety_score += 10
    else:
        analysis['reasons'].append(f'Ù‚Ø±ÙŠØ¨ Ù…Ù† entry points ({distance} Ù…Ø³ØªÙˆÙŠØ§Øª)')
        safety_score -= 20

    if is_unused:
        analysis['reasons'].append('exports ØºÙŠØ± Ù…Ø³ØªØ®Ø¯Ù…Ø©')
        safety_score += 20

    if file_info['size_bytes'] == 0:
        analysis['reasons'].append('Ù…Ù„Ù ÙØ§Ø±Øº')
        safety_score += 30

    # Ø£Ù†Ù…Ø§Ø· Ù…Ø´Ø¨ÙˆÙ‡Ø©
    suspicious = ['test', 'temp', 'backup', 'old', 'deprecated', 'unused', '.bak']
    if any(p in file_path.lower() for p in suspicious):
        analysis['reasons'].append('Ø§Ø³Ù… Ù…Ø´Ø¨ÙˆÙ‡')
        safety_score += 15

    # Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
    analysis['risk_assessment']['deletion_safety_score'] = safety_score

    if safety_score >= 70:
        analysis['decision'] = 'DELETE_SAFE'
        analysis['confidence'] = min(95, 50 + safety_score // 2)
        analysis['recommendation'] = 'Ø¢Ù…Ù† Ù„Ù„Ø­Ø°Ù'
    elif safety_score >= 40:
        analysis['decision'] = 'DELETE_PROBABLY'
        analysis['confidence'] = 60
        analysis['recommendation'] = 'ØºØ§Ù„Ø¨Ø§Ù‹ Ø¢Ù…Ù† Ù„Ù„Ø­Ø°Ù - ÙŠØ­ØªØ§Ø¬ Ù…Ø±Ø§Ø¬Ø¹Ø©'
    elif safety_score >= 20:
        analysis['decision'] = 'UNCERTAIN'
        analysis['confidence'] = 40
        analysis['recommendation'] = 'ØºÙŠØ± Ù…ØªØ£ÙƒØ¯ - ÙŠØ­ØªØ§Ø¬ Ù…Ø±Ø§Ø¬Ø¹Ø© ÙŠØ¯ÙˆÙŠØ©'
    else:
        analysis['decision'] = 'KEEP'
        analysis['confidence'] = 80
        analysis['recommendation'] = 'Ù…Ù„Ù Ù†Ø´Ø· - ÙŠØ¬Ø¨ Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ù‡'

    return analysis


def categorize_ai_results(ai_analysis_results: Dict) -> Dict[str, List]:
    """
    ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ù„ÙØ§Øª Ø­Ø³Ø¨ Ù‚Ø±Ø§Ø±Ø§Øª Ø§Ù„Ù€ AI
    """
    categorized = {
        'KEEP': [],
        'DELETE_SAFE': [],
        'DELETE_PROBABLY': [],
        'UNCERTAIN': [],
        'ERROR': []
    }

    for file_path, result in ai_analysis_results.items():
        analysis = result['analysis']
        decision = analysis.get('decision', 'UNCERTAIN')

        if analysis.get('error'):
            categorized['ERROR'].append({
                'path': file_path,
                'result': result
            })
        else:
            categorized[decision].append({
                'path': file_path,
                'result': result,
                'confidence': analysis.get('confidence', 0)
            })

    # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ø«Ù‚Ø©
    for category in ['DELETE_SAFE', 'DELETE_PROBABLY']:
        categorized[category].sort(key=lambda x: x['confidence'], reverse=True)

    print("\nğŸ“Š ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØªØ§Ø¦Ø¬:")
    print(f"  â”œâ”€ KEEP: {len(categorized['KEEP'])}")
    print(f"  â”œâ”€ DELETE_SAFE: {len(categorized['DELETE_SAFE'])}")
    print(f"  â”œâ”€ DELETE_PROBABLY: {len(categorized['DELETE_PROBABLY'])}")
    print(f"  â”œâ”€ UNCERTAIN: {len(categorized['UNCERTAIN'])}")
    print(f"  â””â”€ ERROR: {len(categorized['ERROR'])}")

    return categorized


def convert_ai_results_to_candidates(categorized_results: Dict) -> Dict[str, List]:
    """
    ØªØ­ÙˆÙŠÙ„ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù€ AI Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ø±Ø´Ø­ÙŠÙ† Ù„Ù„Ø­Ø°Ù
    """
    candidates = {
        'safe_to_delete': [],
        'probably_unused': [],
        'uncertain': [],
        'keep': []
    }

    for category, target_key in [
        ('DELETE_SAFE', 'safe_to_delete'),
        ('DELETE_PROBABLY', 'probably_unused'),
        ('UNCERTAIN', 'uncertain'),
        ('KEEP', 'keep')
    ]:
        for item in categorized_results[category]:
            candidates[target_key].append({
                'path': item['path'],
                'info': item['result']['file_info'],
                'classification': {
                    'category': target_key,
                    'reasons': item['result']['analysis'].get('reasons', []),
                    'safety_score': item['result']['analysis'].get('risk_assessment', {}).get('deletion_safety_score', item.get('confidence', 0)),
                    'risk_factors': []
                },
                'deletion_safety': item.get('confidence', 0)
            })

    return candidates


# ============================================================================
# Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø±Ø´Ø­Ø© Ù„Ù„Ø­Ø°Ù
# ============================================================================

def identify_deletion_candidates(all_files: Dict, dependency_map: Dict, entry_points: List[str]) -> Dict[str, List]:
    """
    ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø±Ø´Ø­Ø© Ù„Ù„Ø­Ø°Ù Ø¨Ø¯Ù‚Ø©
    """
    candidates = {
        'safe_to_delete': [],
        'probably_unused': [],
        'uncertain': [],
        'keep': []
    }

    for file_path, file_info in all_files.items():
        classification = classify_file(file_path, file_info, dependency_map, entry_points)
        category = classification['category']

        candidates[category].append({
            'path': file_path,
            'info': file_info,
            'classification': classification,
            'deletion_safety': classification['safety_score']
        })

    # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø£Ù…Ø§Ù†
    for category in candidates:
        candidates[category].sort(key=lambda x: x['deletion_safety'], reverse=True)

    return candidates


def classify_file(file_path: str, file_info: Dict, dependency_map: Dict, entry_points: List[str]) -> Dict:
    """
    ØªØµÙ†ÙŠÙ Ø¯Ù‚ÙŠÙ‚ Ù„Ù„Ù…Ù„Ù
    """
    classification = {
        'category': 'uncertain',
        'reasons': [],
        'safety_score': 0,
        'risk_factors': []
    }

    # 1. ÙØ­Øµ Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±
    has_importers = len(dependency_map['imported_by'].get(file_path, [])) > 0

    if not has_importers:
        classification['reasons'].append('Ù„Ø§ ÙŠØªÙ… Ø§Ø³ØªÙŠØ±Ø§Ø¯Ù‡ Ù…Ù† Ø£ÙŠ Ù…Ù„Ù')
        classification['safety_score'] += 40

    # 2. ÙØ­Øµ Ø§Ù„Ù…Ø³Ø§ÙØ© Ù…Ù† entry points
    distance = calculate_distance_from_entry_points(file_path, entry_points, dependency_map)

    if distance == -1:
        classification['reasons'].append('ØºÙŠØ± Ù…ØªØµÙ„ Ø¨Ø£ÙŠ entry point')
        classification['safety_score'] += 30
    elif distance > 5:
        classification['reasons'].append(f'Ø¨Ø¹ÙŠØ¯ Ø¹Ù† entry points ({distance} Ù…Ø³ØªÙˆÙŠØ§Øª)')
        classification['safety_score'] += 10
    else:
        classification['risk_factors'].append(f'Ù‚Ø±ÙŠØ¨ Ù…Ù† entry points ({distance} Ù…Ø³ØªÙˆÙŠØ§Øª)')
        classification['safety_score'] -= 20

    # 3. ÙØ­Øµ exports ØºÙŠØ± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©
    if file_path in dependency_map['unused_exports']:
        classification['reasons'].append('Ø¬Ù…ÙŠØ¹ exports ØºÙŠØ± Ù…Ø³ØªØ®Ø¯Ù…Ø©')
        classification['safety_score'] += 20

    # 4. ÙØ­Øµ Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù
    if file_info['size_bytes'] == 0:
        classification['reasons'].append('Ù…Ù„Ù ÙØ§Ø±Øº')
        classification['safety_score'] += 10
        classification['category'] = 'safe_to_delete'
        return classification

    # 5. ÙØ­Øµ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…Ø´Ø¨ÙˆÙ‡Ø©
    suspicious_patterns = ['test', 'temp', 'backup', 'old', 'deprecated', 'unused', '.bak']
    if any(pattern in file_path.lower() for pattern in suspicious_patterns):
        classification['reasons'].append('Ø§Ø³Ù… Ù…Ø´Ø¨ÙˆÙ‡ ÙŠØ¯Ù„ Ø¹Ù„Ù‰ Ø¹Ø¯Ù… Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…')
        classification['safety_score'] += 15

    # 7. Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
    if classification['safety_score'] >= 70:
        classification['category'] = 'safe_to_delete'
    elif classification['safety_score'] >= 40:
        classification['category'] = 'probably_unused'
    elif classification['safety_score'] >= 20:
        classification['category'] = 'uncertain'
    else:
        classification['category'] = 'keep'
        classification['reasons'].append('Ø§Ù„Ù…Ù„Ù Ù†Ø´Ø· ÙˆÙ…Ø³ØªØ®Ø¯Ù…')

    return classification


def calculate_distance_from_entry_points(file_path: str, entry_points: List[str], dependency_map: Dict) -> int:
    """
    Ø­Ø³Ø§Ø¨ Ø£Ù‚ØµØ± Ù…Ø³Ø§ÙØ© Ù…Ù† Ø£ÙŠ entry point
    """
    min_distance = float('inf')

    for entry_point in entry_points:
        queue = deque([(entry_point, 0)])
        visited = {entry_point}

        while queue:
            current, distance = queue.popleft()

            if current == file_path or current.endswith(file_path):
                min_distance = min(min_distance, distance)
                break

            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙŠ ÙŠØ³ØªÙˆØ±Ø¯Ù‡Ø§ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø­Ø§Ù„ÙŠ
            imports = dependency_map['imports'].get(current, [])

            for imported_file in imports:
                if imported_file not in visited:
                    visited.add(imported_file)
                    queue.append((imported_file, distance + 1))

    return min_distance if min_distance != float('inf') else -1


# ============================================================================
# Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ø¢Ù…Ù† ÙˆØ§Ù„Ù…Ø­Ø§ÙƒØ§Ø©
# ============================================================================

def perform_safety_checks(candidates: Dict, dependency_map: Dict, config: Dict) -> Dict:
    """
    Ø¥Ø¬Ø±Ø§Ø¡ ÙØ­ÙˆØµØ§Øª Ø£Ù…Ø§Ù† Ø´Ø§Ù…Ù„Ø© Ù‚Ø¨Ù„ Ø§Ù„Ø­Ø°Ù
    """
    print("\nğŸ”’ Ø¬Ø§Ø±ÙŠ Ø¥Ø¬Ø±Ø§Ø¡ ÙØ­ÙˆØµØ§Øª Ø§Ù„Ø£Ù…Ø§Ù†...")

    safety_report = {
        'approved_for_deletion': [],
        'needs_review': [],
        'blocked': [],
        'warnings': []
    }

    # ÙØ­Øµ ÙƒÙ„ Ù…Ù„Ù Ù…Ø±Ø´Ø­ Ù„Ù„Ø­Ø°Ù
    files_to_check = candidates['safe_to_delete'] + candidates['probably_unused']

    for candidate in files_to_check:
        file_path = candidate['path']

        # 1. ÙØ­Øµ Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ§Øª Ø§Ù„Ø¹ÙƒØ³ÙŠØ©
        reverse_deps = dependency_map['imported_by'].get(file_path, [])
        if reverse_deps:
            candidate['blocked_reason'] = f'ÙŠØªÙ… Ø§Ø³ØªÙŠØ±Ø§Ø¯Ù‡ Ù…Ù†: {", ".join(reverse_deps[:3])}'
            safety_report['blocked'].append(candidate)
            continue

        # 2. ÙØ­Øµ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† entry point
        if file_path in config['entry_points']:
            candidate['blocked_reason'] = 'Ù…Ù„Ù entry point - Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø­Ø°ÙÙ‡'
            safety_report['blocked'].append(candidate)
            continue

        # 3. ÙØ­Øµ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø®Ø§ØµØ©
        if requires_manual_review(file_path, candidate):
            safety_report['needs_review'].append(candidate)
            continue

        # 4. Ø§Ø¬ØªØ§Ø² Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙØ­ÙˆØµØ§Øª
        safety_report['approved_for_deletion'].append(candidate)

    print(f"âœ… Ø§Ù„ÙØ­ÙˆØµØ§Øª Ø§Ù„Ø£Ù…Ù†ÙŠØ© Ø§ÙƒØªÙ…Ù„Øª:")
    print(f"  â”œâ”€ Ù…ÙˆØ§ÙÙ‚ Ù„Ù„Ø­Ø°Ù: {len(safety_report['approved_for_deletion'])}")
    print(f"  â”œâ”€ ÙŠØ­ØªØ§Ø¬ Ù…Ø±Ø§Ø¬Ø¹Ø©: {len(safety_report['needs_review'])}")
    print(f"  â””â”€ Ù…Ø­Ø¸ÙˆØ±: {len(safety_report['blocked'])}")

    return safety_report


def requires_manual_review(file_path: str, candidate: Dict) -> bool:
    """
    ØªØ­Ø¯ÙŠØ¯ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ù„Ù ÙŠØ­ØªØ§Ø¬ Ù…Ø±Ø§Ø¬Ø¹Ø© ÙŠØ¯ÙˆÙŠØ©
    """
    # Ù…Ù„ÙØ§Øª config Ø¯Ø§Ø¦Ù…Ø§Ù‹ ØªØ­ØªØ§Ø¬ Ù…Ø±Ø§Ø¬Ø¹Ø©
    config_extensions = ['.json', '.yaml', '.yml', '.toml', '.ini', '.env']
    if any(file_path.endswith(ext) for ext in config_extensions):
        return True

    # Ù…Ù„ÙØ§Øª Ø°Ø§Øª safety score Ù…ØªÙˆØ³Ø·
    if 40 <= candidate['deletion_safety'] < 70:
        return True

    # Ù…Ù„ÙØ§Øª ÙƒØ¨ÙŠØ±Ø©
    if candidate['info']['size_bytes'] > 500 * 80:
        return True

    return False


def simulate_deletion(safety_report: Dict, dependency_map: Dict) -> Dict:
    """
    Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„Ø­Ø°Ù Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¹Ø¯Ù… ÙƒØ³Ø± Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
    """
    print("\nğŸ­ Ø¬Ø§Ø±ÙŠ Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„Ø­Ø°Ù...")

    simulation_results = {
        'would_break': [],
        'safe': [],
        'uncertain': []
    }

    approved_files = [f['path'] for f in safety_report['approved_for_deletion']]

    # ÙÙŠ Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø¨Ø³ÙŠØ·ØŒ Ù†Ø¹ØªØ¨Ø± Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø¹ØªÙ…Ø¯Ø© Ø¢Ù…Ù†Ø©
    for file_path in approved_files:
        simulation_results['safe'].append(file_path)

    print(f"âœ… Ø§Ù„Ù…Ø­Ø§ÙƒØ§Ø© Ø§ÙƒØªÙ…Ù„Øª:")
    print(f"  â”œâ”€ Ø¢Ù…Ù†: {len(simulation_results['safe'])}")
    print(f"  â””â”€ Ø®Ø·Ø±: {len(simulation_results['would_break'])}")

    return simulation_results


# ============================================================================
# Ø§Ù„Ù…Ø±Ø­Ù„Ø© 4: Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø© ÙˆØ§Ù„Ù…ÙˆØ§ÙÙ‚Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ©
# ============================================================================

def interactive_review(safety_report: Dict, simulation_results: Dict, config: Dict) -> List[str]:
    """
    ÙˆØ§Ø¬Ù‡Ø© ØªÙØ§Ø¹Ù„ÙŠØ© Ù„Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ù‚Ø¨Ù„ Ø§Ù„Ø­Ø°Ù
    """
    if not config.get('safe_mode', True):
        print("âš ï¸  Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø¢Ù…Ù† Ù…Ø¹Ø·Ù„ - Ø³ÙŠØªÙ… Ø§Ù„Ø­Ø°Ù ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹")
        return [f['path'] for f in safety_report['approved_for_deletion']]

    print("\n" + "="*70)
    print("ğŸ“‹ Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø±Ø´Ø­Ø© Ù„Ù„Ø­Ø°Ù")
    print("="*70)

    final_approved = []

    # Ø¹Ø±Ø¶ Ù…Ù„Ø®Øµ
    safe_files = simulation_results['safe']
    print(f"\nğŸŸ¢ Ù…Ù„ÙØ§Øª Ø¢Ù…Ù†Ø© Ù„Ù„Ø­Ø°Ù: {len(safe_files)}")

    if len(safe_files) > 0:
        print("\nØ£ÙˆÙ„ 20 Ù…Ù„Ù:")
        for i, file_path in enumerate(safe_files[:20], 1):
            candidate = next((f for f in safety_report['approved_for_deletion'] if f['path'] == file_path), None)
            if candidate:
                print(f"  {i}. {file_path}")
                print(f"     Ø§Ù„Ø£Ø³Ø¨Ø§Ø¨: {', '.join(candidate['classification']['reasons'][:2])}")

        if len(safe_files) > 20:
            print(f"  ... Ùˆ {len(safe_files) - 20} Ù…Ù„Ù Ø¢Ø®Ø±")

        # Ø·Ù„Ø¨ Ø§Ù„Ù…ÙˆØ§ÙÙ‚Ø©
        print("\n" + "-"*70)

        # ÙÙŠ Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„Ø¥Ù†ØªØ§Ø¬
        if config.get('auto_approve', False):
            print("âœ… Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ - Ø§Ù„Ù…ÙˆØ§ÙÙ‚Ø© Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø¢Ù…Ù†Ø©")
            final_approved = safe_files
        else:
            choice = input(f"\nâ“ Ù‡Ù„ ØªÙˆØ§ÙÙ‚ Ø¹Ù„Ù‰ Ø­Ø°Ù Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù€ {len(safe_files)} Ù…Ù„ÙØŸ (y/n/review): ").lower()

            if choice == 'y':
                final_approved = safe_files
                print(f"âœ… ØªÙ…Øª Ø§Ù„Ù…ÙˆØ§ÙÙ‚Ø© Ø¹Ù„Ù‰ Ø­Ø°Ù {len(final_approved)} Ù…Ù„Ù")
            elif choice == 'review':
                final_approved = detailed_file_review(safe_files, safety_report)
            else:
                print("âŒ ØªÙ… Ø¥Ù„ØºØ§Ø¡ Ø§Ù„Ø­Ø°Ù")
                return []

    return final_approved


def detailed_file_review(files: List[str], safety_report: Dict) -> List[str]:
    """
    Ù…Ø±Ø§Ø¬Ø¹Ø© ØªÙØµÙŠÙ„ÙŠØ© Ù„ÙƒÙ„ Ù…Ù„Ù
    """
    approved = []

    print("\n" + "="*70)
    print("ğŸ” Ù…Ø±Ø§Ø¬Ø¹Ø© ØªÙØµÙŠÙ„ÙŠØ©")
    print("="*70)

    for i, file_path in enumerate(files, 1):
        candidate = next((f for f in safety_report['approved_for_deletion'] if f['path'] == file_path), None)

        if not candidate:
            continue

        print(f"\n[{i}/{len(files)}] {file_path}")
        print(f"  Ø§Ù„Ø­Ø¬Ù…: {candidate['info']['size_bytes']} Ø¨Ø§ÙŠØª")
        print(f"  Ø§Ù„Ø£Ø³Ø¨Ø§Ø¨:")
        for reason in candidate['classification']['reasons']:
            print(f"    - {reason}")

        choice = input(f"  Ø§Ø­Ø°Ù Ù‡Ø°Ø§ Ø§Ù„Ù…Ù„ÙØŸ (y/n/skip-rest): ").lower()

        if choice == 'y':
            approved.append(file_path)
        elif choice == 'skip-rest':
            break

    return approved


# ============================================================================
# Ø§Ù„Ù…Ø±Ø­Ù„Ø© 5: Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¢Ù…Ù†
# ============================================================================

def safe_deletion_execution(approved_files: List[str], config: Dict, backup_path: Path) -> Dict:
    """
    ØªÙ†ÙÙŠØ° Ø§Ù„Ø­Ø°Ù Ø¨Ø´ÙƒÙ„ Ø¢Ù…Ù† ÙˆÙ…Ø±Ø­Ù„ÙŠ
    """
    print("\n" + "="*70)
    print("ğŸ—‘ï¸  Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø­Ø°Ù Ø§Ù„Ø¢Ù…Ù†")
    print("="*70)

    deletion_log = {
        'timestamp': datetime.datetime.now().isoformat(),
        'backup_path': str(backup_path),
        'total_files': len(approved_files),
        'deleted': [],
        'failed': [],
        'rollback_available': True
    }

    if config.get('dry_run', False):
        print("\nâš ï¸  ÙˆØ¶Ø¹ Ø§Ù„Ù…Ø­Ø§ÙƒØ§Ø© (DRY RUN) - Ù„Ù† ÙŠØªÙ… Ø­Ø°Ù Ø£ÙŠ Ù…Ù„Ù ÙØ¹Ù„ÙŠØ§Ù‹")
        for file_path in approved_files:
            print(f"  [Ø§Ù„Ù…Ø­Ø§ÙƒØ§Ø©] Ø³ÙŠØªÙ… Ø­Ø°Ù: {file_path}")
            deletion_log['deleted'].append({
                'path': file_path,
                'dry_run': True
            })
        return deletion_log

    repo_path = Path(config['repo_path'])

    # Ø§Ù„Ø­Ø°Ù Ø§Ù„ÙØ¹Ù„ÙŠ
    for i, file_path in enumerate(approved_files, 1):
        try:
            print(f"\r[{i}/{len(approved_files)}] Ø­Ø°Ù: {file_path[:60]}...", end='', flush=True)

            full_path = repo_path / file_path

            # 1. Ù†Ø³Ø® Ø§Ù„Ù…Ù„Ù Ø¥Ù„Ù‰ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø­Ø°Ù
            deleted_backup = backup_path / 'deleted_files' / file_path
            deleted_backup.parent.mkdir(parents=True, exist_ok=True)
            shutil.copy2(full_path, deleted_backup)

            # 2. Ø­Ø°Ù Ø§Ù„Ù…Ù„Ù Ø§Ù„ÙØ¹Ù„ÙŠ
            full_path.unlink()

            deletion_log['deleted'].append({
                'path': file_path,
                'backup_location': str(deleted_backup),
                'timestamp': datetime.datetime.now().isoformat(),
                'status': 'success'
            })

        except Exception as e:
            print(f"\n  âŒ ÙØ´Ù„ Ø­Ø°Ù {file_path}: {e}")
            deletion_log['failed'].append({
                'path': file_path,
                'error': str(e)
            })

    print(f"\n\nâœ… Ø§ÙƒØªÙ…Ù„Øª Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø­Ø°Ù:")
    print(f"  â”œâ”€ Ù†Ø¬Ø­: {len(deletion_log['deleted'])}")
    print(f"  â”œâ”€ ÙØ´Ù„: {len(deletion_log['failed'])}")

    # Ø­ÙØ¸ Ø³Ø¬Ù„ Ø§Ù„Ø­Ø°Ù
    log_file = backup_path / 'deletion_log.json'
    with open(log_file, 'w', encoding='utf-8') as f:
        json.dump(deletion_log, f, indent=2, ensure_ascii=False)

    print(f"  â””â”€ Ø³Ø¬Ù„ Ø§Ù„Ø­Ø°Ù: {log_file}")

    return deletion_log


# ============================================================================
# Ø§Ù„Ù…Ø±Ø­Ù„Ø© 6: Ø§Ù„ØªØ­Ù‚Ù‚ Ø¨Ø¹Ø¯ Ø§Ù„Ø­Ø°Ù
# ============================================================================

def post_deletion_validation(repo_path: Path, config: Dict) -> Dict:
    """
    Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø³Ù„Ø§Ù…Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¨Ø¹Ø¯ Ø§Ù„Ø­Ø°Ù
    """
    print("\n" + "="*70)
    print("ğŸ§ª Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø³Ù„Ø§Ù…Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚")
    print("="*70)

    validation_report = {
        'build_status': 'skipped',
        'tests_status': 'skipped',
        'linting_status': 'skipped',
        'issues_found': [],
        'overall_status': 'unknown'
    }

    # 1. ÙØ­Øµ Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ (TypeScript ÙÙ‚Ø·)
    print("\n1ï¸âƒ£ ÙØ­Øµ TypeScript...")
    try:
        # ÙØ­Øµ frontend
        frontend_result = subprocess.run(
            ['pnpm', '--filter', 'frontend', 'typecheck'],
            cwd=repo_path,
            capture_output=True,
            text=True,
            timeout=180
        )

        # ÙØ­Øµ backend
        backend_result = subprocess.run(
            ['pnpm', '--filter', '@the-copy/backend', 'build'],
            cwd=repo_path,
            capture_output=True,
            text=True,
            timeout=180
        )

        if frontend_result.returncode == 0 and backend_result.returncode == 0:
            validation_report['build_status'] = 'success'
            print("  âœ… Ø§Ù„Ø¨Ù†Ø§Ø¡ Ù†Ø¬Ø­")
        else:
            validation_report['build_status'] = 'failed'
            if frontend_result.returncode != 0:
                validation_report['issues_found'].append({
                    'type': 'frontend_typecheck_error',
                    'message': frontend_result.stderr[:300] if frontend_result.stderr else frontend_result.stdout[:300]
                })
            if backend_result.returncode != 0:
                validation_report['issues_found'].append({
                    'type': 'backend_build_error',
                    'message': backend_result.stderr[:300] if backend_result.stderr else backend_result.stdout[:300]
                })
            print("  âŒ Ø§Ù„Ø¨Ù†Ø§Ø¡ ÙØ´Ù„")
            for issue in validation_report['issues_found']:
                print(f"     - {issue['type']}: {issue['message'][:100]}")

    except subprocess.TimeoutExpired:
        validation_report['build_status'] = 'timeout'
        print("  â±ï¸  Ø§Ù„Ø¨Ù†Ø§Ø¡ Ø§Ø³ØªØºØ±Ù‚ ÙˆÙ‚ØªØ§Ù‹ Ø·ÙˆÙŠÙ„Ø§Ù‹")
    except Exception as e:
        validation_report['build_status'] = 'error'
        print(f"  âŒ Ø®Ø·Ø£: {e}")

    # 2. Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
    if validation_report['build_status'] == 'success':
        validation_report['overall_status'] = 'healthy'
        print("\nâœ… Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ÙÙŠ Ø­Ø§Ù„Ø© Ø¬ÙŠØ¯Ø©")
    else:
        validation_report['overall_status'] = 'unhealthy'
        print("\nâš ï¸  Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ÙŠØ­ØªØ§Ø¬ Ù…Ø±Ø§Ø¬Ø¹Ø©")

    return validation_report


# ============================================================================
# Ø§Ù„Ù…Ø±Ø­Ù„Ø© 7: Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
# ============================================================================

def collect_repo_stats(repo_path: Path) -> Dict:
    """Ø¬Ù…Ø¹ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹"""
    stats = {
        'total_files': 0,
        'total_size_bytes': 0,
        'unused_files': 0,
        'active_files': 0
    }

    for root, dirs, files in os.walk(repo_path):
        dirs[:] = [d for d in dirs if d not in ['node_modules', '.git', '.next', 'dist', '__pycache__', 'temp_backup']]

        for file in files:
            if file.endswith(('.ts', '.tsx', '.js', '.jsx', '.json')):
                stats['total_files'] += 1
                try:
                    stats['total_size_bytes'] += (Path(root) / file).stat().st_size
                except:
                    pass

    stats['total_size_mb'] = stats['total_size_bytes'] / (1024 * 1024)
    return stats


def generate_final_report(deletion_log: Dict, validation_report: Dict, stats_before: Dict, stats_after: Dict, backup_path: Path) -> str:
    """
    ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø±ÙŠØ± Ù†Ù‡Ø§Ø¦ÙŠ Ø´Ø§Ù…Ù„
    """
    report = f"""
{'='*70}
ğŸ“Š ØªÙ‚Ø±ÙŠØ± ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹ - Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
{'='*70}

â° Ø§Ù„ØªØ§Ø±ÙŠØ®: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

{'â”€'*70}
ğŸ“ˆ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù‚Ø¨Ù„ ÙˆØ¨Ø¹Ø¯
{'â”€'*70}

Ù‚Ø¨Ù„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ:
  â€¢ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª: {stats_before['total_files']}
  â€¢ Ø§Ù„Ø­Ø¬Ù… Ø§Ù„ÙƒÙ„ÙŠ: {stats_before['total_size_mb']:.2f} MB

Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ:
  â€¢ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª: {stats_after['total_files']}
  â€¢ Ø§Ù„Ø­Ø¬Ù… Ø§Ù„ÙƒÙ„ÙŠ: {stats_after['total_size_mb']:.2f} MB

Ø§Ù„ØªØ­Ø³ÙŠÙ†:
  â€¢ ØªÙ… Ø­Ø°Ù: {deletion_log['total_files']} Ù…Ù„Ù
  â€¢ ØªÙ… ØªÙˆÙÙŠØ±: {stats_before['total_size_mb'] - stats_after['total_size_mb']:.2f} MB
  â€¢ Ù†Ø³Ø¨Ø© Ø§Ù„ØªÙ‚Ù„ÙŠÙ„: {(1 - stats_after['total_files']/stats_before['total_files'])*100 if stats_before['total_files'] > 0 else 0:.1f}%

{'â”€'*70}
ğŸ—‘ï¸  ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø­Ø°Ù
{'â”€'*70}

âœ… ØªÙ… Ø­Ø°ÙÙ‡Ø§ Ø¨Ù†Ø¬Ø§Ø­: {len(deletion_log['deleted'])}
âŒ ÙØ´Ù„ Ø§Ù„Ø­Ø°Ù: {len(deletion_log['failed'])}

{'â”€'*70}
ğŸ§ª Ø­Ø§Ù„Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ
{'â”€'*70}

Ø§Ù„Ø¨Ù†Ø§Ø¡ (Build): {get_status_emoji(validation_report['build_status'])} {validation_report['build_status']}

Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø¹Ø§Ù…Ø©: {get_status_emoji(validation_report['overall_status'])} {validation_report['overall_status'].upper()}

{'â”€'*70}
ğŸ’¾ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ
{'â”€'*70}

Ø§Ù„Ù…Ø³Ø§Ø±: {backup_path}
Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ø°ÙˆÙØ©: {backup_path}/deleted_files/
Ø³Ø¬Ù„ Ø§Ù„Ø­Ø°Ù: {backup_path}/deletion_log.json

âš ï¸  Ù…Ù„Ø§Ø­Ø¸Ø©: ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø£ÙŠ Ù…Ù„Ù Ù…Ù† Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©

{'='*70}
"""

    # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
    report_file = backup_path / 'cleanup_report.txt'
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)

    print(report)
    print(f"\nğŸ“„ ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙÙŠ: {report_file}")

    return report


def get_status_emoji(status: str) -> str:
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ emoji Ø­Ø³Ø¨ Ø§Ù„Ø­Ø§Ù„Ø©"""
    emoji_map = {
        'success': 'âœ…',
        'passed': 'âœ…',
        'healthy': 'âœ…',
        'failed': 'âŒ',
        'unhealthy': 'âŒ',
        'warnings': 'âš ï¸',
        'skipped': 'â­ï¸',
        'timeout': 'â±ï¸',
        'unknown': 'â“',
        'error': 'âš ï¸'
    }
    return emoji_map.get(status, 'â“')


# ============================================================================
# Ø§Ù„Ø³ÙƒØ±ÙŠØ¨Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
# ============================================================================

def main():
    """Ù†Ù‚Ø·Ø© Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""

    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘          ğŸ§¹ Ø£Ø¯Ø§Ø© ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹ - Ø¥ØµØ¯Ø§Ø± Ø§Ù„Ø¥Ù†ØªØ§Ø¬                â•‘
â•‘                                                                  â•‘
â•‘  Ø§Ù„Ù‡Ø¯Ù: Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆØ¯Ø¹ Ù†Ø¸ÙŠÙ - ÙƒÙ„ Ù…Ù„Ù Ù…ÙØ¹Ù‘Ù„ ÙˆÙ…ÙÙŠØ¯           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    # 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
    config = load_config('cleanup_config.json')
    repo_path = Path(config['repo_path'])

    # 2. Ø¥Ù†Ø´Ø§Ø¡ backup Ø¥Ù„Ø²Ø§Ù…ÙŠ
    print("\nğŸ“¦ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 0: Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©")
    if config.get('create_backup', True):
        backup_path = create_backup(repo_path)
    else:
        print("âš ï¸  ØªØ¹Ø·ÙŠÙ„ Backup - ØºÙŠØ± Ø¢Ù…Ù†!")
        backup_path = repo_path / 'temp_backup'

    # 3. Ø¬Ù…Ø¹ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù‚Ø¨Ù„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ
    print("\nğŸ“Š Ø¬Ù…Ø¹ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹...")
    stats_before = collect_repo_stats(repo_path)
    print(f"  â€¢ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª: {stats_before['total_files']}")
    print(f"  â€¢ Ø§Ù„Ø­Ø¬Ù… Ø§Ù„ÙƒÙ„ÙŠ: {stats_before['total_size_mb']:.2f} MB")

    # 4. Ø¨Ù†Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ§Øª
    print("\nğŸ—ºï¸  Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø¨Ù†Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ§Øª")
    dependency_map = build_complete_dependency_map(repo_path)

    # 5. Ø¬Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª
    print("\nğŸ“ Ø¬Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª...")
    all_files = collect_all_files(
        repo_path,
        config['ignore_patterns'],
        config
    )
    print(f"  â€¢ ÙˆØ¬Ø¯ {len(all_files)} Ù…Ù„Ù Ø¨Ø±Ù…Ø¬ÙŠ")

    # 6. ØªÙˆÙ„ÙŠØ¯ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹
    print("\nğŸ—ºï¸  ØªÙˆÙ„ÙŠØ¯ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹...")
    repo_map = generate_repo_map(repo_path, config['ignore_patterns'])

    # 7. Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ Ø¨Ø§Ù„Ù€ AI
    print("\nğŸ¤– Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1.5: Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ")
    model = initialize_gemini()
    ai_analysis_results = analyze_files_with_ai(
        all_files,
        dependency_map,
        config['entry_points'],
        model,
        config
    )
    categorized_results = categorize_ai_results(ai_analysis_results)

    # 8. ØªØ­ÙˆÙŠÙ„ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù€ AI Ø¥Ù„Ù‰ Ù…Ø±Ø´Ø­ÙŠÙ† Ù„Ù„Ø­Ø°Ù
    print("\nğŸ¯ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø±Ø´Ø­Ø© Ù„Ù„Ø­Ø°Ù")
    candidates = convert_ai_results_to_candidates(categorized_results)

    print(f"  â€¢ Ø¢Ù…Ù† Ù„Ù„Ø­Ø°Ù: {len(candidates['safe_to_delete'])}")
    print(f"  â€¢ ØºØ§Ù„Ø¨Ø§Ù‹ ØºÙŠØ± Ù…Ø³ØªØ®Ø¯Ù…: {len(candidates['probably_unused'])}")
    print(f"  â€¢ ØºÙŠØ± Ù…ØªØ£ÙƒØ¯: {len(candidates['uncertain'])}")
    print(f"  â€¢ Ø§Ø­ØªÙØ¸ Ø¨Ù‡: {len(candidates['keep'])}")

    # 9. ÙØ­ÙˆØµØ§Øª Ø§Ù„Ø£Ù…Ø§Ù†
    print("\nğŸ”’ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: ÙØ­ÙˆØµØ§Øª Ø§Ù„Ø£Ù…Ø§Ù†")
    safety_report = perform_safety_checks(candidates, dependency_map, config)

    # 10. Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„Ø­Ø°Ù
    print("\nğŸ­ Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„Ø­Ø°Ù")
    simulation_results = simulate_deletion(safety_report, dependency_map)

    # 11. Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ©
    print("\nğŸ‘€ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 4: Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø© ÙˆØ§Ù„Ù…ÙˆØ§ÙÙ‚Ø©")
    final_approved = interactive_review(safety_report, simulation_results, config)

    if not final_approved:
        print("\nâŒ Ù„Ù… ØªØªÙ… Ø§Ù„Ù…ÙˆØ§ÙÙ‚Ø© Ø¹Ù„Ù‰ Ø­Ø°Ù Ø£ÙŠ Ù…Ù„Ù - Ø¥Ù†Ù‡Ø§Ø¡ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬")
        return

    # 12. Ø§Ù„ØªÙ†ÙÙŠØ°
    print("\nğŸ—‘ï¸  Ø§Ù„Ù…Ø±Ø­Ù„Ø© 5: Ø§Ù„ØªÙ†ÙÙŠØ°")
    deletion_log = safe_deletion_execution(final_approved, config, backup_path)

    # 13. Ø§Ù„ØªØ­Ù‚Ù‚ Ø¨Ø¹Ø¯ Ø§Ù„Ø­Ø°Ù
    print("\nğŸ§ª Ø§Ù„Ù…Ø±Ø­Ù„Ø© 6: Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø³Ù„Ø§Ù…Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚")
    validation_report = post_deletion_validation(repo_path, config)

    # 14. Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ
    stats_after = collect_repo_stats(repo_path)

    # 15. Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
    print("\nğŸ“„ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 7: ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ")
    generate_final_report(
        deletion_log,
        validation_report,
        stats_before,
        stats_after,
        backup_path
    )

    print("\n" + "="*70)
    print("âœ… Ø§ÙƒØªÙ…Ù„Øª Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªÙ†Ø¸ÙŠÙ!")
    print(f"ğŸ‰ ØªÙ… ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹ - {len(deletion_log['deleted'])} Ù…Ù„Ù ØªÙ… Ø­Ø°ÙÙ‡")
    print("="*70)


if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù…Ù† Ù‚Ø¨Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
